{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9408f98a",
   "metadata": {},
   "source": [
    "# NeMo Framework - Training a large language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcd95d4",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Large language model (LLM) like ChatGPT possess astonishing versatility, being able to perform tasks such as induction, programming, translation, and more, with results comparable to or even superior to human experts. To learn how to pre-train a large language model (LLM). NVIDIA has introduced NeMo Framework that is capabilities to pre-process training data, distribute training across multiple GPUs efficiently.\n",
    "\n",
    "Pre-trained language model is powerful in a variety of tasks but often lack the specialized focus needed for domain-specific applications. Therefore, to adapt the language model to a domain-specific task, fine-tuning can be employed. In this notebook, you will learn how to implement two type of tuning methods, **(1)Fine-tuning** and **(2)PEFT methods** like **LoRA** for adapting language model on specific downstream task using NVIDIA NeMo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934fb96a",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "This course covers the below sections:\n",
    "1. [Pre-training](#s1)\n",
    "    - [1.1 Download dataset](#s1.1)\n",
    "    - [1.2 Data preprocessing](#s1.2)\n",
    "    - [1.3 Download pre-trained model for continued pre-training](#s1.3)\n",
    "    - [1.4 Run pre-training](#s1.4)\n",
    "    \n",
    "    \n",
    "2. [Instruction Tuning ](#s2)\n",
    "    - [2.1 Download dataset: erhwenkuo/alpaca-data-gpt4-chinese-zhtw](#s2.1)\n",
    "    - [2.2 Split the data into train, validation and test](#s2.2)\n",
    "    - [2.3 Full parameter fine-tuning](#s2.3)\n",
    "    - [2.4. Parameter Efficient Fine-tuning](#s2.4)\n",
    "\n",
    "\n",
    "3. [Evaluation](#s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b4fb48",
   "metadata": {},
   "source": [
    "## 1. Pre-training <a name='s1'></a>\n",
    "\n",
    "The initial phase of our process is concentrated on model pre-training, which serves as the primary stage for the model to acquire knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac50c9db",
   "metadata": {},
   "source": [
    "### 1.1 Download dataset <a name='s1.1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01d51714",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 100%|██████████| 9827/9827 [00:00<00:00, 165622.29 examples/s]\n",
      "Creating json from Arrow format: 100%|██████████| 10/10 [00:00<00:00, 67.04ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "13914259"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('erhwenkuo/wikinews-zhtw')['train']\n",
    "dataset.to_json('./data/custom_dataset/json/wikinews-zhtw.jsonl', force_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df2d1cea-ad29-4f2e-baa8-7ebec3237506",
   "metadata": {},
   "source": [
    "### 1.2 Data preprocessing <a name='s1.2'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86ae6657",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 06:58:50 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
      "    \n",
      "[NeMo W 2024-12-06 06:58:50 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, grad_output):\n",
      "    \n",
      "[NeMo W 2024-12-06 06:58:50 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(\n",
      "    \n",
      "[NeMo W 2024-12-06 06:58:50 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, grad_output):\n",
      "    \n",
      "[NeMo W 2024-12-06 06:58:51 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
      "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
      "    \n",
      "[NeMo W 2024-12-06 06:58:52 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
      "        module is deprecated and will be removed in 0.10.0. Please use \n",
      "        'megatron.core.extensions.transformer_engine' instead.\n",
      "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
      "    \n",
      "[NeMo W 2024-12-06 06:58:52 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/models/language_modeling/megatron_base_prompt_learning_model.py:181: DeprecationWarning: invalid escape sequence '\\{'\n",
      "      \"prompt_template_fields\": re.findall(\"\\{(.*?)\\}\", task.prompt_template),\n",
      "    \n",
      "[NeMo W 2024-12-06 06:58:52 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/models/language_modeling/megatron_base_model.py:389: DeprecationWarning: invalid escape sequence '\\.'\n",
      "      return re.fullmatch(\"[0-9][0-9]\\.[0-9][0-9].*\", nvidia_torch_version)  # \"YY.MM.*\"\n",
      "    \n",
      "[NeMo W 2024-12-06 06:58:52 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
      "    \n",
      "[NeMo W 2024-12-06 06:58:53 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/modules/common/megatron/vocab_parallel_cross_entropy.py:88: DeprecationWarning: invalid escape sequence '\\s'\n",
      "      \"\"\"\n",
      "    \n",
      "[NeMo W 2024-12-06 06:58:54 nemo_logging:349] /opt/NeMo/nemo/collections/asr/parts/utils/wfst_utils.py:1328: DeprecationWarning: invalid escape sequence '\\d'\n",
      "      width, height = re.findall('\\d+', line)\n",
      "    \n",
      "[NeMo W 2024-12-06 06:58:54 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "      cm = get_cmap(\"Set1\")\n",
      "    \n",
      "[NeMo W 2024-12-06 06:58:54 nemo_logging:349] /opt/NeMo/nemo/collections/asr/modules/rnnt.py:1558: DeprecationWarning: invalid escape sequence '\\*'\n",
      "      \"\"\"\n",
      "    \n",
      "[NeMo W 2024-12-06 06:58:54 nemo_logging:349] /opt/NeMo/nemo/collections/common/data/lhotse/nemo_adapters.py:198: DeprecationWarning: invalid escape sequence '\\d'\n",
      "      \"\"\"\n",
      "    \n",
      "[NeMo W 2024-12-06 06:58:55 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
      "    \n",
      "[NeMo W 2024-12-06 06:58:55 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "      other = LooseVersion(other)\n",
      "    \n",
      "[NeMo W 2024-12-06 06:58:55 nemo_logging:349] /opt/NeMo/nemo/collections/asr/parts/utils/vad_utils.py:1109: DeprecationWarning: invalid escape sequence '\\s'\n",
      "      data = pd.read_csv(path2ground_truth_label, sep=\"\\s+\", delimiter=None, header=None)\n",
      "    \n",
      "[NeMo W 2024-12-06 06:58:55 nemo_logging:349] /opt/NeMo/nemo/collections/asr/parts/utils/asr_batching.py:39: DeprecationWarning: invalid escape sequence '\\m'\n",
      "      \"\"\"\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 06:58:55 tokenizer_utils:184] Getting HuggingFace AutoTokenizer with pretrained_model_name: meta-llama/Llama-3.1-8B-Instruct\n",
      "Vocab size: 128256\n",
      "Output prefix: data/custom_dataset/preprocessed/wikinews\n",
      "Time to startup: 1.6398766040802002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 06:58:57 tokenizer_utils:184] Getting HuggingFace AutoTokenizer with pretrained_model_name: meta-llama/Llama-3.1-8B-Instruct\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed 100 documents (196.99595891659135 docs/s, 0.12029878842645736 MB/s).\n",
      "Processed 200 documents (347.26952835539146 docs/s, 0.1846339817601497 MB/s).\n",
      "Processed 300 documents (454.751231302774 docs/s, 0.2391264436275112 MB/s).\n",
      "Processed 400 documents (533.7760358421046 docs/s, 0.2853254999253926 MB/s).\n",
      "Processed 500 documents (607.6853733459014 docs/s, 0.3009936978582454 MB/s).\n",
      "Processed 600 documents (705.3881876503789 docs/s, 0.3187000091937115 MB/s).\n",
      "Processed 700 documents (804.1627841878621 docs/s, 0.33327252837427807 MB/s).\n",
      "Processed 800 documents (875.213218642944 docs/s, 0.3465647397477945 MB/s).\n",
      "Processed 900 documents (938.5666078892957 docs/s, 0.35846737259184336 MB/s).\n",
      "Processed 1000 documents (1011.4016784578616 docs/s, 0.3730967361872535 MB/s).\n",
      "Processed 1100 documents (1059.0062488179028 docs/s, 0.39028761456013134 MB/s).\n",
      "Processed 1200 documents (1109.0824554761934 docs/s, 0.40537630664938495 MB/s).\n",
      "Processed 1300 documents (1148.256113185646 docs/s, 0.4197517240914562 MB/s).\n",
      "Processed 1400 documents (1165.5474257830194 docs/s, 0.42899342159325987 MB/s).\n",
      "Processed 1500 documents (1184.7185212259394 docs/s, 0.43850427087281874 MB/s).\n",
      "Processed 1600 documents (1204.4142142307062 docs/s, 0.4509980331696013 MB/s).\n",
      "Processed 1700 documents (1219.420590600891 docs/s, 0.45886553774657324 MB/s).\n",
      "Processed 1800 documents (1229.1543021271268 docs/s, 0.4682085282069425 MB/s).\n",
      "Processed 1900 documents (1255.3272402460696 docs/s, 0.4764138735378536 MB/s).\n",
      "Processed 2000 documents (1260.7899596209727 docs/s, 0.48052575399183645 MB/s).\n",
      "Processed 2100 documents (1256.1084424152089 docs/s, 0.48498352291817476 MB/s).\n",
      "Processed 2200 documents (1268.541011371885 docs/s, 0.493223060510745 MB/s).\n",
      "Processed 2300 documents (1263.9921342626028 docs/s, 0.4957935495502672 MB/s).\n",
      "Processed 2400 documents (1243.889755384596 docs/s, 0.4913216013301993 MB/s).\n",
      "Processed 2500 documents (1198.7167058703023 docs/s, 0.47903946824505506 MB/s).\n",
      "Processed 2600 documents (1199.6204403393683 docs/s, 0.48192460420916966 MB/s).\n",
      "Processed 2700 documents (1201.3125455280876 docs/s, 0.4869453529288026 MB/s).\n",
      "Processed 2800 documents (1188.1382155646036 docs/s, 0.4870965049524934 MB/s).\n",
      "Processed 2900 documents (1185.7038803530515 docs/s, 0.4887277295746881 MB/s).\n",
      "Processed 3000 documents (1181.981638059718 docs/s, 0.49129222974321707 MB/s).\n",
      "Processed 3100 documents (1183.3037166334339 docs/s, 0.49545028881060266 MB/s).\n",
      "Processed 3200 documents (1190.4456911824107 docs/s, 0.49903419888531525 MB/s).\n",
      "Processed 3300 documents (1172.7766946823522 docs/s, 0.49722154884745307 MB/s).\n",
      "Processed 3400 documents (1163.2980596136847 docs/s, 0.4953011698701895 MB/s).\n",
      "Processed 3500 documents (1152.2303094274318 docs/s, 0.49241367964557925 MB/s).\n",
      "Processed 3600 documents (1152.8905805303857 docs/s, 0.49396678265948696 MB/s).\n",
      "Processed 3700 documents (1157.7549516476995 docs/s, 0.4978231654467371 MB/s).\n",
      "Processed 3800 documents (1182.4335450508418 docs/s, 0.5006640561418627 MB/s).\n",
      "Processed 3900 documents (1198.8988572859237 docs/s, 0.5027827600218352 MB/s).\n",
      "Processed 4000 documents (1217.0004144146064 docs/s, 0.505351954643471 MB/s).\n",
      "Processed 4100 documents (1238.4173799397465 docs/s, 0.5082479558887644 MB/s).\n",
      "Processed 4200 documents (1256.2518545786422 docs/s, 0.5109314329516649 MB/s).\n",
      "Processed 4300 documents (1267.9593606374335 docs/s, 0.5138705299098526 MB/s).\n",
      "Processed 4400 documents (1258.632461028638 docs/s, 0.5125272374987128 MB/s).\n",
      "Processed 4500 documents (1248.0313010780112 docs/s, 0.5107201047812738 MB/s).\n",
      "Processed 4600 documents (1240.5977410777173 docs/s, 0.5136568128972073 MB/s).\n",
      "Processed 4700 documents (1231.4742242686375 docs/s, 0.5142326295464551 MB/s).\n",
      "Processed 4800 documents (1226.2586418345038 docs/s, 0.517034429683843 MB/s).\n",
      "Processed 4900 documents (1233.4244074133333 docs/s, 0.5182808520097613 MB/s).\n",
      "Processed 5000 documents (1240.9422808984364 docs/s, 0.520534774812816 MB/s).\n",
      "Processed 5100 documents (1259.7172740552467 docs/s, 0.5218729084778622 MB/s).\n",
      "Processed 5200 documents (1275.8795927645963 docs/s, 0.5240583049828985 MB/s).\n",
      "Processed 5300 documents (1271.0286820378394 docs/s, 0.5190609608372325 MB/s).\n",
      "Processed 5400 documents (1268.6038924759189 docs/s, 0.5189488438654787 MB/s).\n",
      "Processed 5500 documents (1264.7176211457609 docs/s, 0.518936220790852 MB/s).\n",
      "Processed 5600 documents (1262.476489917862 docs/s, 0.5210322248299147 MB/s).\n",
      "Processed 5700 documents (1251.2371202435784 docs/s, 0.5235825228673966 MB/s).\n",
      "Processed 5800 documents (1237.811564974342 docs/s, 0.523323118205052 MB/s).\n",
      "Processed 5900 documents (1240.8733551581697 docs/s, 0.527228904498147 MB/s).\n",
      "Processed 6000 documents (1235.2806999416616 docs/s, 0.5292151131755443 MB/s).\n",
      "Processed 6100 documents (1243.3830373817923 docs/s, 0.5307879184317735 MB/s).\n",
      "Processed 6200 documents (1214.372687065978 docs/s, 0.5321617931073322 MB/s).\n",
      "Processed 6300 documents (1191.0991420733833 docs/s, 0.5337579392740421 MB/s).\n",
      "Processed 6400 documents (1173.5544617137114 docs/s, 0.5361260356922407 MB/s).\n",
      "Processed 6500 documents (1148.94123094898 docs/s, 0.5354562235853488 MB/s).\n",
      "Processed 6600 documents (1134.1288834443417 docs/s, 0.5328400301550454 MB/s).\n",
      "Processed 6700 documents (1125.8842090557403 docs/s, 0.5306963883875159 MB/s).\n",
      "Processed 6800 documents (1129.9311687766824 docs/s, 0.531951128131869 MB/s).\n",
      "Processed 6900 documents (1134.249968636432 docs/s, 0.5328847549465765 MB/s).\n",
      "Processed 7000 documents (1129.2590592836114 docs/s, 0.530913755061547 MB/s).\n",
      "Processed 7100 documents (1128.9627169508608 docs/s, 0.5317144470512235 MB/s).\n",
      "Processed 7200 documents (1134.3578329899658 docs/s, 0.5330631117356215 MB/s).\n",
      "Processed 7300 documents (1134.0534117102727 docs/s, 0.535117728747827 MB/s).\n",
      "Processed 7400 documents (1136.2317083398632 docs/s, 0.5361783266430494 MB/s).\n",
      "Processed 7500 documents (1136.3078274827203 docs/s, 0.5378341949574347 MB/s).\n",
      "Processed 7600 documents (1134.040984372313 docs/s, 0.5392549947956092 MB/s).\n",
      "Processed 7700 documents (1130.7910439891064 docs/s, 0.5401229145178778 MB/s).\n",
      "Processed 7800 documents (1126.0149342458685 docs/s, 0.5409160834597403 MB/s).\n",
      "Processed 7900 documents (1117.5434241569335 docs/s, 0.5438725908146064 MB/s).\n",
      "Processed 8000 documents (1109.1234659262116 docs/s, 0.5450784097951192 MB/s).\n",
      "Processed 8100 documents (1103.6464237003945 docs/s, 0.5455354098823436 MB/s).\n",
      "Processed 8200 documents (1107.1705721569815 docs/s, 0.5463936349484995 MB/s).\n",
      "Processed 8300 documents (1108.7853135307855 docs/s, 0.5478065349325092 MB/s).\n",
      "Processed 8400 documents (1111.4436875087738 docs/s, 0.5490906624562887 MB/s).\n",
      "Processed 8500 documents (1110.0031499026431 docs/s, 0.5493032930322018 MB/s).\n",
      "Processed 8600 documents (1095.4957651638986 docs/s, 0.5479241774248695 MB/s).\n",
      "Processed 8700 documents (1091.5046260207941 docs/s, 0.5493449197525052 MB/s).\n",
      "Processed 8800 documents (1082.6941534435364 docs/s, 0.5468193617697319 MB/s).\n",
      "Processed 8900 documents (1074.8615892001842 docs/s, 0.5456799190380648 MB/s).\n",
      "Processed 9000 documents (1073.276321639755 docs/s, 0.545491124660087 MB/s).\n",
      "Processed 9100 documents (1070.3604222778924 docs/s, 0.5467400084331739 MB/s).\n",
      "Processed 9200 documents (1072.9048164331543 docs/s, 0.5467666601715598 MB/s).\n",
      "Processed 9300 documents (1063.1195903093667 docs/s, 0.5476944545134924 MB/s).\n",
      "Processed 9400 documents (1059.2186970536163 docs/s, 0.5484399138396066 MB/s).\n",
      "Processed 9500 documents (1057.30289462092 docs/s, 0.5490013081924541 MB/s).\n",
      "Processed 9600 documents (1037.7606958480312 docs/s, 0.5448686908694997 MB/s).\n",
      "Processed 9700 documents (1033.4545620994704 docs/s, 0.5455628252172802 MB/s).\n",
      "Processed 9800 documents (1035.1821166118102 docs/s, 0.5465923103711479 MB/s).\n",
      "[NeMo W 2024-12-06 06:59:06 nemo_logging:349] /usr/lib/python3.10/multiprocessing/pool.py:268: ResourceWarning: unclosed running multiprocessing pool <multiprocessing.pool.Pool state=RUN pool_size=1>\n",
      "      _warn(f\"unclosed running multiprocessing pool {self!r}\",\n",
      "    \n",
      "[NeMo W 2024-12-06 06:59:06 nemo_logging:349] /opt/NeMo/scripts/nlp_language_modeling/preprocess_data_for_megatron.py:364: ResourceWarning: unclosed file <_io.TextIOWrapper name='data/custom_dataset/json/wikinews-zhtw.jsonl' mode='r' encoding='utf-8'>\n",
      "      main()\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file data/custom_dataset/json/wikinews-zhtw.jsonl 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 06:59:06 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpmwec6j7e'>\n",
      "      _warnings.warn(warn_message, ResourceWarning)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "export HF_TOKEN=???\n",
    "\n",
    "mkdir -p data/custom_dataset/preprocessed\n",
    "\n",
    "python /opt/NeMo/scripts/nlp_language_modeling/preprocess_data_for_megatron.py \\\n",
    "--input=data/custom_dataset/json/wikinews-zhtw.jsonl \\\n",
    "--json-keys=text \\\n",
    "--dataset-impl mmap \\\n",
    "--tokenizer-library=huggingface \\\n",
    "--tokenizer-type meta-llama/Llama-3.1-8B-Instruct \\\n",
    "--output-prefix=data/custom_dataset/preprocessed/wikinews \\\n",
    "--append-eod"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c463fb8",
   "metadata": {},
   "source": [
    "### 1.3 Download pre-trained model for continued pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97d33e15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "export HF_TOKEN=???\n",
    "HF_MODEL=meta-llama/Llama-3.1-8B-Instruct ## Download from HF\n",
    "#HF_MODEL=/workspace/Llama-3.1-8B-Instruct/ ## Load from local\n",
    "\n",
    "huggingface-cli download $HF_MODEL\n",
    "\n",
    "python /opt/NeMo/scripts/checkpoint_converters/convert_llama_hf_to_nemo.py \\\n",
    "--input_name_or_path $HF_MODEL \\\n",
    "--output_path Llama-3.1-8B-Instruct.nemo \\\n",
    "--llama31 True \\\n",
    "--precision bf16"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f70251c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 1.4 Run pre-training <a name='s1.4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a7b94169",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 07:43:13 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
      "    \n",
      "[NeMo W 2024-12-06 07:43:13 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, grad_output):\n",
      "    \n",
      "[NeMo W 2024-12-06 07:43:13 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(\n",
      "    \n",
      "[NeMo W 2024-12-06 07:43:13 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, grad_output):\n",
      "    \n",
      "[NeMo W 2024-12-06 07:43:13 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
      "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
      "    \n",
      "[NeMo W 2024-12-06 07:43:14 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
      "        module is deprecated and will be removed in 0.10.0. Please use \n",
      "        'megatron.core.extensions.transformer_engine' instead.\n",
      "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
      "    \n",
      "[NeMo W 2024-12-06 07:43:14 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
      "    \n",
      "[NeMo W 2024-12-06 07:43:15 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "      cm = get_cmap(\"Set1\")\n",
      "    \n",
      "[NeMo W 2024-12-06 07:43:16 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
      "    \n",
      "[NeMo W 2024-12-06 07:43:16 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "      other = LooseVersion(other)\n",
      "    \n",
      "[NeMo W 2024-12-06 07:43:16 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 07:43:16 megatron_gpt_pretraining:37] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2024-12-06 07:43:16 megatron_gpt_pretraining:38] \n",
      "    run:\n",
      "      name: llama3_8b\n",
      "      results_dir: ${base_results_dir}/${.name}\n",
      "      time_limit: 0-01:30:00\n",
      "      dependency: singleton\n",
      "    trainer:\n",
      "      num_nodes: 1\n",
      "      devices: 4\n",
      "      accelerator: gpu\n",
      "      precision: bf16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: null\n",
      "      max_steps: 100\n",
      "      max_time: 05:23:30:00\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 50\n",
      "      limit_val_batches: 1\n",
      "      limit_test_batches: 50\n",
      "      accumulate_grad_batches: 1\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: /workspace/results/Llama-3.1-8B/pretrain\n",
      "      exp_dir: null\n",
      "      name: megatron_llama\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: nemo_llama_pretrain\n",
      "        name: Llama-3.1-8B\n",
      "      resume_if_exists: false\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: val_loss\n",
      "        save_top_k: 10\n",
      "        mode: min\n",
      "        always_save_nemo: false\n",
      "        save_nemo_on_train_end: true\n",
      "        filename: megatron_llama--{val_loss:.2f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: 4\n",
      "      log_step_timing: true\n",
      "      step_timing_kwargs:\n",
      "        sync_cuda: true\n",
      "        buffer_size: 5\n",
      "      seconds_to_sleep: 60\n",
      "    model:\n",
      "      mcore_gpt: true\n",
      "      micro_batch_size: 1\n",
      "      global_batch_size: 8\n",
      "      rampup_batch_size: null\n",
      "      tensor_model_parallel_size: 4\n",
      "      pipeline_model_parallel_size: 1\n",
      "      virtual_pipeline_model_parallel_size: null\n",
      "      context_parallel_size: 1\n",
      "      encoder_seq_length: 8192\n",
      "      max_position_embeddings: 8192\n",
      "      num_layers: 32\n",
      "      hidden_size: 4096\n",
      "      ffn_hidden_size: 14336\n",
      "      num_attention_heads: 32\n",
      "      num_query_groups: 8\n",
      "      init_method_std: 0.02\n",
      "      use_scaled_init_method: true\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      kv_channels: null\n",
      "      apply_query_key_layer_scaling: true\n",
      "      normalization: rmsnorm\n",
      "      layernorm_epsilon: 1.0e-05\n",
      "      do_layer_norm_weight_decay: false\n",
      "      make_vocab_size_divisible_by: 128\n",
      "      pre_process: true\n",
      "      post_process: true\n",
      "      persist_layer_norm: true\n",
      "      bias: false\n",
      "      activation: fast-swiglu\n",
      "      headscale: false\n",
      "      transformer_block_type: pre_ln\n",
      "      openai_gelu: false\n",
      "      normalize_attention_scores: true\n",
      "      position_embedding_type: rope\n",
      "      rotary_percentage: 1.0\n",
      "      apply_rope_fusion: true\n",
      "      cross_entropy_loss_fusion: true\n",
      "      attention_type: multihead\n",
      "      share_embeddings_and_output_weights: false\n",
      "      tokenizer:\n",
      "        library: huggingface\n",
      "        type: meta-llama/Meta-Llama-3-8B\n",
      "        use_fast: true\n",
      "      native_amp_init_scale: 4294967296\n",
      "      native_amp_growth_interval: 1000\n",
      "      hysteresis: 2\n",
      "      fp32_residual_connection: false\n",
      "      fp16_lm_cross_entropy: false\n",
      "      megatron_amp_O2: true\n",
      "      grad_allreduce_chunk_size_mb: 125\n",
      "      grad_div_ar_fusion: true\n",
      "      gradient_accumulation_fusion: true\n",
      "      bias_activation_fusion: true\n",
      "      bias_dropout_add_fusion: true\n",
      "      masked_softmax_fusion: true\n",
      "      seed: 1234\n",
      "      resume_from_checkpoint: null\n",
      "      use_cpu_initialization: false\n",
      "      onnx_safe: false\n",
      "      apex_transformer_log_level: 30\n",
      "      gradient_as_bucket_view: true\n",
      "      sync_batch_comm: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      num_micro_batches_with_partial_activation_checkpoints: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      sequence_parallel: false\n",
      "      deterministic_mode: false\n",
      "      transformer_engine: true\n",
      "      fp8: false\n",
      "      fp8_e4m3: false\n",
      "      fp8_hybrid: false\n",
      "      fp8_margin: 0\n",
      "      fp8_interval: 1\n",
      "      fp8_amax_history_len: 1024\n",
      "      fp8_amax_compute_algo: max\n",
      "      ub_tp_comm_overlap: false\n",
      "      use_flash_attention: true\n",
      "      gc_interval: 100\n",
      "      nsys_profile:\n",
      "        enabled: false\n",
      "        trace:\n",
      "        - nvtx\n",
      "        - cuda\n",
      "        start_step: 10\n",
      "        end_step: 10\n",
      "        ranks:\n",
      "        - 0\n",
      "        gen_shape: false\n",
      "      optim:\n",
      "        name: distributed_fused_adam\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.1\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.95\n",
      "        bucket_cap_mb: 125\n",
      "        overlap_grad_sync: true\n",
      "        overlap_param_sync: true\n",
      "        contiguous_grad_buffer: true\n",
      "        contiguous_param_buffer: true\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 500\n",
      "          constant_steps: 0\n",
      "          min_lr: 1.0e-05\n",
      "      data:\n",
      "        data_impl: mmap\n",
      "        splits_string: 9990,8,2\n",
      "        seq_length: 8192\n",
      "        skip_warmup: true\n",
      "        num_workers: 0\n",
      "        dataloader_type: single\n",
      "        reset_position_ids: true\n",
      "        reset_attention_mask: true\n",
      "        eod_mask_loss: false\n",
      "        index_mapping_dir: null\n",
      "        data_prefix:\n",
      "        - 1.0\n",
      "        - data/custom_dataset/preprocessed/wikinews_text_document\n",
      "      restore_from_path: Llama-3.1-8B-Instruct.nemo\n",
      "      rotary_base: 500000.0\n",
      "      seq_len_interpolation_factor: 8\n",
      "    base_results_dir: results\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 07:43:16 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n",
      "[NeMo W 2024-12-06 07:43:16 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py:1395: DeprecationWarning: torch.set_autocast_gpu_dtype(dtype) is deprecated. Please use torch.set_autocast_dtype('cuda', dtype) instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:678.)\n",
      "      torch.set_autocast_gpu_dtype(dtype)\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 07:43:17 exp_manager:400] ExpManager schema\n",
      "[NeMo I 2024-12-06 07:43:17 exp_manager:401] {'explicit_log_dir': None, 'exp_dir': None, 'name': None, 'version': None, 'use_datetime_version': True, 'resume_if_exists': False, 'resume_past_end': False, 'resume_ignore_no_checkpoint': False, 'resume_from_checkpoint': None, 'create_tensorboard_logger': True, 'summary_writer_kwargs': None, 'create_wandb_logger': False, 'wandb_logger_kwargs': None, 'create_mlflow_logger': False, 'mlflow_logger_kwargs': {'experiment_name': None, 'tracking_uri': None, 'tags': None, 'save_dir': './mlruns', 'prefix': '', 'artifact_location': None, 'run_id': None, 'log_model': False}, 'create_dllogger_logger': False, 'dllogger_logger_kwargs': {'verbose': False, 'stdout': False, 'json_file': './dllogger.json'}, 'create_clearml_logger': False, 'clearml_logger_kwargs': {'project': None, 'task': None, 'connect_pytorch': False, 'model_name': None, 'tags': None, 'log_model': False, 'log_cfg': False, 'log_metrics': False}, 'create_neptune_logger': False, 'neptune_logger_kwargs': None, 'create_checkpoint_callback': True, 'checkpoint_callback_params': {'filepath': None, 'dirpath': None, 'filename': None, 'monitor': 'val_loss', 'verbose': True, 'save_last': True, 'save_top_k': 3, 'save_weights_only': False, 'mode': 'min', 'auto_insert_metric_name': True, 'every_n_epochs': 1, 'every_n_train_steps': None, 'train_time_interval': None, 'prefix': None, 'postfix': '.nemo', 'save_best_model': False, 'always_save_nemo': False, 'save_nemo_on_train_end': True, 'model_parallel_size': None, 'save_on_train_epoch_end': False, 'async_save': False}, 'create_early_stopping_callback': False, 'early_stopping_callback_params': {'monitor': 'val_loss', 'mode': 'min', 'min_delta': 0.001, 'patience': 10, 'verbose': True, 'strict': True, 'check_finite': True, 'stopping_threshold': None, 'divergence_threshold': None, 'check_on_train_epoch_end': None, 'log_rank_zero_only': False}, 'create_preemption_callback': True, 'files_to_copy': None, 'log_step_timing': True, 'step_timing_kwargs': {'reduction': 'mean', 'sync_cuda': False, 'buffer_size': 1}, 'log_local_rank_0_only': False, 'log_global_rank_0_only': False, 'disable_validation_on_resume': True, 'ema': {'enable': False, 'decay': 0.999, 'cpu_offload': False, 'validate_original_weights': False, 'every_n_steps': 1}, 'max_time_per_run': None, 'seconds_to_sleep': 5.0, 'create_straggler_detection_callback': False, 'straggler_detection_params': {'report_time_interval': 300.0, 'calc_relative_gpu_perf': True, 'calc_individual_gpu_perf': True, 'num_gpu_perf_scores_to_log': 5, 'gpu_relative_perf_threshold': 0.7, 'gpu_individual_perf_threshold': 0.7, 'stop_if_detected': False}, 'create_fault_tolerance_callback': False, 'fault_tolerance': {'workload_check_interval': 5.0, 'initial_rank_heartbeat_timeout': 3600.0, 'rank_heartbeat_timeout': 2700.0, 'calculate_timeouts': True, 'safety_factor': 5.0, 'rank_termination_signal': <Signals.SIGKILL: 9>, 'log_level': 'INFO', 'max_rank_restarts': 0, 'max_subsequent_job_failures': 0, 'additional_ft_launcher_args': '', 'simulated_fault': None}, 'log_tflops_per_sec_per_gpu': True}\n",
      "[NeMo I 2024-12-06 07:43:17 exp_manager:459] Experiments will be logged at /workspace/results/Llama-3.1-8B/pretrain\n",
      "[NeMo I 2024-12-06 07:43:17 exp_manager:1010] TensorboardLogger has been set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 07:43:17 exp_manager:1139] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 100. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 07:43:17 exp_manager:593] TFLOPs per sec per GPU will be calculated, conditioned on supported models. Defaults to -1 upon failure.\n",
      "[NeMo I 2024-12-06 07:43:17 megatron_gpt_pretraining:46] Continual training: loading weights from Llama-3.1-8B-Instruct.nemo\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 07:43:54 megatron_init:314] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-12-06 07:43:54 megatron_init:320] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-12-06 07:43:54 megatron_init:325] All data parallel group ranks with context parallel combined: [[0], [1], [2], [3]]\n",
      "[NeMo I 2024-12-06 07:43:54 megatron_init:328] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-12-06 07:43:54 megatron_init:336] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-12-06 07:43:54 megatron_init:339] All context parallel group ranks: [[0], [1], [2], [3]]\n",
      "[NeMo I 2024-12-06 07:43:54 megatron_init:340] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-12-06 07:43:54 megatron_init:347] Rank 0 has model parallel group: [0, 1, 2, 3]\n",
      "[NeMo I 2024-12-06 07:43:54 megatron_init:348] All model parallel group ranks: [[0, 1, 2, 3]]\n",
      "[NeMo I 2024-12-06 07:43:54 megatron_init:357] Rank 0 has tensor model parallel group: [0, 1, 2, 3]\n",
      "[NeMo I 2024-12-06 07:43:54 megatron_init:361] All tensor model parallel group ranks: [[0, 1, 2, 3]]\n",
      "[NeMo I 2024-12-06 07:43:54 megatron_init:362] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-12-06 07:43:54 megatron_init:382] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-12-06 07:43:54 megatron_init:394] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-12-06 07:43:54 megatron_init:400] All pipeline model parallel group ranks: [[0], [1], [2], [3]]\n",
      "[NeMo I 2024-12-06 07:43:54 megatron_init:401] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-12-06 07:43:54 megatron_init:402] All embedding group ranks: [[0], [1], [2], [3]]\n",
      "[NeMo I 2024-12-06 07:43:54 megatron_init:403] Rank 0 has embedding rank: 0\n",
      "[NeMo I 2024-12-06 07:43:54 num_microbatches_calculator:218] setting number of microbatches to constant 8\n",
      "[NeMo I 2024-12-06 07:43:54 tokenizer_utils:184] Getting HuggingFace AutoTokenizer with pretrained_model_name: meta-llama/Meta-Llama-3-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:54 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 07:43:55 megatron_base_model:601] Padded vocab_size: 128512, original vocab_size: 128256, dummy tokens: 256.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:516] apply_query_key_layer_scaling is only enabled when using FP16, setting it to False and setting NVTE_APPLY_QK_LAYER_SCALING=0\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: first_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: last_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: tp_only_amax_red in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_router_pre_softmax in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: external_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 07:43:55 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: config_logger_dir in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply rope scaling ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply rope scaling ...\n",
      "apply rope scaling ...\n",
      "apply rope scaling ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "[rank2]:[W1206 07:46:18.247549901 ProcessGroupGloo.cpp:712] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "[rank1]:[W1206 07:46:21.821214335 ProcessGroupGloo.cpp:712] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "[rank3]:[W1206 07:46:23.521077214 ProcessGroupGloo.cpp:712] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[rank0]:[W1206 07:46:23.529661036 ProcessGroupGloo.cpp:712] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "[NeMo W 2024-12-06 07:47:44 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:755: FutureWarning: `load_state_dict` is deprecated and will be removed in future versions. Please use `load` instead.\n",
      "      checkpoint.load_state_dict(\n",
      "    \n",
      "[NeMo W 2024-12-06 07:47:44 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/distributed/checkpoint/planner_helpers.py:311: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n",
      "      device = getattr(value, \"device\", None)\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 07:48:03 nlp_overrides:1358] Model MegatronGPTModel was successfully restored from /workspace/Llama-3.1-8B-Instruct.nemo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 07:48:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:161: You have overridden `MegatronGPTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.\n",
      "    \n",
      "[NeMo W 2024-12-06 07:48:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:143: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.\n",
      "    \n",
      "[NeMo W 2024-12-06 07:48:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 07:48:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _descriptor.FieldDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 07:48:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 07:48:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 07:48:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _descriptor.EnumValueDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 07:48:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _DATATYPE = _descriptor.EnumDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 07:48:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 07:48:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _descriptor.FieldDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 07:48:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 07:48:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 07:48:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _descriptor.FieldDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 07:48:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _TENSORPROTO = _descriptor.Descriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 07:48:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 07:48:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:35: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _descriptor.EnumValueDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 07:48:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:29: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _DATACLASS = _descriptor.EnumDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 07:48:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:74: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _descriptor.FieldDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 07:48:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:67: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _SUMMARYDESCRIPTION = _descriptor.Descriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 07:48:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:326: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "      np.bool8: (False, True),\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 07:48:04 megatron_gpt_model:1680] Pipeline model parallel rank: 0, Tensor model parallel rank: 0, Number of model parameters on device: 2.01e+09. Number of precise model parameters on device: 8033157120.\n",
      "[NeMo I 2024-12-06 07:48:04 megatron_gpt_model:1524] Building GPT datasets.\n",
      "[NeMo I 2024-12-06 07:48:04 utils:259] Let split_matrix = [(0, 0.999), (0.999, 0.9998), (0.9998, 1.0)]\n",
      "[NeMo I 2024-12-06 07:48:04 utils:259] Building dataset splits with cls=GPTDataset, sizes=[800, 24, 400], and config=GPTDatasetConfig(random_seed=1234, sequence_length=8192, blend=(['data/custom_dataset/preprocessed/wikinews_text_document'], [1.0]), blend_per_split=None, renormalize_blend_weights=False, split='9990,8,2', split_matrix=[(0, 0.999), (0.999, 0.9998), (0.9998, 1.0)], num_dataset_builder_threads=1, path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<nemo.collections.common.tokenizers.huggingface.auto_tokenizer.AutoTokenizer object at 0x7f4d525d5120>, reset_position_ids=True, reset_attention_mask=True, eod_mask_loss=False, create_attention_mask=False, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True, s3_cache_path=None)\n",
      "[NeMo I 2024-12-06 07:48:04 utils:259] Load the _IndexReader from data/custom_dataset/preprocessed/wikinews_text_document.idx\n",
      "[NeMo I 2024-12-06 07:48:04 utils:259] \tExtract the sequence lengths\n",
      "[NeMo I 2024-12-06 07:48:04 utils:259] \tExtract the sequence pointers\n",
      "[NeMo I 2024-12-06 07:48:04 utils:259] \tExtract the document indices\n",
      "[NeMo I 2024-12-06 07:48:04 utils:259] > total number of sequences: 9827\n",
      "[NeMo I 2024-12-06 07:48:04 utils:259] > total number of documents: 9827\n",
      "[NeMo I 2024-12-06 07:48:04 utils:259] Build and save the GPTDataset train indices\n",
      "[NeMo I 2024-12-06 07:48:04 utils:259] > total number of samples: 959\n",
      "[NeMo I 2024-12-06 07:48:04 utils:259] > total number of epochs: 2\n",
      "[NeMo I 2024-12-06 07:48:04 utils:259] Build and save the GPTDataset valid indices\n",
      "[NeMo I 2024-12-06 07:48:04 utils:259] > total number of samples: 25\n",
      "[NeMo I 2024-12-06 07:48:04 utils:259] > total number of epochs: 30\n",
      "[NeMo I 2024-12-06 07:48:04 utils:259] Build and save the GPTDataset test indices\n",
      "[NeMo I 2024-12-06 07:48:04 utils:259] > total number of samples: 402\n",
      "[NeMo I 2024-12-06 07:48:04 utils:259] > total number of epochs: 2179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 07:48:04 utils:259] Building a BlendedDataset for a single MegatronDataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 07:48:04 utils:259] Build and save the BlendedDataset indices\n",
      "[NeMo I 2024-12-06 07:48:04 utils:259] \tBuild and save the dataset and dataset sample indexes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 07:48:04 utils:259] Unable to save the BlendedDataset indexes because path_to_cache is None\n",
      "[NeMo W 2024-12-06 07:48:04 utils:259] Building a BlendedDataset for a single MegatronDataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 07:48:04 utils:259] Build and save the BlendedDataset indices\n",
      "[NeMo I 2024-12-06 07:48:04 utils:259] \tBuild and save the dataset and dataset sample indexes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 07:48:04 utils:259] Unable to save the BlendedDataset indexes because path_to_cache is None\n",
      "[NeMo W 2024-12-06 07:48:04 utils:259] Building a BlendedDataset for a single MegatronDataset\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 07:48:04 utils:259] Build and save the BlendedDataset indices\n",
      "[NeMo I 2024-12-06 07:48:04 utils:259] \tBuild and save the dataset and dataset sample indexes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 07:48:04 utils:259] Unable to save the BlendedDataset indexes because path_to_cache is None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 07:48:04 utils:259] Verifying NumPy indices for BlendedDataset train split\n",
      "[NeMo I 2024-12-06 07:48:04 utils:259] Verifying NumPy indices for BlendedDataset valid split\n",
      "[NeMo I 2024-12-06 07:48:04 utils:259] Verifying NumPy indices for BlendedDataset test split\n",
      "[NeMo I 2024-12-06 07:48:04 megatron_gpt_model:1613] Length of train dataset: 804\n",
      "[NeMo I 2024-12-06 07:48:04 megatron_gpt_model:1615] Length of val dataset: 25\n",
      "[NeMo I 2024-12-06 07:48:04 megatron_gpt_model:1617] Length of test dataset: 402\n",
      "[NeMo I 2024-12-06 07:48:04 megatron_gpt_model:1618] Finished building GPT datasets.\n",
      "[NeMo I 2024-12-06 07:48:04 megatron_gpt_model:1729] Setting up train dataloader with len(len(self._train_ds)): 804 and consumed samples: 0\n",
      "[NeMo I 2024-12-06 07:48:04 megatron_gpt_model:1627] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-12-06 07:48:04 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 804 and consumed_samples: 0\n",
      "[NeMo I 2024-12-06 07:48:04 megatron_gpt_model:1737] Setting up validation dataloader with len(len(self._validation_ds)): 25 and consumed samples: 0\n",
      "[NeMo I 2024-12-06 07:48:04 megatron_gpt_model:1627] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-12-06 07:48:04 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 25 and consumed_samples: 0\n",
      "[NeMo I 2024-12-06 07:48:04 megatron_gpt_model:1758] Setting up test dataloader with len(len(self._test_ds)): 402 and consumed samples: 0\n",
      "[NeMo I 2024-12-06 07:48:04 megatron_gpt_model:1627] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-12-06 07:48:04 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 402 and consumed_samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 07:48:04 modelPT:787] Optimizer config = MegatronDistributedFusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.95]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        is_expert: False\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.1\n",
      "    )\n",
      "[NeMo I 2024-12-06 07:48:04 lr_scheduler:948] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f4d32fb9120>\" \n",
      "    will be used during training (effective maximum steps = 100) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 500\n",
      "    constant_steps: 0\n",
      "    min_lr: 1.0e-05\n",
      "    max_steps: 100\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type          | Params | Mode \n",
      "------------------------------------------------\n",
      "0 | model | Float16Module | 2.0 B  | train\n",
      "------------------------------------------------\n",
      "2.0 B     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 B     Total params\n",
      "8,033.157 Total estimated model params size (MB)\n",
      "650       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "[NeMo W 2024-12-06 07:48:05 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=62` in the `DataLoader` to improve performance.\n",
      "    \n",
      "[NeMo W 2024-12-06 07:48:05 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n",
      "[NeMo W 2024-12-06 07:48:09 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 07:48:13 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 07:48:15 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-12-06 07:48:15 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=62` in the `DataLoader` to improve performance.\n",
      "    \n",
      "[NeMo W 2024-12-06 07:48:15 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n",
      "[NeMo W 2024-12-06 07:48:17 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 07:48:20 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 07:52:25 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  50%|█████     | 50/100 [04:09<04:09, reduced_train_loss=2.370, global_step=49.00, consumed_samples=400.0, train_step_timing in s=4.920]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 8/8 [00:01<00:00,  4.59it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 50: 'val_loss' reached 1.79310 (best 1.79310), saving model to '/workspace/results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama--val_loss=1.79-step=50-consumed_samples=400.0.ckpt' as top 10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: :  50%|█████     | 50/100 [04:11<04:11, reduced_train_loss=2.370, global_step=49.00, consumed_samples=400.0, train_step_timing in s=4.920, val_loss=1.790][NeMo I 2024-12-06 07:52:35 dist_ckpt_io:421] Using TorchDistSaveShardedStrategy(torch_dist, 1) dist-ckpt save strategy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 07:52:45 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
      "    \n",
      "[NeMo W 2024-12-06 07:52:45 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, grad_output):\n",
      "    \n",
      "[NeMo W 2024-12-06 07:52:45 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(\n",
      "    \n",
      "[NeMo W 2024-12-06 07:52:45 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, grad_output):\n",
      "    \n",
      "[NeMo W 2024-12-06 07:52:46 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
      "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
      "    \n",
      "[NeMo W 2024-12-06 07:52:46 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
      "        module is deprecated and will be removed in 0.10.0. Please use \n",
      "        'megatron.core.extensions.transformer_engine' instead.\n",
      "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
      "    \n",
      "[NeMo W 2024-12-06 07:52:46 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
      "    \n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[NeMo W 2024-12-06 07:52:48 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "      cm = get_cmap(\"Set1\")\n",
      "    \n",
      "[NeMo W 2024-12-06 07:52:48 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
      "    \n",
      "[NeMo W 2024-12-06 07:52:48 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "      other = LooseVersion(other)\n",
      "    \n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 100%|██████████| 100/100 [11:39<00:00, reduced_train_loss=2.080, global_step=99.00, consumed_samples=800.0, train_step_timing in s=4.920, val_loss=1.790]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
      "Validation:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 8/8 [00:01<00:00,  4.65it/s]\u001b[A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 100: 'val_loss' reached 1.65426 (best 1.65426), saving model to '/workspace/results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama--val_loss=1.65-step=100-consumed_samples=800.0.ckpt' as top 10\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "`Trainer.fit` stopped: `max_steps=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 100%|██████████| 100/100 [15:17<00:00, reduced_train_loss=2.080, global_step=99.00, consumed_samples=800.0, train_step_timing in s=4.920, val_loss=1.650]\n",
      "[NeMo I 2024-12-06 08:03:33 perf_metrics_utils:42] train_step_timing in s: [4.87, 4.91, 4.91, 4.92, 4.92, 4.88, 4.91, 4.91, 4.91, 4.92]\n",
      "[NeMo I 2024-12-06 08:03:33 perf_metrics:86] TFLOPs per sec per GPU=193.40\n",
      "[NeMo I 2024-12-06 08:03:33 dist_ckpt_io:421] Using TorchDistSaveShardedStrategy(torch_dist, 1) dist-ckpt save strategy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[NeMo W 2024-12-06 08:04:16 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpv4u2rirc'>\n",
      "      _warnings.warn(warn_message, ResourceWarning)\n",
      "    \n",
      "[NeMo W 2024-12-06 08:04:16 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpx6zvs5dp'>\n",
      "      _warnings.warn(warn_message, ResourceWarning)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "MODEL_NAME=Llama-3.1-8B\n",
    "MODEL=Llama-3.1-8B-Instruct.nemo\n",
    "NUM_GPUS=4\n",
    "MAX_STEPS=100\n",
    "MBS=1\n",
    "GBS=8\n",
    "TP=4\n",
    "PP=1\n",
    "CP=1\n",
    "LR=1e-4\n",
    "DATA_SPLITS=\\'9990,8,2\\'\n",
    "DATA_PREFIX=[1.0,data/custom_dataset/preprocessed/wikinews_text_document]\n",
    "export CUDA_DEVICE_MAX_CONNECTIONS=1\n",
    "\n",
    "python /opt/NeMo/examples/nlp/language_modeling/megatron_gpt_pretraining.py \\\n",
    "--config-path=/opt/NeMo-Framework-Launcher/launcher_scripts/conf/training/llama --config-name=llama3_1_8b \\\n",
    "+base_results_dir=results \\\n",
    "trainer.num_nodes=1 \\\n",
    "trainer.devices=$NUM_GPUS \\\n",
    "trainer.max_steps=$MAX_STEPS \\\n",
    "trainer.limit_val_batches=1 \\\n",
    "trainer.val_check_interval=50 \\\n",
    "exp_manager.explicit_log_dir=/workspace/results/$MODEL_NAME/pretrain \\\n",
    "exp_manager.wandb_logger_kwargs.name=$MODEL_NAME \\\n",
    "exp_manager.resume_if_exists=True \\\n",
    "exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True \\\n",
    "exp_manager.checkpoint_callback_params.model_parallel_size=$(($TP*$PP)) \\\n",
    "model.micro_batch_size=$MBS \\\n",
    "model.global_batch_size=$GBS \\\n",
    "model.tensor_model_parallel_size=$TP \\\n",
    "model.pipeline_model_parallel_size=$PP \\\n",
    "model.context_parallel_size=$CP \\\n",
    "model.init_method_std=0.02 \\\n",
    "model.optim.lr=$LR \\\n",
    "model.data.splits_string=${DATA_SPLITS} \\\n",
    "model.data.data_prefix=${DATA_PREFIX} \\\n",
    "model.data.num_workers=0 \\\n",
    "+model.restore_from_path=$MODEL \\\n",
    "+model.rotary_base=500000.0 \\\n",
    "+model.seq_len_interpolation_factor=8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60bdaf30",
   "metadata": {},
   "source": [
    "## 2. Instruction Tuning <a name='s2'></a>\n",
    "\n",
    "We will be using the [erhwenkuo/alpaca-data-gpt4-chinese-zhtw](https://huggingface.co/datasets/erhwenkuo/alpaca-data-gpt4-chinese-zhtw) is a dataset that contains Chinese (zh-tw) Instruction-Following generated by GPT-4 using Alpaca prompts for fine-tuning LLMs.\n",
    "\n",
    "The dataset was originaly shared in [this repository](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM). This dataset is a translation from English to Chinese.\n",
    "\n",
    "### 2.1 Download dataset: erhwenkuo/alpaca-data-gpt4-chinese-zhtw <a name='s2.1'></a>\n",
    "Let's download dataset and save it as json first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46e8df35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 52049/52049 [00:00<00:00, 426102.45 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('erhwenkuo/alpaca-data-gpt4-chinese-zhtw')['train']\n",
    "output_path = 'data/alpaca/gpt4-chinese-zhtw.jsonl'\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    for human_instruction, human_input, assistant_output in zip(dataset['instruction'], dataset['input'], dataset['output']):\n",
    "        f.write(json.dumps({'input': '\\n'.join([human_instruction.strip(),human_input.strip()]).strip(), 'output': assistant_output.strip()}, ensure_ascii=False)+ '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74f42d91",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input\": \"給出三個保持健康的小貼士。\", \"output\": \"1. 飲食要均衡且富有營養：確保你的餐食包含各種水果、蔬菜、瘦肉、全穀物和健康脂肪。這有助於為身體提供必要的營養，使其發揮最佳功能，並有助於預防慢性疾病。2. 經常參加體育鍛煉：鍛鍊對於保持強壯的骨骼、肌肉和心血管健康至關重要。每週至少要進行150分鐘的中等有氧運動或75分鐘的劇烈運動。3. 獲得足夠的睡眠：獲得足夠的高質量睡眠對身體和心理健康至關重要。它有助於調節情緒，提高認知功能，並支援健康的生長和免疫功能。每晚睡眠目標為7-9小時。\"}\n"
     ]
    }
   ],
   "source": [
    "!head -n 1 data/alpaca/gpt4-chinese-zhtw.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba864f07",
   "metadata": {},
   "source": [
    "### 2.2 Split the data into train, validation and test. <a name='s2.2'></a>\n",
    "\n",
    "Generate the train, test and validation splits- you may use your own script to do this or create a new script and use the following sample split_train_val.py by copying it over in the alpaca directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a058d68a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "input_file = \"data/alpaca/gpt4-chinese-zhtw.jsonl\"\n",
    "training_output_file = \"data/alpaca/training.jsonl\"\n",
    "validation_output_file = \"data/alpaca/validation.jsonl\"\n",
    "test_output_file = \"data/alpaca/test.jsonl\"\n",
    "\n",
    "# Specify the proportion of data for training and validation\n",
    "train_proportion = 0.98\n",
    "validation_proportion = 0.01\n",
    "test_proportion = 0.01\n",
    "\n",
    "# Read the JSONL file and shuffle the JSON objects\n",
    "with open(input_file, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    random.shuffle(lines)\n",
    "\n",
    "# Calculate split indices\n",
    "total_lines = len(lines)\n",
    "train_index = int(total_lines * train_proportion)\n",
    "val_index = int(total_lines * validation_proportion)\n",
    "\n",
    "# Distribute JSON objects into training and validation sets\n",
    "train_data = lines[:train_index]\n",
    "validation_data = lines[train_index:train_index+val_index]\n",
    "test_data = lines[train_index+val_index:]\n",
    "\n",
    "# Write JSON objects to training file\n",
    "with open(training_output_file, \"w\") as f:\n",
    "    for line in train_data:\n",
    "        f.write(line.strip() + \"\\n\")\n",
    "\n",
    "# Write JSON objects to validation file\n",
    "with open(validation_output_file, \"w\") as f:\n",
    "    for line in validation_data:\n",
    "        f.write(line.strip() + \"\\n\")\n",
    "\n",
    "# Write JSON objects to training file\n",
    "with open(test_output_file, \"w\") as f:\n",
    "    for line in test_data:\n",
    "        f.write(line.strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "31cafbce",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"input\": \"從以下列表中，列出三個與健身相關的主題。\\n瑜伽，跑步，舉重\", \"output\": \"從列表中選擇三個與健身相關的話題：瑜伽、跑步和舉重。\"}\n"
     ]
    }
   ],
   "source": [
    "# What the dataset looks like after spliting\n",
    "!head -1 data/alpaca/training.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb1542a",
   "metadata": {},
   "source": [
    "### 2.3 Full parameter fine-tuning  <a name='s2.3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75d9433e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:21:05 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
      "    \n",
      "[NeMo W 2024-12-06 08:21:05 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, grad_output):\n",
      "    \n",
      "[NeMo W 2024-12-06 08:21:05 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:21:05 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, grad_output):\n",
      "    \n",
      "[NeMo W 2024-12-06 08:21:06 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
      "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
      "    \n",
      "[NeMo W 2024-12-06 08:21:07 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
      "        module is deprecated and will be removed in 0.10.0. Please use \n",
      "        'megatron.core.extensions.transformer_engine' instead.\n",
      "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
      "    \n",
      "[NeMo W 2024-12-06 08:21:07 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:21:08 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "      cm = get_cmap(\"Set1\")\n",
      "    \n",
      "[NeMo W 2024-12-06 08:21:09 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
      "    \n",
      "[NeMo W 2024-12-06 08:21:09 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "      other = LooseVersion(other)\n",
      "    \n",
      "[NeMo W 2024-12-06 08:21:09 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:21:09 megatron_gpt_finetuning:56] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2024-12-06 08:21:09 megatron_gpt_finetuning:57] \n",
      "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
      "    trainer:\n",
      "      devices: 4\n",
      "      accelerator: gpu\n",
      "      num_nodes: 1\n",
      "      precision: 16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: null\n",
      "      max_steps: 100\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 1.0\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: /workspace/results/Llama-3.1-8B/SFT\n",
      "      exp_dir: null\n",
      "      name: ${name}\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_${model.data.validation_ds.metric.name}\n",
      "        save_top_k: 1\n",
      "        mode: min\n",
      "        save_nemo_on_train_end: true\n",
      "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        always_save_nemo: false\n",
      "        save_best_model: true\n",
      "      create_early_stopping_callback: true\n",
      "      early_stopping_callback_params:\n",
      "        monitor: val_loss\n",
      "        mode: min\n",
      "        min_delta: 0.001\n",
      "        patience: 10\n",
      "        verbose: true\n",
      "        strict: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 4\n",
      "      pipeline_model_parallel_size: 1\n",
      "      global_batch_size: 16\n",
      "      micro_batch_size: 1\n",
      "      restore_from_path: results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama.nemo\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: false\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: false\n",
      "      sequence_parallel: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: true\n",
      "      gradient_as_bucket_view: false\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      fsdp: false\n",
      "      fsdp_sharding_strategy: full\n",
      "      fsdp_grad_reduce_dtype: fp32\n",
      "      fsdp_sharded_checkpoint: false\n",
      "      fsdp_use_orig_params: false\n",
      "      peft:\n",
      "        peft_scheme: null\n",
      "        restore_from_path: null\n",
      "        adapter_tuning:\n",
      "          type: parallel_adapter\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          norm_position: pre\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          norm_type: mixedfusedlayernorm\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        lora_tuning:\n",
      "          variant: nemo\n",
      "          target_modules:\n",
      "          - attention_qkv\n",
      "          adapter_dim: 32\n",
      "          alpha: ${model.peft.lora_tuning.adapter_dim}\n",
      "          adapter_dropout: 0.0\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        p_tuning:\n",
      "          virtual_tokens: 10\n",
      "          bottleneck_dim: 1024\n",
      "          embedding_dim: 1024\n",
      "          init_std: 0.023\n",
      "        ia3_tuning:\n",
      "          layer_selection: null\n",
      "        selective_tuning:\n",
      "          tunable_base_param_names:\n",
      "          - self_attention\n",
      "          - word_embeddings\n",
      "      data:\n",
      "        train_ds:\n",
      "          file_names:\n",
      "          - data/alpaca/training.jsonl\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: true\n",
      "          num_workers: 0\n",
      "          memmap_workers: 2\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: true\n",
      "          concat_sampling_probabilities:\n",
      "          - 1.0\n",
      "          label_key: output\n",
      "          add_eos: true\n",
      "          add_sep: false\n",
      "          add_bos: false\n",
      "          truncation_field: input\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\\nYou\n",
      "            are a knowledgeable assistant trained to provide accurate and helpful information.\n",
      "            Please respond to the user's queries promptly and politely.<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\\n{input}<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n{output}\n",
      "          truncation_method: right\n",
      "          global_sample_mapping: false\n",
      "        validation_ds:\n",
      "          file_names:\n",
      "          - data/alpaca/validation.jsonl\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          global_sample_mapping: false\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "        test_ds:\n",
      "          file_names:\n",
      "          - data/alpaca/test.jsonl\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          global_sample_mapping: false\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 50\n",
      "          min_lr: 0.0\n",
      "          constant_steps: 0\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:21:09 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py:1451: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "      super().__init__(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:21:09 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n",
      "[NeMo W 2024-12-06 08:21:09 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py:1395: DeprecationWarning: torch.set_autocast_gpu_dtype(dtype) is deprecated. Please use torch.set_autocast_dtype('cuda', dtype) instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:678.)\n",
      "      torch.set_autocast_gpu_dtype(dtype)\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:21:10 exp_manager:400] ExpManager schema\n",
      "[NeMo I 2024-12-06 08:21:10 exp_manager:401] {'explicit_log_dir': None, 'exp_dir': None, 'name': None, 'version': None, 'use_datetime_version': True, 'resume_if_exists': False, 'resume_past_end': False, 'resume_ignore_no_checkpoint': False, 'resume_from_checkpoint': None, 'create_tensorboard_logger': True, 'summary_writer_kwargs': None, 'create_wandb_logger': False, 'wandb_logger_kwargs': None, 'create_mlflow_logger': False, 'mlflow_logger_kwargs': {'experiment_name': None, 'tracking_uri': None, 'tags': None, 'save_dir': './mlruns', 'prefix': '', 'artifact_location': None, 'run_id': None, 'log_model': False}, 'create_dllogger_logger': False, 'dllogger_logger_kwargs': {'verbose': False, 'stdout': False, 'json_file': './dllogger.json'}, 'create_clearml_logger': False, 'clearml_logger_kwargs': {'project': None, 'task': None, 'connect_pytorch': False, 'model_name': None, 'tags': None, 'log_model': False, 'log_cfg': False, 'log_metrics': False}, 'create_neptune_logger': False, 'neptune_logger_kwargs': None, 'create_checkpoint_callback': True, 'checkpoint_callback_params': {'filepath': None, 'dirpath': None, 'filename': None, 'monitor': 'val_loss', 'verbose': True, 'save_last': True, 'save_top_k': 3, 'save_weights_only': False, 'mode': 'min', 'auto_insert_metric_name': True, 'every_n_epochs': 1, 'every_n_train_steps': None, 'train_time_interval': None, 'prefix': None, 'postfix': '.nemo', 'save_best_model': False, 'always_save_nemo': False, 'save_nemo_on_train_end': True, 'model_parallel_size': None, 'save_on_train_epoch_end': False, 'async_save': False}, 'create_early_stopping_callback': False, 'early_stopping_callback_params': {'monitor': 'val_loss', 'mode': 'min', 'min_delta': 0.001, 'patience': 10, 'verbose': True, 'strict': True, 'check_finite': True, 'stopping_threshold': None, 'divergence_threshold': None, 'check_on_train_epoch_end': None, 'log_rank_zero_only': False}, 'create_preemption_callback': True, 'files_to_copy': None, 'log_step_timing': True, 'step_timing_kwargs': {'reduction': 'mean', 'sync_cuda': False, 'buffer_size': 1}, 'log_local_rank_0_only': False, 'log_global_rank_0_only': False, 'disable_validation_on_resume': True, 'ema': {'enable': False, 'decay': 0.999, 'cpu_offload': False, 'validate_original_weights': False, 'every_n_steps': 1}, 'max_time_per_run': None, 'seconds_to_sleep': 5.0, 'create_straggler_detection_callback': False, 'straggler_detection_params': {'report_time_interval': 300.0, 'calc_relative_gpu_perf': True, 'calc_individual_gpu_perf': True, 'num_gpu_perf_scores_to_log': 5, 'gpu_relative_perf_threshold': 0.7, 'gpu_individual_perf_threshold': 0.7, 'stop_if_detected': False}, 'create_fault_tolerance_callback': False, 'fault_tolerance': {'workload_check_interval': 5.0, 'initial_rank_heartbeat_timeout': 3600.0, 'rank_heartbeat_timeout': 2700.0, 'calculate_timeouts': True, 'safety_factor': 5.0, 'rank_termination_signal': <Signals.SIGKILL: 9>, 'log_level': 'INFO', 'max_rank_restarts': 0, 'max_subsequent_job_failures': 0, 'additional_ft_launcher_args': '', 'simulated_fault': None}, 'log_tflops_per_sec_per_gpu': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:21:10 exp_manager:784] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/workspace/results/Llama-3.1-8B/SFT/checkpoints. Training from scratch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:21:10 exp_manager:459] Experiments will be logged at /workspace/results/Llama-3.1-8B/SFT\n",
      "[NeMo I 2024-12-06 08:21:10 exp_manager:1010] TensorboardLogger has been set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:21:10 exp_manager:1139] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 100. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:21:10 exp_manager:593] TFLOPs per sec per GPU will be calculated, conditioned on supported models. Defaults to -1 upon failure.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:21:37 megatron_init:314] Rank 0 has data parallel group : [0]\n",
      "[NeMo I 2024-12-06 08:21:37 megatron_init:320] Rank 0 has combined group of data parallel and context parallel : [0]\n",
      "[NeMo I 2024-12-06 08:21:37 megatron_init:325] All data parallel group ranks with context parallel combined: [[0], [1], [2], [3]]\n",
      "[NeMo I 2024-12-06 08:21:37 megatron_init:328] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-12-06 08:21:37 megatron_init:336] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-12-06 08:21:37 megatron_init:339] All context parallel group ranks: [[0], [1], [2], [3]]\n",
      "[NeMo I 2024-12-06 08:21:37 megatron_init:340] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-12-06 08:21:37 megatron_init:347] Rank 0 has model parallel group: [0, 1, 2, 3]\n",
      "[NeMo I 2024-12-06 08:21:37 megatron_init:348] All model parallel group ranks: [[0, 1, 2, 3]]\n",
      "[NeMo I 2024-12-06 08:21:37 megatron_init:357] Rank 0 has tensor model parallel group: [0, 1, 2, 3]\n",
      "[NeMo I 2024-12-06 08:21:37 megatron_init:361] All tensor model parallel group ranks: [[0, 1, 2, 3]]\n",
      "[NeMo I 2024-12-06 08:21:37 megatron_init:362] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-12-06 08:21:37 megatron_init:382] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-12-06 08:21:37 megatron_init:394] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-12-06 08:21:37 megatron_init:400] All pipeline model parallel group ranks: [[0], [1], [2], [3]]\n",
      "[NeMo I 2024-12-06 08:21:37 megatron_init:401] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-12-06 08:21:37 megatron_init:402] All embedding group ranks: [[0], [1], [2], [3]]\n",
      "[NeMo I 2024-12-06 08:21:37 megatron_init:403] Rank 0 has embedding rank: 0\n",
      "[NeMo I 2024-12-06 08:21:37 num_microbatches_calculator:218] setting number of microbatches to constant 16\n",
      "[NeMo I 2024-12-06 08:21:37 megatron_base_model:985] Gradient accumulation fusion can only be used with megatron amp O2 mixed precision.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:21:37 tokenizer_utils:184] Getting HuggingFace AutoTokenizer with pretrained_model_name: meta-llama/Meta-Llama-3-8B\n",
      "[NeMo I 2024-12-06 08:21:37 megatron_base_model:601] Padded vocab_size: 128512, original vocab_size: 128256, dummy tokens: 256.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: first_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: last_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: tp_only_amax_red in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_router_pre_softmax in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: external_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:37 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: config_logger_dir in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:21:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:718: UserWarning: To guarantee overlapping TP and SP collectives with the backwardGEMMs, set environment variable CUDA_DEVICE_MAX_CONNECTIONS = 1\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply rope scaling ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply rope scaling ...\n",
      "apply rope scaling ...\n",
      "apply rope scaling ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "[rank2]:[W1206 08:22:43.866428893 ProcessGroupGloo.cpp:712] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "[rank3]:[W1206 08:22:45.866524168 ProcessGroupGloo.cpp:712] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "[rank1]:[W1206 08:22:47.418901922 ProcessGroupGloo.cpp:712] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[rank0]:[W1206 08:22:47.422518697 ProcessGroupGloo.cpp:712] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "[NeMo W 2024-12-06 08:23:41 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:755: FutureWarning: `load_state_dict` is deprecated and will be removed in future versions. Please use `load` instead.\n",
      "      checkpoint.load_state_dict(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:23:41 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/distributed/checkpoint/planner_helpers.py:311: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n",
      "      device = getattr(value, \"device\", None)\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:23:53 nlp_overrides:1358] Model MegatronGPTSFTModel was successfully restored from /workspace/results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama.nemo.\n",
      "[NeMo I 2024-12-06 08:23:53 megatron_gpt_finetuning:75] Running full finetuning since no peft scheme is given.\n",
      "      | Name  | Type     | Params | Mode \n",
      "    -------------------------------------------\n",
      "    0 | model | GPTModel | 2.0 B  | train\n",
      "    -------------------------------------------\n",
      "    2.0 B     Trainable params\n",
      "    0         Non-trainable params\n",
      "    2.0 B     Total params\n",
      "    8,033.157 Total estimated model params size (MB)\n",
      "    649       Modules in train mode\n",
      "    0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:23:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:161: You have overridden `MegatronGPTSFTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.\n",
      "    \n",
      "[NeMo W 2024-12-06 08:23:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:143: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.\n",
      "    \n",
      "[NeMo W 2024-12-06 08:23:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:23:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _descriptor.FieldDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:23:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:23:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:23:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _descriptor.EnumValueDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:23:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _DATATYPE = _descriptor.EnumDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:23:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:23:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _descriptor.FieldDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:23:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:23:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:23:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _descriptor.FieldDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:23:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _TENSORPROTO = _descriptor.Descriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:23:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:23:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:35: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _descriptor.EnumValueDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:23:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:29: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _DATACLASS = _descriptor.EnumDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:23:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:74: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _descriptor.FieldDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:23:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:67: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _SUMMARYDESCRIPTION = _descriptor.Descriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:23:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:326: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "      np.bool8: (False, True),\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:23:53 megatron_gpt_sft_model:836] Building GPT SFT validation datasets.\n",
      "[NeMo I 2024-12-06 08:23:53 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-12-06 08:23:53 text_memmap_dataset:527] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:23:53 text_memmap_dataset:493] Building indexing for fn = data/alpaca/validation.jsonl\n",
      "[NeMo I 2024-12-06 08:23:53 text_memmap_dataset:505] Saving idx file = data/alpaca/validation.jsonl.idx.npy\n",
      "[NeMo I 2024-12-06 08:23:53 text_memmap_dataset:507] Saving metadata file = data/alpaca/validation.jsonl.idx.info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:23:53 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/text_memmap_dataset.py:508: ResourceWarning: unclosed file <_io.BufferedWriter name='data/alpaca/validation.jsonl.idx.info'>\n",
      "      pickle.dump(data, open(idx_fn + \".info\", \"wb\"))\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:23:53 text_memmap_dataset:542] Time building 1 / 1 mem-mapped files: 0:00:00.085171\n",
      "[NeMo I 2024-12-06 08:23:53 text_memmap_dataset:527] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:23:53 text_memmap_dataset:542] Time building 0 / 1 mem-mapped files: 0:00:00.078167\n",
      "[NeMo I 2024-12-06 08:23:53 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-12-06 08:23:53 text_memmap_dataset:249] Loading data/alpaca/validation.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:23:53 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/text_memmap_dataset.py:263: ResourceWarning: unclosed file <_io.BufferedReader name='data/alpaca/validation.jsonl.idx.info'>\n",
      "      idx_info_dict = pickle.load(open(idx_fn + \".info\", \"rb\"))\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:23:53 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001086\n",
      "[NeMo I 2024-12-06 08:23:53 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2024-12-06 08:23:53 megatron_gpt_sft_model:840] Length of val dataset: 520\n",
      "[NeMo I 2024-12-06 08:23:53 megatron_gpt_sft_model:828] Building GPT SFT test datasets.\n",
      "[NeMo I 2024-12-06 08:23:53 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-12-06 08:23:53 text_memmap_dataset:527] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:23:53 text_memmap_dataset:493] Building indexing for fn = data/alpaca/test.jsonl\n",
      "[NeMo I 2024-12-06 08:23:53 text_memmap_dataset:505] Saving idx file = data/alpaca/test.jsonl.idx.npy\n",
      "[NeMo I 2024-12-06 08:23:53 text_memmap_dataset:507] Saving metadata file = data/alpaca/test.jsonl.idx.info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:23:53 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/text_memmap_dataset.py:508: ResourceWarning: unclosed file <_io.BufferedWriter name='data/alpaca/test.jsonl.idx.info'>\n",
      "      pickle.dump(data, open(idx_fn + \".info\", \"wb\"))\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:23:53 text_memmap_dataset:542] Time building 1 / 1 mem-mapped files: 0:00:00.082728\n",
      "[NeMo I 2024-12-06 08:23:53 text_memmap_dataset:527] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:23:53 text_memmap_dataset:542] Time building 0 / 1 mem-mapped files: 0:00:00.077773\n",
      "[NeMo I 2024-12-06 08:23:53 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-12-06 08:23:53 text_memmap_dataset:249] Loading data/alpaca/test.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:23:53 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/text_memmap_dataset.py:263: ResourceWarning: unclosed file <_io.BufferedReader name='data/alpaca/test.jsonl.idx.info'>\n",
      "      idx_info_dict = pickle.load(open(idx_fn + \".info\", \"rb\"))\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:23:53 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001110\n",
      "[NeMo I 2024-12-06 08:23:53 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2024-12-06 08:23:53 megatron_gpt_sft_model:831] Length of test dataset: 521\n",
      "[NeMo I 2024-12-06 08:23:53 megatron_gpt_sft_model:847] Building GPT SFT traing datasets.\n",
      "[NeMo I 2024-12-06 08:23:53 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-12-06 08:23:53 text_memmap_dataset:527] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:23:53 text_memmap_dataset:493] Building indexing for fn = data/alpaca/training.jsonl\n",
      "[NeMo I 2024-12-06 08:23:53 text_memmap_dataset:505] Saving idx file = data/alpaca/training.jsonl.idx.npy\n",
      "[NeMo I 2024-12-06 08:23:53 text_memmap_dataset:507] Saving metadata file = data/alpaca/training.jsonl.idx.info\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:23:53 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/text_memmap_dataset.py:508: ResourceWarning: unclosed file <_io.BufferedWriter name='data/alpaca/training.jsonl.idx.info'>\n",
      "      pickle.dump(data, open(idx_fn + \".info\", \"wb\"))\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:23:53 text_memmap_dataset:542] Time building 1 / 1 mem-mapped files: 0:00:00.112768\n",
      "[NeMo I 2024-12-06 08:23:53 text_memmap_dataset:527] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:23:54 text_memmap_dataset:542] Time building 0 / 1 mem-mapped files: 0:00:00.107503\n",
      "[NeMo I 2024-12-06 08:23:54 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-12-06 08:23:54 text_memmap_dataset:249] Loading data/alpaca/training.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:23:54 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/text_memmap_dataset.py:263: ResourceWarning: unclosed file <_io.BufferedReader name='data/alpaca/training.jsonl.idx.info'>\n",
      "      idx_info_dict = pickle.load(open(idx_fn + \".info\", \"rb\"))\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:23:54 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001019\n",
      "[NeMo I 2024-12-06 08:23:54 text_memmap_dataset:165] Computing global indices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:23:54 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron/dataset_utils.py:1332: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)\n",
      "      counts = torch.cuda.LongTensor([1])\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Entering directory '/opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "> building indices for blendable datasets ...\n",
      " > sample ratios:\n",
      "   dataset 0, input: 1, achieved: 1\n",
      "[NeMo I 2024-12-06 08:23:54 blendable_dataset:67] > elapsed time for building blendable dataset indices: 0.15 (sec)\n",
      "[NeMo I 2024-12-06 08:23:54 megatron_gpt_sft_model:849] Length of train dataset: 1608\n",
      "[NeMo I 2024-12-06 08:23:54 megatron_gpt_sft_model:854] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-12-06 08:23:54 megatron_gpt_sft_model:854] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-12-06 08:23:54 megatron_gpt_sft_model:854] Building dataloader with consumed samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:23:54 nlp_overrides:274] Configuring DDP for model parallelism.\n",
      "[NeMo I 2024-12-06 08:23:54 modelPT:787] Optimizer config = FusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.98]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        is_expert: False\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2024-12-06 08:23:54 lr_scheduler:948] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7fd245122380>\" \n",
      "    will be used during training (effective maximum steps = 100) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 50\n",
      "    min_lr: 0.0\n",
      "    constant_steps: 0\n",
      "    max_steps: 100\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type     | Params | Mode \n",
      "-------------------------------------------\n",
      "0 | model | GPTModel | 2.0 B  | train\n",
      "-------------------------------------------\n",
      "2.0 B     Trainable params\n",
      "0         Non-trainable params\n",
      "2.0 B     Total params\n",
      "8,033.157 Total estimated model params size (MB)\n",
      "649       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "[NeMo W 2024-12-06 08:23:54 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=62` in the `DataLoader` to improve performance.\n",
      "    \n",
      "[NeMo W 2024-12-06 08:23:54 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s][NeMo I 2024-12-06 08:23:55 num_microbatches_calculator:218] setting number of microbatches to constant 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:23:55 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:23:57 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:23:58 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:23:59 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:23:59 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:23:59 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:636: UserWarning: When using async grad allreduce it is recommended to set the environment variable CUDA_DEVICE_MAX_CONNECTIONS to 1 for maximum speedup\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:01 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:01 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:04 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:05 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:05 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:05 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:08 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:08 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:14<00:00,  0.13it/s][NeMo I 2024-12-06 08:24:10 num_microbatches_calculator:218] setting number of microbatches to constant 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:24:10 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:10 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('validation_loss_dataloader0', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:10 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('validation_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:10 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=62` in the `DataLoader` to improve performance.\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:10 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:11 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:12 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:12 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:13 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:18 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:18 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:21 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:215: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:22 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:25 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:25 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:28 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:28 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:31 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:31 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:34 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:34 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:37 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:37 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[rank2]:W1206 08:24:43.767000 139923277452416 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "[rank2]:W1206 08:24:43.767000 139923277452416 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
      "[rank2]:W1206 08:24:43.767000 139923277452416 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 512, actual 656\n",
      "[rank2]:W1206 08:24:43.767000 139923277452416 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "[rank2]:W1206 08:24:43.767000 139923277452416 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "[rank1]:W1206 08:24:43.767000 140029768479872 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "[rank1]:W1206 08:24:43.767000 140029768479872 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
      "[rank1]:W1206 08:24:43.767000 140029768479872 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 512, actual 656\n",
      "[rank1]:W1206 08:24:43.767000 140029768479872 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "[rank1]:W1206 08:24:43.767000 140029768479872 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "[rank3]:W1206 08:24:43.769000 139667422819456 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "[rank3]:W1206 08:24:43.769000 139667422819456 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
      "[rank3]:W1206 08:24:43.769000 139667422819456 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 512, actual 656\n",
      "[rank3]:W1206 08:24:43.769000 139667422819456 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "[rank3]:W1206 08:24:43.769000 139667422819456 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "[rank0]:W1206 08:24:43.769000 140545736246400 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "[rank0]:W1206 08:24:43.769000 140545736246400 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
      "[rank0]:W1206 08:24:43.769000 140545736246400 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 512, actual 656\n",
      "[rank0]:W1206 08:24:43.769000 140545736246400 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "[rank0]:W1206 08:24:43.769000 140545736246400 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "[NeMo W 2024-12-06 08:24:44 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:44 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:47 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:24:47 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:28:11 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 100%|██████████| 100/100 [04:00<00:00, reduced_train_loss=2.850, global_step=99.00, consumed_samples=1600.0, train_step_timing in s=2.230]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-12-06 08:28:11 num_microbatches_calculator:218] setting number of microbatches to constant 16\n",
      "\n",
      "Validation:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▎         | 1/33 [00:01<00:37,  0.86it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|▌         | 2/33 [00:02<00:33,  0.92it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|▉         | 3/33 [00:03<00:32,  0.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|█▏        | 4/33 [00:04<00:31,  0.93it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|█▌        | 5/33 [00:05<00:29,  0.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 6/33 [00:06<00:28,  0.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██        | 7/33 [00:07<00:27,  0.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|██▍       | 8/33 [00:08<00:26,  0.94it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|██▋       | 9/33 [00:09<00:25,  0.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|███       | 10/33 [00:10<00:24,  0.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|███▎      | 11/33 [00:11<00:23,  0.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▋      | 12/33 [00:12<00:22,  0.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 13/33 [00:13<00:21,  0.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|████▏     | 14/33 [00:14<00:19,  0.95it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|████▌     | 15/33 [00:15<00:18,  0.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████▊     | 16/33 [00:16<00:17,  0.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████▏    | 17/33 [00:17<00:16,  0.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████▍    | 18/33 [00:18<00:15,  0.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████▊    | 19/33 [00:19<00:14,  0.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 20/33 [00:20<00:13,  0.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▎   | 21/33 [00:21<00:12,  0.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████▋   | 22/33 [00:22<00:11,  0.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|██████▉   | 23/33 [00:23<00:10,  0.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████▎  | 24/33 [00:24<00:09,  0.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|███████▌  | 25/33 [00:25<00:08,  0.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▉  | 26/33 [00:27<00:07,  0.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 27/33 [00:28<00:06,  0.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|████████▍ | 28/33 [00:29<00:05,  0.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|████████▊ | 29/33 [00:30<00:04,  0.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|█████████ | 30/33 [00:31<00:03,  0.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|█████████▍| 31/33 [00:32<00:02,  0.96it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|█████████▋| 32/33 [00:33<00:01,  0.96it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 33/33 [00:34<00:00,  0.96it/s]\u001b[A[NeMo I 2024-12-06 08:28:46 num_microbatches_calculator:218] setting number of microbatches to constant 16\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 100: 'validation_loss' reached 2.73181 (best 2.73181), saving model to '/workspace/results/Llama-3.1-8B/SFT/checkpoints/megatron_gpt_peft_None_tuning--validation_loss=2.732-step=100-consumed_samples=1600.0.ckpt' as top 1\n",
      "[NeMo W 2024-12-06 08:28:46 nlp_overrides:610] Distributed checkpoints requires DistributedCheckpointIO plugin to be used. Setting up a default now.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 100%|██████████| 100/100 [04:35<00:00, reduced_train_loss=2.850, global_step=99.00, consumed_samples=1600.0, train_step_timing in s=2.230, val_loss=2.730][NeMo I 2024-12-06 08:28:46 dist_ckpt_io:421] Using TorchDistSaveShardedStrategy(torch_dist, 1) dist-ckpt save strategy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:29:02 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
      "    \n",
      "[NeMo W 2024-12-06 08:29:02 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, grad_output):\n",
      "    \n",
      "[NeMo W 2024-12-06 08:29:02 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:29:02 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, grad_output):\n",
      "    \n",
      "[NeMo W 2024-12-06 08:29:03 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
      "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
      "    \n",
      "[NeMo W 2024-12-06 08:29:03 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
      "        module is deprecated and will be removed in 0.10.0. Please use \n",
      "        'megatron.core.extensions.transformer_engine' instead.\n",
      "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
      "    \n",
      "[NeMo W 2024-12-06 08:29:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:29:05 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "      cm = get_cmap(\"Set1\")\n",
      "    \n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[NeMo W 2024-12-06 08:29:05 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
      "    \n",
      "[NeMo W 2024-12-06 08:29:05 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "      other = LooseVersion(other)\n",
      "    \n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[rank: 0] Metric val_loss improved. New best score: 2.732\n",
      "[rank: 1] Metric val_loss improved. New best score: 2.732\n",
      "[rank: 3] Metric val_loss improved. New best score: 2.732\n",
      "[rank: 2] Metric val_loss improved. New best score: 2.732\n",
      "`Trainer.fit` stopped: `max_steps=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 100%|██████████| 100/100 [06:38<00:00, reduced_train_loss=2.850, global_step=99.00, consumed_samples=1600.0, train_step_timing in s=2.230, val_loss=2.730]\n",
      "[NeMo I 2024-12-06 08:30:49 perf_metrics:86] TFLOPs per sec per GPU=-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo E 2024-12-06 08:30:49 perf_metrics:84] Failed to calculate TFLOPs per sec per GPU.\n",
      "    FLOPs measurement not supported for finetuning jobs\n",
      "Restoring states from the checkpoint path at /workspace/results/Llama-3.1-8B/SFT/checkpoints/megatron_gpt_peft_None_tuning--validation_loss=2.732-step=100-consumed_samples=1600.0\n",
      "[NeMo W 2024-12-06 08:30:49 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:755: FutureWarning: `load_state_dict` is deprecated and will be removed in future versions. Please use `load` instead.\n",
      "      checkpoint.load_state_dict(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:30:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/distributed/checkpoint/planner_helpers.py:311: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n",
      "      device = getattr(value, \"device\", None)\n",
      "    \n",
      "Restored all states from the checkpoint at /workspace/results/Llama-3.1-8B/SFT/checkpoints/megatron_gpt_peft_None_tuning--validation_loss=2.732-step=100-consumed_samples=1600.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:31:01 dist_ckpt_io:421] Using TorchDistSaveShardedStrategy(torch_dist, 1) dist-ckpt save strategy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[NeMo W 2024-12-06 08:32:27 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpfw7tcl_j'>\n",
      "      _warnings.warn(warn_message, ResourceWarning)\n",
      "    \n",
      "[NeMo W 2024-12-06 08:32:27 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp00llgi7c'>\n",
      "      _warnings.warn(warn_message, ResourceWarning)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "MODEL_NAME=Llama-3.1-8B\n",
    "MODEL=results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama.nemo\n",
    "NUM_GPUS=4\n",
    "MAX_STEPS=100\n",
    "VAL_INTERVAL=1.0\n",
    "GBS=16\n",
    "MBS=1\n",
    "TP=4\n",
    "PP=1\n",
    "LR=1e-4\n",
    "SEQ_LEN=8192\n",
    "TRAIN_DS=[data/alpaca/training.jsonl]\n",
    "VALID_DS=[data/alpaca/validation.jsonl]\n",
    "TEST_DS=[data/alpaca/test.jsonl]\n",
    "CONCAT_SAMPLING_PROBS=[1.0]\n",
    "PROMPT_TEMPLATE=\"\\\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\\n",
    "You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\\n\\\n",
    "<|start_header_id|>user<|end_header_id|>\\n\\\n",
    "{input}<|eot_id|>\\n\\\n",
    "<|start_header_id|>assistant<|end_header_id|>\\n\\\n",
    "{output}\\\"\"\n",
    "\n",
    "python /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\n",
    "--config-path=/opt/NeMo/examples/nlp/language_modeling/tuning/conf --config-name=megatron_gpt_finetuning_config \\\n",
    "trainer.devices=$NUM_GPUS \\\n",
    "trainer.max_epochs=null \\\n",
    "trainer.max_steps=$MAX_STEPS \\\n",
    "trainer.val_check_interval=$VAL_INTERVAL \\\n",
    "exp_manager.explicit_log_dir=/workspace/results/$MODEL_NAME/SFT \\\n",
    "exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True \\\n",
    "model.tensor_model_parallel_size=$TP \\\n",
    "model.pipeline_model_parallel_size=$PP \\\n",
    "model.restore_from_path=$MODEL \\\n",
    "model.global_batch_size=$GBS \\\n",
    "model.micro_batch_size=$MBS \\\n",
    "model.data.train_ds.file_names=${TRAIN_DS} \\\n",
    "model.data.validation_ds.file_names=${VALID_DS} \\\n",
    "model.data.test_ds.file_names=${TEST_DS} \\\n",
    "model.data.train_ds.max_seq_length=$SEQ_LEN \\\n",
    "model.data.validation_ds.max_seq_length=$SEQ_LEN \\\n",
    "model.data.test_ds.max_seq_length=$SEQ_LEN \\\n",
    "model.data.train_ds.num_workers=0 \\\n",
    "model.data.validation_ds.num_workers=0 \\\n",
    "model.data.test_ds.num_workers=0 \\\n",
    "model.data.train_ds.concat_sampling_probabilities=${CONCAT_SAMPLING_PROBS} \\\n",
    "model.data.train_ds.prompt_template=\"$PROMPT_TEMPLATE\" \\\n",
    "model.optim.lr=$LR \\\n",
    "model.peft.peft_scheme=null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d361e97e",
   "metadata": {},
   "source": [
    "### 2.4. Parameter Efficient Fine-tuning <a name='s2.4'></a>\n",
    "Fine-tuning language model can be computationally expensive and risk overfitting, especially with small, specialized datasets. Parameter-efficient fine-tuning methods like LoRA offer a solution. These techniques adapt the model to specific tasks by modifying only a subset of parameters, reducing computational costs and mitigating overfitting risks. In essence, LoRA enable a more efficient and targeted adaptation of large language models for specialized tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "77a7fb05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:35:57 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
      "    \n",
      "[NeMo W 2024-12-06 08:35:57 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, grad_output):\n",
      "    \n",
      "[NeMo W 2024-12-06 08:35:57 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:35:57 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, grad_output):\n",
      "    \n",
      "[NeMo W 2024-12-06 08:35:58 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
      "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
      "    \n",
      "[NeMo W 2024-12-06 08:35:58 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
      "        module is deprecated and will be removed in 0.10.0. Please use \n",
      "        'megatron.core.extensions.transformer_engine' instead.\n",
      "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
      "    \n",
      "[NeMo W 2024-12-06 08:35:59 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:36:00 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "      cm = get_cmap(\"Set1\")\n",
      "    \n",
      "[NeMo W 2024-12-06 08:36:00 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
      "    \n",
      "[NeMo W 2024-12-06 08:36:00 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "      other = LooseVersion(other)\n",
      "    \n",
      "[NeMo W 2024-12-06 08:36:01 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:36:01 megatron_gpt_finetuning:56] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2024-12-06 08:36:01 megatron_gpt_finetuning:57] \n",
      "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
      "    trainer:\n",
      "      devices: 4\n",
      "      accelerator: gpu\n",
      "      num_nodes: 1\n",
      "      precision: 16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: null\n",
      "      max_steps: 100\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 1.0\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: /workspace/results/Llama-3.1-8B/PEFT\n",
      "      exp_dir: null\n",
      "      name: ${name}\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_${model.data.validation_ds.metric.name}\n",
      "        save_top_k: 1\n",
      "        mode: min\n",
      "        save_nemo_on_train_end: true\n",
      "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        always_save_nemo: false\n",
      "        save_best_model: true\n",
      "      create_early_stopping_callback: true\n",
      "      early_stopping_callback_params:\n",
      "        monitor: val_loss\n",
      "        mode: min\n",
      "        min_delta: 0.001\n",
      "        patience: 10\n",
      "        verbose: true\n",
      "        strict: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      global_batch_size: 16\n",
      "      micro_batch_size: 1\n",
      "      restore_from_path: results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama.nemo\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: false\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: false\n",
      "      sequence_parallel: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: true\n",
      "      gradient_as_bucket_view: false\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      fsdp: false\n",
      "      fsdp_sharding_strategy: full\n",
      "      fsdp_grad_reduce_dtype: fp32\n",
      "      fsdp_sharded_checkpoint: false\n",
      "      fsdp_use_orig_params: false\n",
      "      peft:\n",
      "        peft_scheme: lora\n",
      "        restore_from_path: null\n",
      "        adapter_tuning:\n",
      "          type: parallel_adapter\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          norm_position: pre\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          norm_type: mixedfusedlayernorm\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        lora_tuning:\n",
      "          variant: nemo\n",
      "          target_modules:\n",
      "          - attention_qkv\n",
      "          adapter_dim: 32\n",
      "          alpha: ${model.peft.lora_tuning.adapter_dim}\n",
      "          adapter_dropout: 0.0\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        p_tuning:\n",
      "          virtual_tokens: 10\n",
      "          bottleneck_dim: 1024\n",
      "          embedding_dim: 1024\n",
      "          init_std: 0.023\n",
      "        ia3_tuning:\n",
      "          layer_selection: null\n",
      "        selective_tuning:\n",
      "          tunable_base_param_names:\n",
      "          - self_attention\n",
      "          - word_embeddings\n",
      "      data:\n",
      "        train_ds:\n",
      "          file_names:\n",
      "          - data/alpaca/training.jsonl\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: true\n",
      "          num_workers: 0\n",
      "          memmap_workers: 2\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: true\n",
      "          concat_sampling_probabilities:\n",
      "          - 1.0\n",
      "          label_key: output\n",
      "          add_eos: true\n",
      "          add_sep: false\n",
      "          add_bos: false\n",
      "          truncation_field: input\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\\nYou\n",
      "            are a knowledgeable assistant trained to provide accurate and helpful information.\n",
      "            Please respond to the user's queries promptly and politely.<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\\n{input}<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n{output}\n",
      "          truncation_method: right\n",
      "          global_sample_mapping: false\n",
      "        validation_ds:\n",
      "          file_names:\n",
      "          - data/alpaca/validation.jsonl\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          global_sample_mapping: false\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "        test_ds:\n",
      "          file_names:\n",
      "          - data/alpaca/test.jsonl\n",
      "          names: null\n",
      "          global_batch_size: ${model.global_batch_size}\n",
      "          micro_batch_size: ${model.micro_batch_size}\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
      "          pin_memory: true\n",
      "          max_seq_length: 2048\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          label_key: ${model.data.train_ds.label_key}\n",
      "          add_eos: ${model.data.train_ds.add_eos}\n",
      "          add_sep: ${model.data.train_ds.add_sep}\n",
      "          add_bos: ${model.data.train_ds.add_bos}\n",
      "          write_predictions_to_file: false\n",
      "          output_file_path_prefix: null\n",
      "          truncation_field: ${model.data.train_ds.truncation_field}\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: ${model.data.train_ds.prompt_template}\n",
      "          tokens_to_generate: 32\n",
      "          truncation_method: right\n",
      "          global_sample_mapping: false\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "      optim:\n",
      "        name: fused_adam\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.01\n",
      "        betas:\n",
      "        - 0.9\n",
      "        - 0.98\n",
      "        sched:\n",
      "          name: CosineAnnealing\n",
      "          warmup_steps: 50\n",
      "          min_lr: 0.0\n",
      "          constant_steps: 0\n",
      "          monitor: val_loss\n",
      "          reduce_on_plateau: false\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:36:01 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py:1451: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "      super().__init__(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:36:01 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n",
      "[NeMo W 2024-12-06 08:36:01 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py:1395: DeprecationWarning: torch.set_autocast_gpu_dtype(dtype) is deprecated. Please use torch.set_autocast_dtype('cuda', dtype) instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:678.)\n",
      "      torch.set_autocast_gpu_dtype(dtype)\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:36:02 exp_manager:400] ExpManager schema\n",
      "[NeMo I 2024-12-06 08:36:02 exp_manager:401] {'explicit_log_dir': None, 'exp_dir': None, 'name': None, 'version': None, 'use_datetime_version': True, 'resume_if_exists': False, 'resume_past_end': False, 'resume_ignore_no_checkpoint': False, 'resume_from_checkpoint': None, 'create_tensorboard_logger': True, 'summary_writer_kwargs': None, 'create_wandb_logger': False, 'wandb_logger_kwargs': None, 'create_mlflow_logger': False, 'mlflow_logger_kwargs': {'experiment_name': None, 'tracking_uri': None, 'tags': None, 'save_dir': './mlruns', 'prefix': '', 'artifact_location': None, 'run_id': None, 'log_model': False}, 'create_dllogger_logger': False, 'dllogger_logger_kwargs': {'verbose': False, 'stdout': False, 'json_file': './dllogger.json'}, 'create_clearml_logger': False, 'clearml_logger_kwargs': {'project': None, 'task': None, 'connect_pytorch': False, 'model_name': None, 'tags': None, 'log_model': False, 'log_cfg': False, 'log_metrics': False}, 'create_neptune_logger': False, 'neptune_logger_kwargs': None, 'create_checkpoint_callback': True, 'checkpoint_callback_params': {'filepath': None, 'dirpath': None, 'filename': None, 'monitor': 'val_loss', 'verbose': True, 'save_last': True, 'save_top_k': 3, 'save_weights_only': False, 'mode': 'min', 'auto_insert_metric_name': True, 'every_n_epochs': 1, 'every_n_train_steps': None, 'train_time_interval': None, 'prefix': None, 'postfix': '.nemo', 'save_best_model': False, 'always_save_nemo': False, 'save_nemo_on_train_end': True, 'model_parallel_size': None, 'save_on_train_epoch_end': False, 'async_save': False}, 'create_early_stopping_callback': False, 'early_stopping_callback_params': {'monitor': 'val_loss', 'mode': 'min', 'min_delta': 0.001, 'patience': 10, 'verbose': True, 'strict': True, 'check_finite': True, 'stopping_threshold': None, 'divergence_threshold': None, 'check_on_train_epoch_end': None, 'log_rank_zero_only': False}, 'create_preemption_callback': True, 'files_to_copy': None, 'log_step_timing': True, 'step_timing_kwargs': {'reduction': 'mean', 'sync_cuda': False, 'buffer_size': 1}, 'log_local_rank_0_only': False, 'log_global_rank_0_only': False, 'disable_validation_on_resume': True, 'ema': {'enable': False, 'decay': 0.999, 'cpu_offload': False, 'validate_original_weights': False, 'every_n_steps': 1}, 'max_time_per_run': None, 'seconds_to_sleep': 5.0, 'create_straggler_detection_callback': False, 'straggler_detection_params': {'report_time_interval': 300.0, 'calc_relative_gpu_perf': True, 'calc_individual_gpu_perf': True, 'num_gpu_perf_scores_to_log': 5, 'gpu_relative_perf_threshold': 0.7, 'gpu_individual_perf_threshold': 0.7, 'stop_if_detected': False}, 'create_fault_tolerance_callback': False, 'fault_tolerance': {'workload_check_interval': 5.0, 'initial_rank_heartbeat_timeout': 3600.0, 'rank_heartbeat_timeout': 2700.0, 'calculate_timeouts': True, 'safety_factor': 5.0, 'rank_termination_signal': <Signals.SIGKILL: 9>, 'log_level': 'INFO', 'max_rank_restarts': 0, 'max_subsequent_job_failures': 0, 'additional_ft_launcher_args': '', 'simulated_fault': None}, 'log_tflops_per_sec_per_gpu': True}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:36:02 exp_manager:784] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/workspace/results/Llama-3.1-8B/PEFT/checkpoints. Training from scratch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:36:02 exp_manager:459] Experiments will be logged at /workspace/results/Llama-3.1-8B/PEFT\n",
      "[NeMo I 2024-12-06 08:36:02 exp_manager:1010] TensorboardLogger has been set up\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:36:02 exp_manager:1139] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 100. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:36:02 exp_manager:593] TFLOPs per sec per GPU will be calculated, conditioned on supported models. Defaults to -1 upon failure.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:36:28 megatron_init:314] Rank 0 has data parallel group : [0, 1, 2, 3]\n",
      "[NeMo I 2024-12-06 08:36:28 megatron_init:320] Rank 0 has combined group of data parallel and context parallel : [0, 1, 2, 3]\n",
      "[NeMo I 2024-12-06 08:36:28 megatron_init:325] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3]]\n",
      "[NeMo I 2024-12-06 08:36:28 megatron_init:328] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-12-06 08:36:28 megatron_init:336] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-12-06 08:36:28 megatron_init:339] All context parallel group ranks: [[0], [1], [2], [3]]\n",
      "[NeMo I 2024-12-06 08:36:28 megatron_init:340] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-12-06 08:36:28 megatron_init:347] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-12-06 08:36:28 megatron_init:348] All model parallel group ranks: [[0], [1], [2], [3]]\n",
      "[NeMo I 2024-12-06 08:36:28 megatron_init:357] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-12-06 08:36:28 megatron_init:361] All tensor model parallel group ranks: [[0], [1], [2], [3]]\n",
      "[NeMo I 2024-12-06 08:36:28 megatron_init:362] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-12-06 08:36:28 megatron_init:382] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-12-06 08:36:28 megatron_init:394] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-12-06 08:36:28 megatron_init:400] All pipeline model parallel group ranks: [[0], [1], [2], [3]]\n",
      "[NeMo I 2024-12-06 08:36:28 megatron_init:401] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-12-06 08:36:28 megatron_init:402] All embedding group ranks: [[0], [1], [2], [3]]\n",
      "[NeMo I 2024-12-06 08:36:28 megatron_init:403] Rank 0 has embedding rank: 0\n",
      "[NeMo I 2024-12-06 08:36:28 num_microbatches_calculator:218] setting number of microbatches to constant 4\n",
      "[NeMo I 2024-12-06 08:36:28 megatron_base_model:975] When not using pipeline model parallel, gradient accumulation fusion can only be used with distributed_fused_adam.\n",
      "[NeMo I 2024-12-06 08:36:28 megatron_base_model:985] Gradient accumulation fusion can only be used with megatron amp O2 mixed precision.\n",
      "[NeMo I 2024-12-06 08:36:28 tokenizer_utils:184] Getting HuggingFace AutoTokenizer with pretrained_model_name: meta-llama/Meta-Llama-3-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:36:28 megatron_base_model:601] Padded vocab_size: 128256, original vocab_size: 128256, dummy tokens: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: first_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: last_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: tp_only_amax_red in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_router_pre_softmax in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: external_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:36:28 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: config_logger_dir in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply rope scaling ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply rope scaling ...\n",
      "apply rope scaling ...\n",
      "apply rope scaling ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "[rank1]:[W1206 08:37:36.682011633 ProcessGroupGloo.cpp:712] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "[rank3]:[W1206 08:37:39.076393917 ProcessGroupGloo.cpp:712] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "[rank2]:[W1206 08:37:39.076768676 ProcessGroupGloo.cpp:712] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[rank0]:[W1206 08:37:39.080341850 ProcessGroupGloo.cpp:712] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "[NeMo W 2024-12-06 08:38:34 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:755: FutureWarning: `load_state_dict` is deprecated and will be removed in future versions. Please use `load` instead.\n",
      "      checkpoint.load_state_dict(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:38:34 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/distributed/checkpoint/planner_helpers.py:311: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n",
      "      device = getattr(value, \"device\", None)\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:38:58 nlp_overrides:1358] Model MegatronGPTSFTModel was successfully restored from /workspace/results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama.nemo.\n",
      "[NeMo I 2024-12-06 08:38:58 megatron_gpt_finetuning:72] Adding adapter weights to the model for PEFT\n",
      "[NeMo I 2024-12-06 08:38:58 nlp_adapter_mixins:249] Before adding PEFT params:\n",
      "      | Name  | Type     | Params | Mode \n",
      "    -------------------------------------------\n",
      "    0 | model | GPTModel | 8.0 B  | train\n",
      "    -------------------------------------------\n",
      "    0         Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,121.045Total estimated model params size (MB)\n",
      "    649       Modules in train mode\n",
      "    0         Modules in eval mode\n",
      "[NeMo I 2024-12-06 08:39:00 nlp_adapter_mixins:254] After adding PEFT params:\n",
      "      | Name  | Type     | Params | Mode \n",
      "    -------------------------------------------\n",
      "    0 | model | GPTModel | 8.0 B  | train\n",
      "    -------------------------------------------\n",
      "    10.5 M    Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,162.988Total estimated model params size (MB)\n",
      "    809       Modules in train mode\n",
      "    0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:39:00 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:161: You have overridden `MegatronGPTSFTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:00 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:143: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:00 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:00 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _descriptor.FieldDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:00 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:00 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:00 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _descriptor.EnumValueDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:00 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _DATATYPE = _descriptor.EnumDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:00 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:00 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _descriptor.FieldDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:00 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:00 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:00 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _descriptor.FieldDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:00 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _TENSORPROTO = _descriptor.Descriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:00 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      DESCRIPTOR = _descriptor.FileDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:00 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:35: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _descriptor.EnumValueDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:00 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:29: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _DATACLASS = _descriptor.EnumDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:00 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:74: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _descriptor.FieldDescriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:00 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:67: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
      "      _SUMMARYDESCRIPTION = _descriptor.Descriptor(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:00 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:326: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "      np.bool8: (False, True),\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:39:00 megatron_gpt_sft_model:836] Building GPT SFT validation datasets.\n",
      "[NeMo I 2024-12-06 08:39:00 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-12-06 08:39:00 text_memmap_dataset:527] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:39:00 text_memmap_dataset:542] Time building 0 / 1 mem-mapped files: 0:00:00.086083\n",
      "[NeMo I 2024-12-06 08:39:00 text_memmap_dataset:527] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:39:00 text_memmap_dataset:542] Time building 0 / 1 mem-mapped files: 0:00:00.093862\n",
      "[NeMo I 2024-12-06 08:39:00 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-12-06 08:39:00 text_memmap_dataset:249] Loading data/alpaca/validation.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:39:00 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/text_memmap_dataset.py:263: ResourceWarning: unclosed file <_io.BufferedReader name='data/alpaca/validation.jsonl.idx.info'>\n",
      "      idx_info_dict = pickle.load(open(idx_fn + \".info\", \"rb\"))\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:39:00 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001234\n",
      "[NeMo I 2024-12-06 08:39:00 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2024-12-06 08:39:00 megatron_gpt_sft_model:840] Length of val dataset: 520\n",
      "[NeMo I 2024-12-06 08:39:00 megatron_gpt_sft_model:828] Building GPT SFT test datasets.\n",
      "[NeMo I 2024-12-06 08:39:00 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-12-06 08:39:00 text_memmap_dataset:527] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:39:00 text_memmap_dataset:542] Time building 0 / 1 mem-mapped files: 0:00:00.089561\n",
      "[NeMo I 2024-12-06 08:39:00 text_memmap_dataset:527] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:39:01 text_memmap_dataset:542] Time building 0 / 1 mem-mapped files: 0:00:00.080832\n",
      "[NeMo I 2024-12-06 08:39:01 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-12-06 08:39:01 text_memmap_dataset:249] Loading data/alpaca/test.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:39:01 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/text_memmap_dataset.py:263: ResourceWarning: unclosed file <_io.BufferedReader name='data/alpaca/test.jsonl.idx.info'>\n",
      "      idx_info_dict = pickle.load(open(idx_fn + \".info\", \"rb\"))\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:39:01 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001131\n",
      "[NeMo I 2024-12-06 08:39:01 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2024-12-06 08:39:01 megatron_gpt_sft_model:831] Length of test dataset: 521\n",
      "[NeMo I 2024-12-06 08:39:01 megatron_gpt_sft_model:847] Building GPT SFT traing datasets.\n",
      "[NeMo I 2024-12-06 08:39:01 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-12-06 08:39:01 text_memmap_dataset:527] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:39:01 text_memmap_dataset:542] Time building 0 / 1 mem-mapped files: 0:00:00.096878\n",
      "[NeMo I 2024-12-06 08:39:01 text_memmap_dataset:527] Processing 1 data files using 2 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:39:01 text_memmap_dataset:542] Time building 0 / 1 mem-mapped files: 0:00:00.111388\n",
      "[NeMo I 2024-12-06 08:39:01 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-12-06 08:39:01 text_memmap_dataset:249] Loading data/alpaca/training.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:39:01 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/text_memmap_dataset.py:263: ResourceWarning: unclosed file <_io.BufferedReader name='data/alpaca/training.jsonl.idx.info'>\n",
      "      idx_info_dict = pickle.load(open(idx_fn + \".info\", \"rb\"))\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:39:01 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001271\n",
      "[NeMo I 2024-12-06 08:39:01 text_memmap_dataset:165] Computing global indices\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:39:01 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron/dataset_utils.py:1332: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)\n",
      "      counts = torch.cuda.LongTensor([1])\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "make: Entering directory '/opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "make: Nothing to be done for 'default'.\n",
      "make: Leaving directory '/opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron'\n",
      "> building indices for blendable datasets ...\n",
      " > sample ratios:\n",
      "   dataset 0, input: 1, achieved: 1\n",
      "[NeMo I 2024-12-06 08:39:02 blendable_dataset:67] > elapsed time for building blendable dataset indices: 0.12 (sec)\n",
      "[NeMo I 2024-12-06 08:39:02 megatron_gpt_sft_model:849] Length of train dataset: 1608\n",
      "[NeMo I 2024-12-06 08:39:02 megatron_gpt_sft_model:854] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-12-06 08:39:02 megatron_gpt_sft_model:854] Building dataloader with consumed samples: 0\n",
      "[NeMo I 2024-12-06 08:39:02 megatron_gpt_sft_model:854] Building dataloader with consumed samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:39:02 nlp_overrides:274] Configuring DDP for model parallelism.\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
      "[NeMo I 2024-12-06 08:39:02 nlp_adapter_mixins:335] Optimizer groups set:\n",
      "      | Name  | Type     | Params | Mode \n",
      "    -------------------------------------------\n",
      "    0 | model | GPTModel | 8.0 B  | train\n",
      "    -------------------------------------------\n",
      "    10.5 M    Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,162.988Total estimated model params size (MB)\n",
      "    809       Modules in train mode\n",
      "    0         Modules in eval mode\n",
      "[NeMo I 2024-12-06 08:39:02 modelPT:787] Optimizer config = FusedAdam (\n",
      "    Parameter Group 0\n",
      "        betas: [0.9, 0.98]\n",
      "        bias_correction: True\n",
      "        eps: 1e-08\n",
      "        lr: 0.0001\n",
      "        weight_decay: 0.01\n",
      "    )\n",
      "[NeMo I 2024-12-06 08:39:02 lr_scheduler:948] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f5a0e4c4f70>\" \n",
      "    will be used during training (effective maximum steps = 100) - \n",
      "    Parameters : \n",
      "    (warmup_steps: 50\n",
      "    min_lr: 0.0\n",
      "    constant_steps: 0\n",
      "    max_steps: 100\n",
      "    )\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name  | Type     | Params | Mode \n",
      "-------------------------------------------\n",
      "0 | model | GPTModel | 8.0 B  | train\n",
      "-------------------------------------------\n",
      "10.5 M    Trainable params\n",
      "8.0 B     Non-trainable params\n",
      "8.0 B     Total params\n",
      "32,162.988Total estimated model params size (MB)\n",
      "809       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "[NeMo W 2024-12-06 08:39:02 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=62` in the `DataLoader` to improve performance.\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:02 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s][NeMo I 2024-12-06 08:39:03 num_microbatches_calculator:218] setting number of microbatches to constant 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:39:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:03 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:05 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:05 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:06 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:06 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:10 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:10 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:11 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:11 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:12 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:12 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:15 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:15 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:12<00:00,  0.16it/s][NeMo I 2024-12-06 08:39:15 num_microbatches_calculator:218] setting number of microbatches to constant 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:39:15 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:15 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('validation_loss_dataloader0', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:15 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('validation_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:16 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=62` in the `DataLoader` to improve performance.\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:16 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:17 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:18 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:18 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:18 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:22 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:24 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:215: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:25 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:25 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:25 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:28 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:28 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:30 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:30 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:31 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:31 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:32 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:32 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:33 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:33 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[rank0]:W1206 08:39:34.500000 140029439964288 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "[rank0]:W1206 08:39:34.500000 140029439964288 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
      "[rank0]:W1206 08:39:34.500000 140029439964288 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 400, actual 496\n",
      "[rank0]:W1206 08:39:34.500000 140029439964288 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "[rank0]:W1206 08:39:34.500000 140029439964288 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "[rank2]:W1206 08:39:34.501000 140296887690368 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "[rank2]:W1206 08:39:34.501000 140296887690368 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
      "[rank2]:W1206 08:39:34.501000 140296887690368 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 512, actual 288\n",
      "[rank2]:W1206 08:39:34.501000 140296887690368 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "[rank2]:W1206 08:39:34.501000 140296887690368 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "[NeMo W 2024-12-06 08:39:35 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:35 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[rank1]:W1206 08:39:35.698000 139696724202624 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "[rank1]:W1206 08:39:35.698000 139696724202624 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
      "[rank1]:W1206 08:39:35.698000 139696724202624 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 336, actual 560\n",
      "[rank1]:W1206 08:39:35.698000 139696724202624 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "[rank1]:W1206 08:39:35.698000 139696724202624 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "[rank3]:W1206 08:39:36.823000 139912104313984 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "[rank3]:W1206 08:39:36.823000 139912104313984 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
      "[rank3]:W1206 08:39:36.823000 139912104313984 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 400, actual 256\n",
      "[rank3]:W1206 08:39:36.823000 139912104313984 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "[rank3]:W1206 08:39:36.823000 139912104313984 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "[NeMo W 2024-12-06 08:39:37 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 08:39:37 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:40:31 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 100%|██████████| 100/100 [01:14<00:00, reduced_train_loss=1.400, global_step=99.00, consumed_samples=1600.0, train_step_timing in s=0.557]\n",
      "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-12-06 08:40:32 num_microbatches_calculator:218] setting number of microbatches to constant 4\n",
      "\n",
      "Validation:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[A\n",
      "Validation DataLoader 0:   3%|▎         | 1/33 [00:00<00:16,  1.96it/s]\u001b[A\n",
      "Validation DataLoader 0:   6%|▌         | 2/33 [00:00<00:13,  2.25it/s]\u001b[A\n",
      "Validation DataLoader 0:   9%|▉         | 3/33 [00:01<00:12,  2.40it/s]\u001b[A\n",
      "Validation DataLoader 0:  12%|█▏        | 4/33 [00:01<00:11,  2.48it/s]\u001b[A\n",
      "Validation DataLoader 0:  15%|█▌        | 5/33 [00:01<00:11,  2.51it/s]\u001b[A\n",
      "Validation DataLoader 0:  18%|█▊        | 6/33 [00:02<00:10,  2.52it/s]\u001b[A\n",
      "Validation DataLoader 0:  21%|██        | 7/33 [00:02<00:10,  2.60it/s]\u001b[A\n",
      "Validation DataLoader 0:  24%|██▍       | 8/33 [00:03<00:09,  2.64it/s]\u001b[A\n",
      "Validation DataLoader 0:  27%|██▋       | 9/33 [00:03<00:08,  2.67it/s]\u001b[A\n",
      "Validation DataLoader 0:  30%|███       | 10/33 [00:03<00:08,  2.68it/s]\u001b[A\n",
      "Validation DataLoader 0:  33%|███▎      | 11/33 [00:04<00:08,  2.72it/s]\u001b[A\n",
      "Validation DataLoader 0:  36%|███▋      | 12/33 [00:04<00:07,  2.75it/s]\u001b[A\n",
      "Validation DataLoader 0:  39%|███▉      | 13/33 [00:04<00:07,  2.77it/s]\u001b[A\n",
      "Validation DataLoader 0:  42%|████▏     | 14/33 [00:05<00:06,  2.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  45%|████▌     | 15/33 [00:05<00:06,  2.79it/s]\u001b[A\n",
      "Validation DataLoader 0:  48%|████▊     | 16/33 [00:05<00:06,  2.80it/s]\u001b[A\n",
      "Validation DataLoader 0:  52%|█████▏    | 17/33 [00:06<00:05,  2.82it/s]\u001b[A\n",
      "Validation DataLoader 0:  55%|█████▍    | 18/33 [00:06<00:05,  2.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  58%|█████▊    | 19/33 [00:06<00:04,  2.84it/s]\u001b[A\n",
      "Validation DataLoader 0:  61%|██████    | 20/33 [00:06<00:04,  2.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  64%|██████▎   | 21/33 [00:07<00:04,  2.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  67%|██████▋   | 22/33 [00:07<00:03,  2.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  70%|██████▉   | 23/33 [00:08<00:03,  2.85it/s]\u001b[A\n",
      "Validation DataLoader 0:  73%|███████▎  | 24/33 [00:08<00:03,  2.86it/s]\u001b[A\n",
      "Validation DataLoader 0:  76%|███████▌  | 25/33 [00:08<00:02,  2.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  79%|███████▉  | 26/33 [00:09<00:02,  2.88it/s]\u001b[A\n",
      "Validation DataLoader 0:  82%|████████▏ | 27/33 [00:09<00:02,  2.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  85%|████████▍ | 28/33 [00:09<00:01,  2.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  88%|████████▊ | 29/33 [00:10<00:01,  2.90it/s]\u001b[A\n",
      "Validation DataLoader 0:  91%|█████████ | 30/33 [00:10<00:01,  2.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  94%|█████████▍| 31/33 [00:10<00:00,  2.89it/s]\u001b[A\n",
      "Validation DataLoader 0:  97%|█████████▋| 32/33 [00:11<00:00,  2.90it/s]\u001b[A\n",
      "Validation DataLoader 0: 100%|██████████| 33/33 [00:11<00:00,  2.91it/s]\u001b[A[NeMo I 2024-12-06 08:40:43 num_microbatches_calculator:218] setting number of microbatches to constant 4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 100: 'validation_loss' reached 1.55920 (best 1.55920), saving model to '/workspace/results/Llama-3.1-8B/PEFT/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=1.559-step=100-consumed_samples=1600.0.ckpt' as top 1\n",
      "[rank: 2] Metric val_loss improved. New best score: 1.559\n",
      "[rank: 3] Metric val_loss improved. New best score: 1.559\n",
      "[rank: 0] Metric val_loss improved. New best score: 1.559\n",
      "[rank: 1] Metric val_loss improved. New best score: 1.559\n",
      "`Trainer.fit` stopped: `max_steps=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: : 100%|██████████| 100/100 [01:32<00:00, reduced_train_loss=1.400, global_step=99.00, consumed_samples=1600.0, train_step_timing in s=0.557, val_loss=1.560]\n",
      "[NeMo I 2024-12-06 08:40:49 perf_metrics:86] TFLOPs per sec per GPU=-1.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo E 2024-12-06 08:40:49 perf_metrics:84] Failed to calculate TFLOPs per sec per GPU.\n",
      "    FLOPs measurement not supported for finetuning jobs\n",
      "Restoring states from the checkpoint path at /workspace/results/Llama-3.1-8B/PEFT/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=1.559-step=100-consumed_samples=1600.0.ckpt\n",
      "Restored all states from the checkpoint at /workspace/results/Llama-3.1-8B/PEFT/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=1.559-step=100-consumed_samples=1600.0.ckpt\n",
      "[NeMo W 2024-12-06 08:41:04 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpky8sq6l4'>\n",
      "      _warnings.warn(warn_message, ResourceWarning)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "MODEL_NAME=Llama-3.1-8B\n",
    "MODEL=results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama.nemo\n",
    "NUM_GPUS=4\n",
    "MAX_STEPS=100\n",
    "VAL_INTERVAL=1.0\n",
    "GBS=16\n",
    "MBS=1\n",
    "TP=4\n",
    "PP=1\n",
    "LR=1e-4\n",
    "SEQ_LEN=8192\n",
    "TRAIN_DS=[data/alpaca/training.jsonl]\n",
    "VALID_DS=[data/alpaca/validation.jsonl]\n",
    "TEST_DS=[data/alpaca/test.jsonl]\n",
    "CONCAT_SAMPLING_PROBS=[1.0]\n",
    "PROMPT_TEMPLATE=\"\\\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\\n",
    "You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\\n\\\n",
    "<|start_header_id|>user<|end_header_id|>\\n\\\n",
    "{input}<|eot_id|>\\n\\\n",
    "<|start_header_id|>assistant<|end_header_id|>\\n\\\n",
    "{output}\\\"\"\n",
    "\n",
    "python /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\n",
    "--config-path=/opt/NeMo/examples/nlp/language_modeling/tuning/conf --config-name=megatron_gpt_finetuning_config \\\n",
    "trainer.devices=$NUM_GPUS \\\n",
    "trainer.max_epochs=null \\\n",
    "trainer.max_steps=$MAX_STEPS \\\n",
    "trainer.val_check_interval=$VAL_INTERVAL \\\n",
    "exp_manager.explicit_log_dir=/workspace/results/$MODEL_NAME/PEFT \\\n",
    "exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True \\\n",
    "model.tensor_model_parallel_size=1 \\\n",
    "model.restore_from_path=$MODEL \\\n",
    "model.global_batch_size=$GBS \\\n",
    "model.micro_batch_size=$MBS \\\n",
    "model.data.train_ds.file_names=${TRAIN_DS} \\\n",
    "model.data.validation_ds.file_names=${VALID_DS} \\\n",
    "model.data.test_ds.file_names=${TEST_DS} \\\n",
    "model.data.train_ds.max_seq_length=$SEQ_LEN \\\n",
    "model.data.validation_ds.max_seq_length=$SEQ_LEN \\\n",
    "model.data.test_ds.max_seq_length=$SEQ_LEN \\\n",
    "model.data.train_ds.num_workers=0 \\\n",
    "model.data.validation_ds.num_workers=0 \\\n",
    "model.data.test_ds.num_workers=0 \\\n",
    "model.data.train_ds.concat_sampling_probabilities=${CONCAT_SAMPLING_PROBS} \\\n",
    "model.data.train_ds.prompt_template=\"$PROMPT_TEMPLATE\" \\\n",
    "model.optim.lr=$LR \\\n",
    "model.peft.peft_scheme=lora \\\n",
    "model.peft.lora_tuning.adapter_dim=32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efcc65a7",
   "metadata": {},
   "source": [
    "## 3 Evaluation <a name='s3'></a>\n",
    "\n",
    "If you want to evaluate an SFT .nemo file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae68bb21",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:43:10 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
      "    \n",
      "[NeMo W 2024-12-06 08:43:10 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, grad_output):\n",
      "    \n",
      "[NeMo W 2024-12-06 08:43:10 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:43:10 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, grad_output):\n",
      "    \n",
      "[NeMo W 2024-12-06 08:43:10 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
      "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
      "    \n",
      "[NeMo W 2024-12-06 08:43:11 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
      "        module is deprecated and will be removed in 0.10.0. Please use \n",
      "        'megatron.core.extensions.transformer_engine' instead.\n",
      "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
      "    \n",
      "[NeMo W 2024-12-06 08:43:11 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:43:12 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "      cm = get_cmap(\"Set1\")\n",
      "    \n",
      "[NeMo W 2024-12-06 08:43:13 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
      "    \n",
      "[NeMo W 2024-12-06 08:43:13 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "      other = LooseVersion(other)\n",
      "    \n",
      "[NeMo W 2024-12-06 08:43:13 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:43:13 megatron_gpt_generate:125] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2024-12-06 08:43:13 megatron_gpt_generate:126] \n",
      "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
      "    trainer:\n",
      "      devices: 4\n",
      "      accelerator: gpu\n",
      "      num_nodes: 1\n",
      "      precision: bf16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 9999\n",
      "      max_steps: 20000\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 200\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: null\n",
      "      name: ${name}\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_${model.data.test_ds.metric.name}\n",
      "        save_top_k: 1\n",
      "        mode: max\n",
      "        save_nemo_on_train_end: true\n",
      "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        always_save_nemo: true\n",
      "        save_best_model: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      global_batch_size: 32\n",
      "      micro_batch_size: 1\n",
      "      restore_from_path: results/Llama-3.1-8B/SFT/checkpoints/megatron_gpt_peft_None_tuning.nemo\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: true\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: true\n",
      "      sequence_parallel: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: true\n",
      "      gradient_as_bucket_view: false\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      peft:\n",
      "        peft_scheme: adapter\n",
      "        restore_from_path: null\n",
      "        restore_from_ckpt:\n",
      "          checkpoint_dir: null\n",
      "          checkpoint_name: null\n",
      "        adapter_tuning:\n",
      "          type: parallel_adapter\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          norm_position: pre\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          norm_type: mixedfusedlayernorm\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        lora_tuning:\n",
      "          variant: nemo\n",
      "          target_modules:\n",
      "          - attention_qkv\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        p_tuning:\n",
      "          virtual_tokens: 10\n",
      "          bottleneck_dim: 1024\n",
      "          embedding_dim: 1024\n",
      "          init_std: 0.023\n",
      "        ia3_tuning:\n",
      "          layer_selection: null\n",
      "      data:\n",
      "        test_ds:\n",
      "          file_names:\n",
      "          - data/alpaca/test.jsonl\n",
      "          names:\n",
      "          - alpaca_test\n",
      "          global_batch_size: 32\n",
      "          micro_batch_size: 1\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          pin_memory: true\n",
      "          max_seq_length: 8192\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          context_key: input\n",
      "          label_key: output\n",
      "          add_eos: true\n",
      "          add_sep: false\n",
      "          add_bos: false\n",
      "          write_predictions_to_file: true\n",
      "          output_file_path_prefix: data/alpaca/prediction\n",
      "          truncation_field: input\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\\nYou\n",
      "            are a knowledgeable assistant trained to provide accurate and helpful information.\n",
      "            Please respond to the user's queries promptly and politely.<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\\n{input}<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n{output}\n",
      "          tokens_to_generate: 128\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "    inference:\n",
      "      greedy: true\n",
      "      top_k: 0\n",
      "      top_p: 0.9\n",
      "      temperature: 1.0\n",
      "      all_probs: false\n",
      "      repetition_penalty: 1.0\n",
      "      min_tokens_to_generate: 0\n",
      "      compute_logprob: false\n",
      "      outfile_path: output.txt\n",
      "      compute_attention_mask: true\n",
      "    server: false\n",
      "    port: 5555\n",
      "    web_server: false\n",
      "    share: true\n",
      "    username: test\n",
      "    password: test2\n",
      "    web_port: 9889\n",
      "    chat: false\n",
      "    chatbot_config:\n",
      "      value: false\n",
      "      attributes:\n",
      "      - name: Quality\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: quality\n",
      "        type: int\n",
      "        default: 4\n",
      "      - name: Toxicity\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: toxcity\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Humor\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: humor\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Creativity\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: creativity\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Violence\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: violence\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Helpfulness\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: helpfulness\n",
      "        type: int\n",
      "        default: 4\n",
      "      - name: Not_Appropriate\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: not_appropriate\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Language\n",
      "        choices:\n",
      "        - ar\n",
      "        - bg\n",
      "        - bn\n",
      "        - ca\n",
      "        - cs\n",
      "        - da\n",
      "        - de\n",
      "        - el\n",
      "        - en\n",
      "        - eo\n",
      "        - es\n",
      "        - eu\n",
      "        - fa\n",
      "        - fi\n",
      "        - fr\n",
      "        - gl\n",
      "        - he\n",
      "        - hu\n",
      "        - id\n",
      "        - it\n",
      "        - ja\n",
      "        - ko\n",
      "        - nb\n",
      "        - nl\n",
      "        - pl\n",
      "        - pt\n",
      "        - ro\n",
      "        - ru\n",
      "        - sk\n",
      "        - sv\n",
      "        - th\n",
      "        - tr\n",
      "        - uk\n",
      "        - vi\n",
      "        - zh\n",
      "        key: lang\n",
      "        type: list\n",
      "        default: en\n",
      "      user: User\n",
      "      assistant: Assistant\n",
      "      system: 'A chat between a curious human and an artificial intelligence assistant.\n",
      "        The assistant gives helpful, detailed, and polite answers to the human''s questions.\n",
      "    \n",
      "    \n",
      "        '\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:43:13 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n",
      "[NeMo W 2024-12-06 08:43:13 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py:1659: DeprecationWarning: torch.set_autocast_gpu_dtype(dtype) is deprecated. Please use torch.set_autocast_dtype('cuda', dtype) instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:678.)\n",
      "      torch.set_autocast_gpu_dtype(dtype)\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:44:08 megatron_init:314] Rank 0 has data parallel group : [0, 1, 2, 3]\n",
      "[NeMo I 2024-12-06 08:44:08 megatron_init:320] Rank 0 has combined group of data parallel and context parallel : [0, 1, 2, 3]\n",
      "[NeMo I 2024-12-06 08:44:08 megatron_init:325] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3]]\n",
      "[NeMo I 2024-12-06 08:44:08 megatron_init:328] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-12-06 08:44:08 megatron_init:336] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-12-06 08:44:08 megatron_init:339] All context parallel group ranks: [[0], [1], [2], [3]]\n",
      "[NeMo I 2024-12-06 08:44:08 megatron_init:340] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-12-06 08:44:08 megatron_init:347] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-12-06 08:44:08 megatron_init:348] All model parallel group ranks: [[0], [1], [2], [3]]\n",
      "[NeMo I 2024-12-06 08:44:08 megatron_init:357] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-12-06 08:44:08 megatron_init:361] All tensor model parallel group ranks: [[0], [1], [2], [3]]\n",
      "[NeMo I 2024-12-06 08:44:08 megatron_init:362] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-12-06 08:44:08 megatron_init:382] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-12-06 08:44:08 megatron_init:394] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-12-06 08:44:08 megatron_init:400] All pipeline model parallel group ranks: [[0], [1], [2], [3]]\n",
      "[NeMo I 2024-12-06 08:44:08 megatron_init:401] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-12-06 08:44:08 megatron_init:402] All embedding group ranks: [[0], [1], [2], [3]]\n",
      "[NeMo I 2024-12-06 08:44:08 megatron_init:403] Rank 0 has embedding rank: 0\n",
      "setting number of microbatches to constant 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:44:08 tokenizer_utils:184] Getting HuggingFace AutoTokenizer with pretrained_model_name: meta-llama/Meta-Llama-3-8B\n",
      "[NeMo I 2024-12-06 08:44:08 megatron_base_model:601] Padded vocab_size: 128256, original vocab_size: 128256, dummy tokens: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:516] apply_query_key_layer_scaling is only enabled when using FP16, setting it to False and setting NVTE_APPLY_QK_LAYER_SCALING=0\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: first_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: last_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: tp_only_amax_red in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_router_pre_softmax in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: external_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 08:44:08 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: config_logger_dir in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply rope scaling ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply rope scaling ...\n",
      "apply rope scaling ...\n",
      "apply rope scaling ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "[rank3]:[W1206 08:46:01.443169326 ProcessGroupGloo.cpp:712] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "[rank2]:[W1206 08:46:04.452372283 ProcessGroupGloo.cpp:712] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "[rank1]:[W1206 08:46:06.232547468 ProcessGroupGloo.cpp:712] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[rank0]:[W1206 08:46:06.235800074 ProcessGroupGloo.cpp:712] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "[NeMo W 2024-12-06 08:48:00 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:755: FutureWarning: `load_state_dict` is deprecated and will be removed in future versions. Please use `load` instead.\n",
      "      checkpoint.load_state_dict(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:48:00 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/distributed/checkpoint/planner_helpers.py:311: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n",
      "      device = getattr(value, \"device\", None)\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:48:35 nlp_overrides:1358] Model MegatronGPTSFTModel was successfully restored from /workspace/results/Llama-3.1-8B/SFT/checkpoints/megatron_gpt_peft_None_tuning.nemo.\n",
      "[NeMo I 2024-12-06 08:48:35 megatron_gpt_generate:156] Freezing parameters for PEFT eval:\n",
      "      | Name  | Type          | Params | Mode\n",
      "    -----------------------------------------------\n",
      "    0 | model | Float16Module | 8.0 B  | eval\n",
      "    -----------------------------------------------\n",
      "    0         Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,121.045Total estimated model params size (MB)\n",
      "    0         Modules in train mode\n",
      "    650       Modules in eval mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:48:40 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:161: You have overridden `MegatronGPTSFTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.\n",
      "    \n",
      "[NeMo W 2024-12-06 08:48:40 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:143: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:48:40 megatron_gpt_sft_model:828] Building GPT SFT test datasets.\n",
      "[NeMo I 2024-12-06 08:48:40 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-12-06 08:48:40 text_memmap_dataset:527] Processing 1 data files using 127 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:48:43 text_memmap_dataset:542] Time building 0 / 1 mem-mapped files: 0:00:02.804848\n",
      "[NeMo I 2024-12-06 08:48:43 text_memmap_dataset:527] Processing 1 data files using 127 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:48:46 text_memmap_dataset:542] Time building 0 / 1 mem-mapped files: 0:00:02.956817\n",
      "[NeMo I 2024-12-06 08:48:46 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-12-06 08:48:46 text_memmap_dataset:249] Loading data/alpaca/test.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:48:46 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/text_memmap_dataset.py:263: ResourceWarning: unclosed file <_io.BufferedReader name='data/alpaca/test.jsonl.idx.info'>\n",
      "      idx_info_dict = pickle.load(open(idx_fn + \".info\", \"rb\"))\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 08:48:46 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001262\n",
      "[NeMo I 2024-12-06 08:48:46 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2024-12-06 08:48:46 megatron_gpt_sft_model:831] Length of test dataset: 521\n",
      "[NeMo I 2024-12-06 08:48:46 megatron_gpt_sft_model:854] Building dataloader with consumed samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "[NeMo W 2024-12-06 08:48:46 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=62` in the `DataLoader` to improve performance.\n",
      "    \n",
      "[NeMo W 2024-12-06 08:48:46 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: Found `dataloader_iter` argument in the `test_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: |          | 0/? [00:00<?, ?it/s]setting number of microbatches to constant 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:48:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/selective_scan_interface.py:164: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
      "    \n",
      "[NeMo W 2024-12-06 08:48:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/selective_scan_interface.py:240: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, dout):\n",
      "    \n",
      "[NeMo W 2024-12-06 08:48:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/layer_norm.py:986: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:48:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/layer_norm.py:1045: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, dout, *args):\n",
      "    \n",
      "[NeMo W 2024-12-06 08:48:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm/distributed/tensor_parallel.py:26: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(ctx, x, weight, bias, process_group=None, sequence_parallel=True):\n",
      "    \n",
      "[NeMo W 2024-12-06 08:48:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm/distributed/tensor_parallel.py:62: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, grad_output):\n",
      "    \n",
      "[NeMo W 2024-12-06 08:48:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:758: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n",
      "    \n",
      "[NeMo W 2024-12-06 08:48:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:836: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, dout, *args):\n",
      "    \n",
      "[NeMo W 2024-12-06 08:48:53 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/modules/common/text_generation_utils.py:495: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)\n",
      "      input_info_tensor = torch.cuda.FloatTensor(input_info)\n",
      "    \n",
      "[NeMo W 2024-12-06 08:48:54 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/modules/common/text_generation_utils.py:503: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "      string_tensor = torch.as_tensor(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/17 [00:00<?, ?it/s]setting number of microbatches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:48:56 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 08:48:57 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting number of microbatches to constant 8\n",
      "Testing DataLoader 0:   6%|▌         | 1/17 [00:27<07:12,  0.04it/s]setting number of microbatches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:49:17 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting number of microbatches to constant 8\n",
      "Testing DataLoader 0:  12%|█▏        | 2/17 [00:42<05:18,  0.05it/s]setting number of microbatches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:49:30 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting number of microbatches to constant 8\n",
      "Testing DataLoader 0:  18%|█▊        | 3/17 [00:55<04:19,  0.05it/s]setting number of microbatches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:49:43 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting number of microbatches to constant 8\n",
      "Testing DataLoader 0:  24%|██▎       | 4/17 [01:07<03:39,  0.06it/s]setting number of microbatches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:49:55 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting number of microbatches to constant 8\n",
      "Testing DataLoader 0:  29%|██▉       | 5/17 [01:19<03:10,  0.06it/s]setting number of microbatches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:50:07 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting number of microbatches to constant 8\n",
      "Testing DataLoader 0:  35%|███▌      | 6/17 [01:33<02:51,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 8\n",
      "Testing DataLoader 0:  41%|████      | 7/17 [01:48<02:35,  0.06it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank2]:W1206 08:50:46.855000 139914147329152 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "[rank2]:W1206 08:50:46.855000 139914147329152 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
      "[rank2]:W1206 08:50:46.855000 139914147329152 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 544, actual 736\n",
      "[rank2]:W1206 08:50:46.855000 139914147329152 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "[rank2]:W1206 08:50:46.855000 139914147329152 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:  47%|████▋     | 8/17 [02:00<02:15,  0.07it/s]setting number of microbatches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:50:47 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting number of microbatches to constant 8\n",
      "Testing DataLoader 0:  53%|█████▎    | 9/17 [02:15<02:00,  0.07it/s]setting number of microbatches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank1]:W1206 08:51:13.009000 139950582879360 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "[rank1]:W1206 08:51:13.009000 139950582879360 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
      "[rank1]:W1206 08:51:13.009000 139950582879360 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 272, actual 688\n",
      "[rank1]:W1206 08:51:13.009000 139950582879360 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "[rank1]:W1206 08:51:13.009000 139950582879360 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting number of microbatches to constant 8\n",
      "Testing DataLoader 0:  59%|█████▉    | 10/17 [02:27<01:43,  0.07it/s]setting number of microbatches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank3]:W1206 08:51:25.533000 140031525524608 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "[rank3]:W1206 08:51:25.533000 140031525524608 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
      "[rank3]:W1206 08:51:25.533000 140031525524608 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 352, actual 560\n",
      "[rank3]:W1206 08:51:25.533000 140031525524608 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "[rank3]:W1206 08:51:25.533000 140031525524608 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting number of microbatches to constant 8\n",
      "Testing DataLoader 0:  65%|██████▍   | 11/17 [02:40<01:27,  0.07it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 8\n",
      "Testing DataLoader 0:  71%|███████   | 12/17 [02:53<01:12,  0.07it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 8\n",
      "Testing DataLoader 0:  76%|███████▋  | 13/17 [03:04<00:56,  0.07it/s]setting number of microbatches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:51:55 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting number of microbatches to constant 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:W1206 08:52:08.936000 140025514927232 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "[rank0]:W1206 08:52:08.936000 140025514927232 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
      "[rank0]:W1206 08:52:08.936000 140025514927232 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 768, actual 400\n",
      "[rank0]:W1206 08:52:08.936000 140025514927232 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "[rank0]:W1206 08:52:08.936000 140025514927232 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:  82%|████████▏ | 14/17 [03:22<00:43,  0.07it/s]setting number of microbatches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:52:09 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting number of microbatches to constant 8\n",
      "Testing DataLoader 0:  88%|████████▊ | 15/17 [03:35<00:28,  0.07it/s]setting number of microbatches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:52:24 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting number of microbatches to constant 8\n",
      "Testing DataLoader 0:  94%|█████████▍| 16/17 [03:50<00:14,  0.07it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 8\n",
      "Testing DataLoader 0: 100%|██████████| 17/17 [04:08<00:00,  0.07it/s][NeMo I 2024-12-06 08:52:55 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 給定資料：每月支出：1.住房：$1,500，2.交通：$250，3.娛樂：$400，4.美味品：$400，5.其他支出：$250，總支出：$1,500，收入：$1,000，支出：$1,000，收入：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$ label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 08:52:55 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 給定資料：每月支出：1.住房：$1,500，2.交通：$250，3.娛樂：$400，4.美味品：$400，5.其他支出：$250，總支出：$1,500，收入：$1,000，支出：$1,000，收入：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$ label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 08:52:55 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 給定資料：每月支出：1.住房：$1,500，2.交通：$250，3.娛樂：$400，4.美味品：$400，5.其他支出：$250，總支出：$1,500，收入：$1,000，支出：$1,000，收入：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$ label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 08:52:55 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 給定資料：每月支出：1.住房：$1,500，2.交通：$250，3.娛樂：$400，4.美味品：$400，5.其他支出：$250，總支出：$1,500，收入：$1,000，支出：$1,000，收入：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$ label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 08:52:55 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 給定資料：每月支出：1.住房：$1,500，2.交通：$250，3.娛樂：$400，4.美味品：$400，5.其他支出：$250，總支出：$1,500，收入：$1,000，支出：$1,000，收入：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$ label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 08:52:55 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 給定資料集：“每月總支出：$1,500，交通：$250，娛樂：$400，其他：$600，其他：$250，其他：$250，總支出：$1,500，收入：$1,000，支出：$1,000，收入：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 08:52:55 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 給定資料集：“每月總支出：$1,500，交通：$250，娛樂：$400，其他：$600，其他：$250，其他：$250，總支出：$1,500，收入：$1,000，支出：$1,000，收入：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 08:52:55 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 給定資料集：“每月總支出：$1,500，交通：$250，娛樂：$400，其他：$600，其他：$250，其他：$250，總支出：$1,500，收入：$1,000，支出：$1,000，收入：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 08:52:55 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 給定資料集：“每月總支出：$1,500，交通：$250，娛樂：$400，其他：$600，其他：$250，其他：$250，總支出：$1,500，收入：$1,000，支出：$1,000，收入：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 08:52:55 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 給定資料集：“每月總支出：$1,500，交通：$250，娛樂：$400，其他：$600，其他：$250，其他：$250，總支出：$1,500，收入：$1,000，支出：$1,000，收入：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 08:52:55 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 給定資料集：“每月總支出：$1,500，交通：$250，娛樂：$400，其他：$600，其他：$250，其他：$250，總支出：$1,500，收入：$1,000，支出：$1,000，收入：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 08:52:55 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 給定資料集：“每月總支出：$1,500，交通：$250，娛樂：$400，其他：$600，其他：$250，其他：$250，總支出：$1,500，收入：$1,000，支出：$1,000，收入：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 08:52:55 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 給定資料集：“每月總支出：$1,500，交通：$250，娛樂：$400，其他：$600，其他：$250，其他：$250，總支出：$1,500，收入：$1,000，支出：$1,000，收入：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 08:52:55 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 給定資料集：“每月總支出：$1,500，交通：$250，娛樂：$400，其他：$600，其他：$250，其他：$250，總支出：$1,500，收入：$1,000，支出：$1,000，收入：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 08:52:55 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 給定資料集：“每月總支出：$1,500，交通：$250，娛樂：$400，其他：$600，其他：$250，其他：$250，總支出：$1,500，收入：$1,000，支出：$1,000，收入：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 08:52:55 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 給定資料集：“每月總支出：$1,500，交通：$250，娛樂：$400，其他：$600，其他：$250，其他：$250，總支出：$1,500，收入：$1,000，支出：$1,000，收入：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 08:52:55 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 給定資料集：“每月總支出：$1,500，交通：$250，娛樂：$400，其他：$600，其他：$250，其他：$250，總支出：$1,500，收入：$1,000，支出：$1,000，收入：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 08:52:55 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 給定資料集：“每月總支出：$1,500，交通：$250，娛樂：$400，其他：$600，其他：$250，其他：$250，總支出：$1,500，收入：$1,000，支出：$1,000，收入：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 08:52:55 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 給定資料集：“每月總支出：$1,500，交通：$250，娛樂：$400，其他：$600，其他：$250，其他：$250，總支出：$1,500，收入：$1,000，支出：$1,000，收入：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 08:52:55 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 給定資料集：“每月總支出：$1,500，交通：$250，娛樂：$400，其他：$600，其他：$250，其他：$250，總支出：$1,500，收入：$1,000，支出：$1,000，收入：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 08:52:55 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 給定資料集：“每月總支出：$1,500，交通：$250，娛樂：$400，其他：$600，其他：$250，其他：$250，總支出：$1,500，收入：$1,000，支出：$1,000，收入：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 08:52:55 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 給定資料集：“每月總支出：$1,500，交通：$250，娛樂：$400，其他：$600，其他：$250，其他：$250，總支出：$1,500，收入：$1,000，支出：$1,000，收入：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 08:52:55 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 給定資料集：“每月總支出：$1,500，交通：$250，娛樂：$400，其他：$600，其他：$250，其他：$250，總支出：$1,500，收入：$1,000，支出：$1,000，收入：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1,000，剩下：$1,000，支出：$1 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 08:52:55 megatron_gpt_sft_model:586] Total deduplicated inference data size: 544 to 521\n",
      "[NeMo I 2024-12-06 08:52:55 megatron_gpt_sft_model:737] Predictions saved to data/alpaca/prediction_test_alpaca_test_inputs_preds_labels.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:52:55 megatron_gpt_sft_model:677] No training data found, reconfiguring microbatches based on validation batch sizes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting number of microbatches to constant 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:52:55 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-12-06 08:52:55 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('test_loss_alpaca_test', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-12-06 08:52:55 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('test_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 17/17 [04:08<00:00,  0.07it/s]\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.7225630283355713    \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m  test_loss_alpaca_test  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.7225630283355713    \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   2.7225630283355713    \u001b[0m\u001b[35m \u001b[0m│\n",
      "└───────────────────────────┴───────────────────────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 08:53:03 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpdx1n55ja'>\n",
      "      _warnings.warn(warn_message, ResourceWarning)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "MODEL_NAME=Llama-3.1-8B\n",
    "MODEL=results/Llama-3.1-8B/SFT/checkpoints/megatron_gpt_peft_None_tuning.nemo\n",
    "NUM_GPUS=4\n",
    "TP=1\n",
    "GB=32\n",
    "SEQ_LEN=8192\n",
    "TEST_DS=[data/alpaca/test.jsonl]\n",
    "OUTPUT=data/alpaca/prediction\n",
    "PROMPT_TEMPLATE=\"\\\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\\n",
    "You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\\n\\\n",
    "<|start_header_id|>user<|end_header_id|>\\n\\\n",
    "{input}<|eot_id|>\\n\\\n",
    "<|start_header_id|>assistant<|end_header_id|>\\n\\\n",
    "{output}\\\"\"\n",
    "\n",
    "python /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py \\\n",
    "trainer.precision=bf16 \\\n",
    "trainer.devices=$NUM_GPUS \\\n",
    "model.restore_from_path=$MODEL \\\n",
    "model.global_batch_size=$GB \\\n",
    "model.tensor_model_parallel_size=$TP \\\n",
    "model.pipeline_model_parallel_size=1 \\\n",
    "model.megatron_amp_O2=True \\\n",
    "model.peft.restore_from_path=null \\\n",
    "model.data.test_ds.file_names=$TEST_DS \\\n",
    "model.data.test_ds.names=\\['alpaca_test'] \\\n",
    "model.data.test_ds.global_batch_size=$GB \\\n",
    "model.data.test_ds.tokens_to_generate=128 \\\n",
    "model.data.test_ds.label_key='output' \\\n",
    "model.data.test_ds.add_eos=True \\\n",
    "model.data.test_ds.add_sep=False \\\n",
    "model.data.test_ds.add_bos=False \\\n",
    "model.data.test_ds.max_seq_length=$SEQ_LEN \\\n",
    "model.data.test_ds.truncation_field=\"input\" \\\n",
    "model.data.test_ds.prompt_template=\"$PROMPT_TEMPLATE\" \\\n",
    "model.data.test_ds.write_predictions_to_file=True \\\n",
    "model.data.test_ds.output_file_path_prefix=$OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb57f32b-6f82-47b8-881b-3c1403963973",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def modify_and_overwrite_jsonl(file_path):\n",
    "    data_list = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            data_list.append(data)\n",
    "    \n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        for data in data_list:\n",
    "            json_line = json.dumps(data, ensure_ascii=False) + \"\\n\"\n",
    "            file.write(json_line)\n",
    "\n",
    "file_path = \"data/alpaca/prediction_test_alpaca_test_inputs_preds_labels.jsonl\"\n",
    "modify_and_overwrite_jsonl(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92b2d8f",
   "metadata": {},
   "source": [
    "If you want to evaluate a PEFT Model, you should provide a base GPT model and a PEFT model .nemo file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af7fd0f3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 09:00:36 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
      "    \n",
      "[NeMo W 2024-12-06 09:00:36 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, grad_output):\n",
      "    \n",
      "[NeMo W 2024-12-06 09:00:36 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(\n",
      "    \n",
      "[NeMo W 2024-12-06 09:00:36 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, grad_output):\n",
      "    \n",
      "[NeMo W 2024-12-06 09:00:37 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
      "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
      "    \n",
      "[NeMo W 2024-12-06 09:00:37 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
      "        module is deprecated and will be removed in 0.10.0. Please use \n",
      "        'megatron.core.extensions.transformer_engine' instead.\n",
      "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
      "    \n",
      "[NeMo W 2024-12-06 09:00:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
      "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
      "    \n",
      "[NeMo W 2024-12-06 09:00:39 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
      "      cm = get_cmap(\"Set1\")\n",
      "    \n",
      "[NeMo W 2024-12-06 09:00:39 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
      "    \n",
      "[NeMo W 2024-12-06 09:00:39 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "      other = LooseVersion(other)\n",
      "    \n",
      "[NeMo W 2024-12-06 09:00:40 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
      "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
      "      ret = run_job(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 09:00:40 megatron_gpt_generate:125] \n",
      "    \n",
      "    ************** Experiment configuration ***********\n",
      "[NeMo I 2024-12-06 09:00:40 megatron_gpt_generate:126] \n",
      "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
      "    trainer:\n",
      "      devices: 4\n",
      "      accelerator: gpu\n",
      "      num_nodes: 1\n",
      "      precision: 16\n",
      "      logger: false\n",
      "      enable_checkpointing: false\n",
      "      use_distributed_sampler: false\n",
      "      max_epochs: 9999\n",
      "      max_steps: 20000\n",
      "      log_every_n_steps: 10\n",
      "      val_check_interval: 200\n",
      "      gradient_clip_val: 1.0\n",
      "    exp_manager:\n",
      "      explicit_log_dir: null\n",
      "      exp_dir: null\n",
      "      name: ${name}\n",
      "      create_wandb_logger: false\n",
      "      wandb_logger_kwargs:\n",
      "        project: null\n",
      "        name: null\n",
      "      resume_if_exists: true\n",
      "      resume_ignore_no_checkpoint: true\n",
      "      create_checkpoint_callback: true\n",
      "      checkpoint_callback_params:\n",
      "        monitor: validation_${model.data.test_ds.metric.name}\n",
      "        save_top_k: 1\n",
      "        mode: max\n",
      "        save_nemo_on_train_end: true\n",
      "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
      "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
      "        always_save_nemo: true\n",
      "        save_best_model: false\n",
      "    model:\n",
      "      seed: 1234\n",
      "      tensor_model_parallel_size: 1\n",
      "      pipeline_model_parallel_size: 1\n",
      "      global_batch_size: 32\n",
      "      micro_batch_size: 1\n",
      "      restore_from_path: results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama.nemo\n",
      "      resume_from_checkpoint: null\n",
      "      save_nemo_on_validation_end: true\n",
      "      sync_batch_comm: false\n",
      "      megatron_amp_O2: false\n",
      "      sequence_parallel: false\n",
      "      activations_checkpoint_granularity: null\n",
      "      activations_checkpoint_method: null\n",
      "      activations_checkpoint_num_layers: null\n",
      "      activations_checkpoint_layers_per_pipeline: null\n",
      "      answer_only_loss: true\n",
      "      gradient_as_bucket_view: false\n",
      "      hidden_dropout: 0.0\n",
      "      attention_dropout: 0.0\n",
      "      ffn_dropout: 0.0\n",
      "      peft:\n",
      "        peft_scheme: lora\n",
      "        restore_from_path: results/Llama-3.1-8B/PEFT/checkpoints/megatron_gpt_peft_lora_tuning.nemo\n",
      "        restore_from_ckpt:\n",
      "          checkpoint_dir: null\n",
      "          checkpoint_name: null\n",
      "        adapter_tuning:\n",
      "          type: parallel_adapter\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          norm_position: pre\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          norm_type: mixedfusedlayernorm\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        lora_tuning:\n",
      "          variant: nemo\n",
      "          target_modules:\n",
      "          - attention_qkv\n",
      "          adapter_dim: 32\n",
      "          adapter_dropout: 0.0\n",
      "          column_init_method: xavier\n",
      "          row_init_method: zero\n",
      "          layer_selection: null\n",
      "          weight_tying: false\n",
      "          position_embedding_strategy: null\n",
      "        p_tuning:\n",
      "          virtual_tokens: 10\n",
      "          bottleneck_dim: 1024\n",
      "          embedding_dim: 1024\n",
      "          init_std: 0.023\n",
      "        ia3_tuning:\n",
      "          layer_selection: null\n",
      "      data:\n",
      "        test_ds:\n",
      "          file_names:\n",
      "          - data/alpaca/test.jsonl\n",
      "          names:\n",
      "          - alpaca_test\n",
      "          global_batch_size: 32\n",
      "          micro_batch_size: 1\n",
      "          shuffle: false\n",
      "          num_workers: 0\n",
      "          pin_memory: true\n",
      "          max_seq_length: 8192\n",
      "          min_seq_length: 1\n",
      "          drop_last: false\n",
      "          context_key: input\n",
      "          label_key: output\n",
      "          add_eos: true\n",
      "          add_sep: false\n",
      "          add_bos: false\n",
      "          write_predictions_to_file: true\n",
      "          output_file_path_prefix: data/alpaca/prediction_peft\n",
      "          truncation_field: input\n",
      "          index_mapping_dir: null\n",
      "          prompt_template: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\\nYou\n",
      "            are a knowledgeable assistant trained to provide accurate and helpful information.\n",
      "            Please respond to the user's queries promptly and politely.<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\\n{input}<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n{output}\n",
      "          tokens_to_generate: 128\n",
      "          truncation_method: right\n",
      "          metric:\n",
      "            name: loss\n",
      "            average: null\n",
      "            num_classes: null\n",
      "    inference:\n",
      "      greedy: true\n",
      "      top_k: 0\n",
      "      top_p: 0.9\n",
      "      temperature: 1.0\n",
      "      all_probs: false\n",
      "      repetition_penalty: 1.0\n",
      "      min_tokens_to_generate: 0\n",
      "      compute_logprob: false\n",
      "      outfile_path: output.txt\n",
      "      compute_attention_mask: true\n",
      "    server: false\n",
      "    port: 5555\n",
      "    web_server: false\n",
      "    share: true\n",
      "    username: test\n",
      "    password: test2\n",
      "    web_port: 9889\n",
      "    chat: false\n",
      "    chatbot_config:\n",
      "      value: false\n",
      "      attributes:\n",
      "      - name: Quality\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: quality\n",
      "        type: int\n",
      "        default: 4\n",
      "      - name: Toxicity\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: toxcity\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Humor\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: humor\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Creativity\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: creativity\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Violence\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: violence\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Helpfulness\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: helpfulness\n",
      "        type: int\n",
      "        default: 4\n",
      "      - name: Not_Appropriate\n",
      "        min: 0\n",
      "        max: 4\n",
      "        key: not_appropriate\n",
      "        type: int\n",
      "        default: 0\n",
      "      - name: Language\n",
      "        choices:\n",
      "        - ar\n",
      "        - bg\n",
      "        - bn\n",
      "        - ca\n",
      "        - cs\n",
      "        - da\n",
      "        - de\n",
      "        - el\n",
      "        - en\n",
      "        - eo\n",
      "        - es\n",
      "        - eu\n",
      "        - fa\n",
      "        - fi\n",
      "        - fr\n",
      "        - gl\n",
      "        - he\n",
      "        - hu\n",
      "        - id\n",
      "        - it\n",
      "        - ja\n",
      "        - ko\n",
      "        - nb\n",
      "        - nl\n",
      "        - pl\n",
      "        - pt\n",
      "        - ro\n",
      "        - ru\n",
      "        - sk\n",
      "        - sv\n",
      "        - th\n",
      "        - tr\n",
      "        - uk\n",
      "        - vi\n",
      "        - zh\n",
      "        key: lang\n",
      "        type: list\n",
      "        default: en\n",
      "      user: User\n",
      "      assistant: Assistant\n",
      "      system: 'A chat between a curious human and an artificial intelligence assistant.\n",
      "        The assistant gives helpful, detailed, and polite answers to the human''s questions.\n",
      "    \n",
      "    \n",
      "        '\n",
      "    \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 09:00:40 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py:1451: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "      super().__init__(\n",
      "    \n",
      "[NeMo W 2024-12-06 09:00:40 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
      "    \n",
      "[NeMo W 2024-12-06 09:00:40 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py:1395: DeprecationWarning: torch.set_autocast_gpu_dtype(dtype) is deprecated. Please use torch.set_autocast_dtype('cuda', dtype) instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:678.)\n",
      "      torch.set_autocast_gpu_dtype(dtype)\n",
      "    \n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 09:01:07 megatron_init:314] Rank 0 has data parallel group : [0, 1, 2, 3]\n",
      "[NeMo I 2024-12-06 09:01:07 megatron_init:320] Rank 0 has combined group of data parallel and context parallel : [0, 1, 2, 3]\n",
      "[NeMo I 2024-12-06 09:01:07 megatron_init:325] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3]]\n",
      "[NeMo I 2024-12-06 09:01:07 megatron_init:328] Ranks 0 has data parallel rank: 0\n",
      "[NeMo I 2024-12-06 09:01:07 megatron_init:336] Rank 0 has context parallel group: [0]\n",
      "[NeMo I 2024-12-06 09:01:07 megatron_init:339] All context parallel group ranks: [[0], [1], [2], [3]]\n",
      "[NeMo I 2024-12-06 09:01:07 megatron_init:340] Ranks 0 has context parallel rank: 0\n",
      "[NeMo I 2024-12-06 09:01:07 megatron_init:347] Rank 0 has model parallel group: [0]\n",
      "[NeMo I 2024-12-06 09:01:07 megatron_init:348] All model parallel group ranks: [[0], [1], [2], [3]]\n",
      "[NeMo I 2024-12-06 09:01:07 megatron_init:357] Rank 0 has tensor model parallel group: [0]\n",
      "[NeMo I 2024-12-06 09:01:07 megatron_init:361] All tensor model parallel group ranks: [[0], [1], [2], [3]]\n",
      "[NeMo I 2024-12-06 09:01:07 megatron_init:362] Rank 0 has tensor model parallel rank: 0\n",
      "[NeMo I 2024-12-06 09:01:07 megatron_init:382] Rank 0 has pipeline model parallel group: [0]\n",
      "[NeMo I 2024-12-06 09:01:07 megatron_init:394] Rank 0 has embedding group: [0]\n",
      "[NeMo I 2024-12-06 09:01:07 megatron_init:400] All pipeline model parallel group ranks: [[0], [1], [2], [3]]\n",
      "[NeMo I 2024-12-06 09:01:07 megatron_init:401] Rank 0 has pipeline model parallel rank 0\n",
      "[NeMo I 2024-12-06 09:01:07 megatron_init:402] All embedding group ranks: [[0], [1], [2], [3]]\n",
      "[NeMo I 2024-12-06 09:01:07 megatron_init:403] Rank 0 has embedding rank: 0\n",
      "setting number of microbatches to constant 8\n",
      "[NeMo I 2024-12-06 09:01:07 tokenizer_utils:184] Getting HuggingFace AutoTokenizer with pretrained_model_name: meta-llama/Meta-Llama-3-8B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 09:01:07 megatron_base_model:601] Padded vocab_size: 128256, original vocab_size: 128256, dummy tokens: 0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: first_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: last_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: tp_only_amax_red in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_router_pre_softmax in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: external_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
      "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: config_logger_dir in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply rope scaling ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply rope scaling ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
      "[rank1]:[W1206 09:02:04.152759686 ProcessGroupGloo.cpp:712] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apply rope scaling ...\n",
      "apply rope scaling ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
      "[rank2]:[W1206 09:02:08.040314218 ProcessGroupGloo.cpp:712] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
      "[rank3]:[W1206 09:02:10.199016260 ProcessGroupGloo.cpp:712] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "----------------------------------------------------------------------------------------------------\n",
      "distributed_backend=nccl\n",
      "All distributed processes registered. Starting with 4 processes\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "[rank0]:[W1206 09:02:10.208471371 ProcessGroupGloo.cpp:712] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
      "[NeMo W 2024-12-06 09:03:05 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:755: FutureWarning: `load_state_dict` is deprecated and will be removed in future versions. Please use `load` instead.\n",
      "      checkpoint.load_state_dict(\n",
      "    \n",
      "[NeMo W 2024-12-06 09:03:05 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/distributed/checkpoint/planner_helpers.py:311: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n",
      "      device = getattr(value, \"device\", None)\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 09:03:24 nlp_overrides:1358] Model MegatronGPTSFTModel was successfully restored from /workspace/results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama.nemo.\n",
      "[NeMo I 2024-12-06 09:03:29 nlp_adapter_mixins:249] Before adding PEFT params:\n",
      "      | Name  | Type     | Params | Mode \n",
      "    -------------------------------------------\n",
      "    0 | model | GPTModel | 8.0 B  | train\n",
      "    -------------------------------------------\n",
      "    0         Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,121.045Total estimated model params size (MB)\n",
      "    649       Modules in train mode\n",
      "    0         Modules in eval mode\n",
      "[NeMo I 2024-12-06 09:03:31 nlp_adapter_mixins:254] After adding PEFT params:\n",
      "      | Name  | Type     | Params | Mode \n",
      "    -------------------------------------------\n",
      "    0 | model | GPTModel | 8.0 B  | train\n",
      "    -------------------------------------------\n",
      "    10.5 M    Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,162.988Total estimated model params size (MB)\n",
      "    809       Modules in train mode\n",
      "    0         Modules in eval mode\n",
      "[NeMo I 2024-12-06 09:03:31 megatron_gpt_generate:156] Freezing parameters for PEFT eval:\n",
      "      | Name  | Type     | Params | Mode\n",
      "    ------------------------------------------\n",
      "    0 | model | GPTModel | 8.0 B  | eval\n",
      "    ------------------------------------------\n",
      "    0         Trainable params\n",
      "    8.0 B     Non-trainable params\n",
      "    8.0 B     Total params\n",
      "    32,162.988Total estimated model params size (MB)\n",
      "    0         Modules in train mode\n",
      "    809       Modules in eval mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 09:03:31 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:161: You have overridden `MegatronGPTSFTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.\n",
      "    \n",
      "[NeMo W 2024-12-06 09:03:31 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:143: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 09:03:32 megatron_gpt_sft_model:828] Building GPT SFT test datasets.\n",
      "[NeMo I 2024-12-06 09:03:32 text_memmap_dataset:116] Building data files\n",
      "[NeMo I 2024-12-06 09:03:32 text_memmap_dataset:527] Processing 1 data files using 127 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 09:03:34 text_memmap_dataset:542] Time building 0 / 1 mem-mapped files: 0:00:02.889242\n",
      "[NeMo I 2024-12-06 09:03:35 text_memmap_dataset:527] Processing 1 data files using 127 workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 09:03:38 text_memmap_dataset:542] Time building 0 / 1 mem-mapped files: 0:00:03.145660\n",
      "[NeMo I 2024-12-06 09:03:38 text_memmap_dataset:158] Loading data files\n",
      "[NeMo I 2024-12-06 09:03:38 text_memmap_dataset:249] Loading data/alpaca/test.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 09:03:38 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/text_memmap_dataset.py:263: ResourceWarning: unclosed file <_io.BufferedReader name='data/alpaca/test.jsonl.idx.info'>\n",
      "      idx_info_dict = pickle.load(open(idx_fn + \".info\", \"rb\"))\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-06 09:03:38 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001094\n",
      "[NeMo I 2024-12-06 09:03:38 text_memmap_dataset:165] Computing global indices\n",
      "[NeMo I 2024-12-06 09:03:38 megatron_gpt_sft_model:831] Length of test dataset: 521\n",
      "[NeMo I 2024-12-06 09:03:38 megatron_gpt_sft_model:854] Building dataloader with consumed samples: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
      "[NeMo W 2024-12-06 09:03:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=62` in the `DataLoader` to improve performance.\n",
      "    \n",
      "[NeMo W 2024-12-06 09:03:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: Found `dataloader_iter` argument in the `test_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing: |          | 0/? [00:00<?, ?it/s]setting number of microbatches to constant 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 09:03:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 09:03:39 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 09:03:40 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 09:03:41 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 09:03:44 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 09:03:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/selective_scan_interface.py:164: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
      "    \n",
      "[NeMo W 2024-12-06 09:03:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/selective_scan_interface.py:240: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, dout):\n",
      "    \n",
      "[NeMo W 2024-12-06 09:03:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/layer_norm.py:986: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(\n",
      "    \n",
      "[NeMo W 2024-12-06 09:03:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/layer_norm.py:1045: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, dout, *args):\n",
      "    \n",
      "[NeMo W 2024-12-06 09:03:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm/distributed/tensor_parallel.py:26: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(ctx, x, weight, bias, process_group=None, sequence_parallel=True):\n",
      "    \n",
      "[NeMo W 2024-12-06 09:03:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm/distributed/tensor_parallel.py:62: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, grad_output):\n",
      "    \n",
      "[NeMo W 2024-12-06 09:03:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:758: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
      "      def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n",
      "    \n",
      "[NeMo W 2024-12-06 09:03:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:836: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
      "      def backward(ctx, dout, *args):\n",
      "    \n",
      "[NeMo W 2024-12-06 09:03:45 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/modules/common/text_generation_utils.py:495: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)\n",
      "      input_info_tensor = torch.cuda.FloatTensor(input_info)\n",
      "    \n",
      "[NeMo W 2024-12-06 09:03:45 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/modules/common/text_generation_utils.py:503: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
      "      string_tensor = torch.as_tensor(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   0%|          | 0/17 [00:00<?, ?it/s]setting number of microbatches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 09:03:46 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 09:03:46 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 09:03:47 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 09:03:48 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
      "      warnings.warn(\n",
      "    \n",
      "[NeMo W 2024-12-06 09:03:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 09:03:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 09:03:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting number of microbatches to constant 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 09:04:10 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 09:04:11 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 09:04:13 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:   6%|▌         | 1/17 [00:31<08:31,  0.03it/s]setting number of microbatches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 09:04:14 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 09:04:15 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n",
      "[NeMo W 2024-12-06 09:04:15 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting number of microbatches to constant 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 09:04:30 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:  12%|█▏        | 2/17 [00:51<06:24,  0.04it/s]setting number of microbatches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 09:04:32 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting number of microbatches to constant 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 09:04:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:  18%|█▊        | 3/17 [01:10<05:28,  0.04it/s]setting number of microbatches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 09:04:50 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting number of microbatches to constant 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 09:05:05 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:  24%|██▎       | 4/17 [01:26<04:40,  0.05it/s]setting number of microbatches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 09:05:05 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting number of microbatches to constant 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 09:05:20 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:  29%|██▉       | 5/17 [01:41<04:03,  0.05it/s]setting number of microbatches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 09:05:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting number of microbatches to constant 8\n",
      "Testing DataLoader 0:  35%|███▌      | 6/17 [02:01<03:42,  0.05it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 8\n",
      "Testing DataLoader 0:  41%|████      | 7/17 [02:21<03:21,  0.05it/s]setting number of microbatches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank2]:W1206 09:06:15.180000 139969535759488 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "[rank2]:W1206 09:06:15.180000 139969535759488 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
      "[rank2]:W1206 09:06:15.180000 139969535759488 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 544, actual 736\n",
      "[rank2]:W1206 09:06:15.180000 139969535759488 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "[rank2]:W1206 09:06:15.180000 139969535759488 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting number of microbatches to constant 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 09:06:16 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:  47%|████▋     | 8/17 [02:37<02:57,  0.05it/s]setting number of microbatches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 09:06:16 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting number of microbatches to constant 8\n",
      "Testing DataLoader 0:  53%|█████▎    | 9/17 [02:57<02:37,  0.05it/s]setting number of microbatches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank1]:W1206 09:06:50.536000 140583929025664 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "[rank1]:W1206 09:06:50.536000 140583929025664 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
      "[rank1]:W1206 09:06:50.536000 140583929025664 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 272, actual 688\n",
      "[rank1]:W1206 09:06:50.536000 140583929025664 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "[rank1]:W1206 09:06:50.536000 140583929025664 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting number of microbatches to constant 8\n",
      "Testing DataLoader 0:  59%|█████▉    | 10/17 [03:14<02:16,  0.05it/s]setting number of microbatches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank3]:W1206 09:07:07.066000 140178608280704 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "[rank3]:W1206 09:07:07.066000 140178608280704 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
      "[rank3]:W1206 09:07:07.066000 140178608280704 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 352, actual 560\n",
      "[rank3]:W1206 09:07:07.066000 140178608280704 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "[rank3]:W1206 09:07:07.066000 140178608280704 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting number of microbatches to constant 8\n",
      "Testing DataLoader 0:  65%|██████▍   | 11/17 [03:32<01:55,  0.05it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 8\n",
      "Testing DataLoader 0:  71%|███████   | 12/17 [03:51<01:36,  0.05it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 09:07:44 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:  76%|███████▋  | 13/17 [04:06<01:15,  0.05it/s]setting number of microbatches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 09:07:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting number of microbatches to constant 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank0]:W1206 09:08:06.010000 139846295696512 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
      "[rank0]:W1206 09:08:06.010000 139846295696512 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
      "[rank0]:W1206 09:08:06.010000 139846295696512 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 768, actual 400\n",
      "[rank0]:W1206 09:08:06.010000 139846295696512 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "[rank0]:W1206 09:08:06.010000 139846295696512 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "[NeMo W 2024-12-06 09:08:06 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:  82%|████████▏ | 14/17 [04:27<00:57,  0.05it/s]setting number of microbatches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 09:08:07 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting number of microbatches to constant 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 09:08:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
      "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0:  88%|████████▊ | 15/17 [04:43<00:37,  0.05it/s]setting number of microbatches to constant 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 09:08:25 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
      "      warnings.warn(\n",
      "    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "setting number of microbatches to constant 8\n",
      "Testing DataLoader 0:  94%|█████████▍| 16/17 [05:02<00:18,  0.05it/s]setting number of microbatches to constant 1\n",
      "setting number of microbatches to constant 8\n",
      "Testing DataLoader 0: 100%|██████████| 17/17 [05:16<00:00,  0.05it/s][NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
      "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
      "    <|start_header_id|>user<|end_header_id|>\n",
      "    給定一份財務資料樣本，計算每月總支出。\n",
      "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
      "    <|start_header_id|>assistant<|end_header_id|>\n",
      "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
      "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:586] Total deduplicated inference data size: 544 to 521\n",
      "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:737] Predictions saved to data/alpaca/prediction_peft_test_alpaca_test_inputs_preds_labels.jsonl\n",
      "setting number of microbatches to constant 8\n",
      "Testing DataLoader 0: 100%|██████████| 17/17 [05:23<00:00,  0.05it/s]\n",
      "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
      "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
      "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
      "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.5607960224151611    \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m  test_loss_alpaca_test  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.5607960224151611    \u001b[0m\u001b[35m \u001b[0m│\n",
      "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.5607960224151611    \u001b[0m\u001b[35m \u001b[0m│\n",
      "└───────────────────────────┴───────────────────────────┘\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NeMo W 2024-12-06 09:09:01 megatron_gpt_sft_model:677] No training data found, reconfiguring microbatches based on validation batch sizes.\n",
      "[NeMo W 2024-12-06 09:09:01 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-12-06 09:09:01 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('test_loss_alpaca_test', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-12-06 09:09:01 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('test_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
      "    \n",
      "[NeMo W 2024-12-06 09:09:15 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp6uivhzmw'>\n",
      "      _warnings.warn(warn_message, ResourceWarning)\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "\n",
    "MODEL_NAME=Llama-3.1-8B\n",
    "MODEL=results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama.nemo\n",
    "PEFT_MODEL=results/Llama-3.1-8B/PEFT/checkpoints/megatron_gpt_peft_lora_tuning.nemo\n",
    "NUM_GPUS=4\n",
    "GB=32\n",
    "SEQ_LEN=8192\n",
    "TEST_DS=[data/alpaca/test.jsonl]\n",
    "OUTPUT=data/alpaca/prediction_peft\n",
    "PROMPT_TEMPLATE=\"\\\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\\n",
    "You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\\n\\\n",
    "<|start_header_id|>user<|end_header_id|>\\n\\\n",
    "{input}<|eot_id|>\\n\\\n",
    "<|start_header_id|>assistant<|end_header_id|>\\n\\\n",
    "{output}\\\"\"\n",
    "\n",
    "python /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py \\\n",
    "model.restore_from_path=$MODEL \\\n",
    "model.peft.restore_from_path=$PEFT_MODEL \\\n",
    "model.peft.peft_scheme=lora \\\n",
    "trainer.devices=$NUM_GPUS \\\n",
    "model.global_batch_size=$GB \\\n",
    "model.data.test_ds.file_names=$TEST_DS \\\n",
    "model.data.test_ds.names=\\['alpaca_test'] \\\n",
    "model.data.test_ds.global_batch_size=$GB \\\n",
    "model.data.test_ds.tokens_to_generate=128 \\\n",
    "model.data.test_ds.label_key='output' \\\n",
    "model.data.test_ds.add_eos=True \\\n",
    "model.data.test_ds.add_sep=False \\\n",
    "model.data.test_ds.add_bos=False \\\n",
    "model.data.test_ds.max_seq_length=$SEQ_LEN \\\n",
    "model.data.test_ds.truncation_field=\"input\" \\\n",
    "model.data.test_ds.prompt_template=\"$PROMPT_TEMPLATE\" \\\n",
    "model.data.test_ds.write_predictions_to_file=True \\\n",
    "model.data.test_ds.output_file_path_prefix=$OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9a27d877",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = \"data/alpaca/prediction_peft_test_alpaca_test_inputs_preds_labels.jsonl\"\n",
    "modify_and_overwrite_jsonl(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ce2655-fa5d-4cf9-99ce-2b2545e830fe",
   "metadata": {},
   "source": [
    "## 4. Export and Deploy a NeMo Checkpoint to TensorRT-LLM <a name='s4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1cc9bc-4121-406c-bd89-0b7d4837a565",
   "metadata": {},
   "source": [
    "Open a terminal and run the following code:\n",
    "\n",
    "```sh\n",
    "python /opt/NeMo/scripts/deploy/nlp/deploy_triton.py \\\n",
    "--nemo_checkpoint results/Llama-3.1-8B/SFT/checkpoints/megatron_gpt_peft_None_tuning.nemo \\\n",
    "--model_type llama \\\n",
    "--dtype bfloat16 \\\n",
    "--triton_model_name Llama\n",
    "```\n",
    "\n",
    "The command above launches a inference server. Keep it running and run the following cell to send a request to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dc111b-b740-4aaa-aa18-1c0638bed8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "PROMPT_TEMPLATE=\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\\n",
    "You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\\n\\\n",
    "<|start_header_id|>user<|end_header_id|>\\n\\\n",
    "{input}<|eot_id|>\\n\\\n",
    "<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
    "\n",
    "INPUT=\"今天天氣好嗎?\"\n",
    "\n",
    "PROMPT=\"${PROMPT_TEMPLATE//\\{input\\}/$INPUT}\"\n",
    "\n",
    "python /opt/NeMo/scripts/deploy/nlp/query.py \\\n",
    "--url \"http://localhost:8000\" \\\n",
    "--model_name Llama \\\n",
    "--prompt \"$PROMPT\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f086ab6-476b-41bd-bd52-e0ffdcc7f66d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1e7f62-ba35-4285-a338-abe809766713",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
