{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "9408f98a",
      "metadata": {
        "id": "9408f98a"
      },
      "source": [
        "# NeMo Framework - Training a large language model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "edcd95d4",
      "metadata": {
        "id": "edcd95d4"
      },
      "source": [
        "## Overview\n",
        "Large language model (LLM) like ChatGPT possess astonishing versatility, being able to perform tasks such as induction, programming, translation, and more, with results comparable to or even superior to human experts. To learn how to pre-train a large language model (LLM). NVIDIA has introduced NeMo Framework that is capabilities to pre-process training data, distribute training across multiple GPUs efficiently.\n",
        "\n",
        "Pre-trained language model is powerful in a variety of tasks but often lack the specialized focus needed for domain-specific applications. Therefore, to adapt the language model to a domain-specific task, fine-tuning can be employed. In this notebook, you will learn how to implement two type of tuning methods, **(1)Fine-tuning** and **(2)PEFT methods** like **LoRA** for adapting language model on specific downstream task using NVIDIA NeMo."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "934fb96a",
      "metadata": {
        "id": "934fb96a"
      },
      "source": [
        "## Table of Contents\n",
        "\n",
        "This course covers the below sections:\n",
        "1. [Pre-training](#s1)\n",
        "    - [1.1 Download dataset](#s1.1)\n",
        "    - [1.2 Data preprocessing](#s1.2)\n",
        "    - [1.3 Download pre-trained model for continued pre-training](#s1.3)\n",
        "    - [1.4 Run pre-training](#s1.4)\n",
        "    \n",
        "    \n",
        "2. [Instruction Tuning ](#s2)\n",
        "    - [2.1 Download dataset: erhwenkuo/alpaca-data-gpt4-chinese-zhtw](#s2.1)\n",
        "    - [2.2 Split the data into train, validation and test](#s2.2)\n",
        "    - [2.3 Full parameter fine-tuning](#s2.3)\n",
        "    - [2.4. Parameter Efficient Fine-tuning](#s2.4)\n",
        "\n",
        "\n",
        "3. [Evaluation](#s3)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01b4fb48",
      "metadata": {
        "id": "01b4fb48"
      },
      "source": [
        "## 1. Pre-training <a name='s1'></a>\n",
        "\n",
        "The initial phase of our process is concentrated on model pre-training, which serves as the primary stage for the model to acquire knowledge."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ac50c9db",
      "metadata": {
        "id": "ac50c9db"
      },
      "source": [
        "### 1.1 Download dataset <a name='s1.1'></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "01d51714",
      "metadata": {
        "tags": [],
        "id": "01d51714",
        "outputId": "c91cc8b5-6023-47fb-b3a8-99cd2bf8c4d0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Generating train split: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9827/9827 [00:00<00:00, 84785.62 examples/s]\n",
            "Creating json from Arrow format: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 10/10 [00:00<00:00, 78.31ba/s]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "13914259"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "dataset = load_dataset('erhwenkuo/wikinews-zhtw')['train']\n",
        "dataset.to_json('./data/custom_dataset/json/wikinews-zhtw.jsonl', force_ascii=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df2d1cea-ad29-4f2e-baa8-7ebec3237506",
      "metadata": {
        "id": "df2d1cea-ad29-4f2e-baa8-7ebec3237506"
      },
      "source": [
        "### 1.2 Data preprocessing <a name='s1.2'></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86ae6657",
      "metadata": {
        "tags": [],
        "id": "86ae6657",
        "outputId": "703b19ef-27de-4336-a4fa-771278d13342"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 09:49:39 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 09:49:39 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 09:49:39 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 09:49:39 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 09:49:40 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 09:49:40 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 09:49:40 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/models/language_modeling/megatron_base_prompt_learning_model.py:181: DeprecationWarning: invalid escape sequence '\\{'\n",
            "      \"prompt_template_fields\": re.findall(\"\\{(.*?)\\}\", task.prompt_template),\n",
            "    \n",
            "[NeMo W 2024-12-12 09:49:40 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/models/language_modeling/megatron_base_model.py:389: DeprecationWarning: invalid escape sequence '\\.'\n",
            "      return re.fullmatch(\"[0-9][0-9]\\.[0-9][0-9].*\", nvidia_torch_version)  # \"YY.MM.*\"\n",
            "    \n",
            "[NeMo W 2024-12-12 09:49:40 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 09:49:41 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/modules/common/megatron/vocab_parallel_cross_entropy.py:88: DeprecationWarning: invalid escape sequence '\\s'\n",
            "      \"\"\"\n",
            "    \n",
            "[NeMo W 2024-12-12 09:49:41 nemo_logging:349] /opt/NeMo/nemo/collections/asr/parts/utils/wfst_utils.py:1328: DeprecationWarning: invalid escape sequence '\\d'\n",
            "      width, height = re.findall('\\d+', line)\n",
            "    \n",
            "[NeMo W 2024-12-12 09:49:42 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 09:49:42 nemo_logging:349] /opt/NeMo/nemo/collections/asr/modules/rnnt.py:1558: DeprecationWarning: invalid escape sequence '\\*'\n",
            "      \"\"\"\n",
            "    \n",
            "[NeMo W 2024-12-12 09:49:42 nemo_logging:349] /opt/NeMo/nemo/collections/common/data/lhotse/nemo_adapters.py:198: DeprecationWarning: invalid escape sequence '\\d'\n",
            "      \"\"\"\n",
            "    \n",
            "[NeMo W 2024-12-12 09:49:42 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 09:49:42 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 09:49:42 nemo_logging:349] /opt/NeMo/nemo/collections/asr/parts/utils/vad_utils.py:1109: DeprecationWarning: invalid escape sequence '\\s'\n",
            "      data = pd.read_csv(path2ground_truth_label, sep=\"\\s+\", delimiter=None, header=None)\n",
            "    \n",
            "[NeMo W 2024-12-12 09:49:42 nemo_logging:349] /opt/NeMo/nemo/collections/asr/parts/utils/asr_batching.py:39: DeprecationWarning: invalid escape sequence '\\m'\n",
            "      \"\"\"\n",
            "    \n",
            "[NeMo I 2024-12-12 09:49:42 tokenizer_utils:184] Getting HuggingFace AutoTokenizer with pretrained_model_name: /workspace/tokenizer-llama31-8B-Instruct\n",
            "Vocab size: 128256\n",
            "Output prefix: data/custom_dataset/preprocessed/wikinews\n",
            "Time to startup: 0.3509535789489746\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "Processing file data/custom_dataset/json/wikinews-zhtw.jsonl 1/1\n",
            "[NeMo I 2024-12-12 09:49:43 tokenizer_utils:184] Getting HuggingFace AutoTokenizer with pretrained_model_name: /workspace/tokenizer-llama31-8B-Instruct\n",
            "Processed 100 documents (237.1602095046063 docs/s, 0.14482574172218757 MB/s).\n",
            "Processed 200 documents (417.35115653812704 docs/s, 0.2218945214939173 MB/s).\n",
            "Processed 300 documents (550.441541807034 docs/s, 0.2894442483202951 MB/s).\n",
            "Processed 400 documents (637.9128347839577 docs/s, 0.3409909517695133 MB/s).\n",
            "Processed 500 documents (763.8484389774133 docs/s, 0.3783430971608897 MB/s).\n",
            "Processed 600 documents (885.5994941004782 docs/s, 0.4001209147716256 MB/s).\n",
            "Processed 700 documents (1008.0419145846137 docs/s, 0.4177670046247498 MB/s).\n",
            "Processed 800 documents (1103.6509410876251 docs/s, 0.4370209373248535 MB/s).\n",
            "Processed 900 documents (1198.3257796600324 docs/s, 0.4576773668837693 MB/s).\n",
            "Processed 1000 documents (1286.3895496552088 docs/s, 0.4745372216244996 MB/s).\n",
            "Processed 1100 documents (1341.7440186587023 docs/s, 0.4944881798168446 MB/s).\n",
            "Processed 1200 documents (1399.4767079424169 docs/s, 0.5115171521345144 MB/s).\n",
            "Processed 1300 documents (1445.0418504905372 docs/s, 0.5282434825841376 MB/s).\n",
            "Processed 1400 documents (1479.4007445344039 docs/s, 0.5445108223537687 MB/s).\n",
            "Processed 1500 documents (1510.9908148696081 docs/s, 0.5592686479521687 MB/s).\n",
            "Processed 1600 documents (1527.8592351331156 docs/s, 0.5721133990810378 MB/s).\n",
            "Processed 1700 documents (1551.7548849815516 docs/s, 0.5839222703276299 MB/s).\n",
            "Processed 1800 documents (1564.124778658574 docs/s, 0.5958052290752979 MB/s).\n",
            "Processed 1900 documents (1596.2250689281855 docs/s, 0.6057892665319984 MB/s).\n",
            "Processed 2000 documents (1612.7928190163484 docs/s, 0.6146848485559264 MB/s).\n",
            "Processed 2100 documents (1614.7802346481049 docs/s, 0.6234667171191625 MB/s).\n",
            "Processed 2200 documents (1624.074863812216 docs/s, 0.6314586344841496 MB/s).\n",
            "Processed 2300 documents (1629.1348489213688 docs/s, 0.6390186517370808 MB/s).\n",
            "Processed 2400 documents (1636.3534927996952 docs/s, 0.6463400915911806 MB/s).\n",
            "Processed 2500 documents (1634.0312642246643 docs/s, 0.6530028855664176 MB/s).\n",
            "Processed 2600 documents (1639.4399211759624 docs/s, 0.6586136819358673 MB/s).\n",
            "Processed 2700 documents (1642.9525854846336 docs/s, 0.6659616846275567 MB/s).\n",
            "Processed 2800 documents (1636.5971859123933 docs/s, 0.6709495232372376 MB/s).\n",
            "Processed 2900 documents (1638.6829730026118 docs/s, 0.6754382963221828 MB/s).\n",
            "Processed 3000 documents (1636.0922152385833 docs/s, 0.6800438912143847 MB/s).\n",
            "Processed 3100 documents (1633.2563538242025 docs/s, 0.6838458468686004 MB/s).\n",
            "Processed 3200 documents (1639.3719762803703 docs/s, 0.6872238581883815 MB/s).\n",
            "Processed 3300 documents (1627.989577067667 docs/s, 0.690217926982546 MB/s).\n",
            "Processed 3400 documents (1628.684961567141 docs/s, 0.6934504533447603 MB/s).\n",
            "Processed 3500 documents (1630.9245046338385 docs/s, 0.6969869912118216 MB/s).\n",
            "Processed 3600 documents (1633.6636870600523 docs/s, 0.6999585295192897 MB/s).\n",
            "Processed 3700 documents (1637.6909448305796 docs/s, 0.7041908903250398 MB/s).\n",
            "Processed 3800 documents (1670.6922065067074 docs/s, 0.7074017310954089 MB/s).\n",
            "Processed 3900 documents (1693.603431315757 docs/s, 0.7102472426298347 MB/s).\n",
            "Processed 4000 documents (1717.3016243783472 docs/s, 0.7130989622624321 MB/s).\n",
            "Processed 4100 documents (1744.4375724095453 docs/s, 0.7159192407295241 MB/s).\n",
            "Processed 4200 documents (1765.723227699121 docs/s, 0.7181390384708679 MB/s).\n",
            "Processed 4300 documents (1777.465174340925 docs/s, 0.7203598154563079 MB/s).\n",
            "Processed 4400 documents (1773.8548577162708 docs/s, 0.7223307503176706 MB/s).\n",
            "Processed 4500 documents (1769.6655571703154 docs/s, 0.7241835825793446 MB/s).\n",
            "Processed 4600 documents (1752.2689095168068 docs/s, 0.7255090297193143 MB/s).\n",
            "Processed 4700 documents (1740.8297143369832 docs/s, 0.7269266574602955 MB/s).\n",
            "Processed 4800 documents (1726.5221810787357 docs/s, 0.7279634008491858 MB/s).\n",
            "Processed 4900 documents (1736.6406425204366 docs/s, 0.7297306478050334 MB/s).\n",
            "Processed 5000 documents (1743.885306066277 docs/s, 0.7315029547025985 MB/s).\n",
            "Processed 5100 documents (1772.0320651482377 docs/s, 0.7341135561140094 MB/s).\n",
            "Processed 5200 documents (1792.407998520077 docs/s, 0.7362186078287168 MB/s).\n",
            "Processed 5300 documents (1807.2150360267271 docs/s, 0.7380280132904194 MB/s).\n",
            "Processed 5400 documents (1807.6873581037626 docs/s, 0.7394721631567093 MB/s).\n",
            "Processed 5500 documents (1803.5706970048839 docs/s, 0.7400372587399641 MB/s).\n",
            "Processed 5600 documents (1795.7454291184417 docs/s, 0.7411157701816732 MB/s).\n",
            "Processed 5700 documents (1772.5140334801733 docs/s, 0.7417118262018548 MB/s).\n",
            "Processed 5800 documents (1756.3412483723116 docs/s, 0.7425475773038233 MB/s).\n",
            "Processed 5900 documents (1749.8649119551787 docs/s, 0.7434919580751876 MB/s).\n",
            "Processed 6000 documents (1737.957827950502 docs/s, 0.7445704839852068 MB/s).\n",
            "Processed 6100 documents (1749.7627157962233 docs/s, 0.7469563937615646 MB/s).\n",
            "Processed 6200 documents (1702.777922196806 docs/s, 0.7461904915937867 MB/s).\n",
            "Processed 6300 documents (1663.1444294660275 docs/s, 0.7452919005899152 MB/s).\n",
            "Processed 6400 documents (1631.0592379098514 docs/s, 0.7451322897472301 MB/s).\n",
            "Processed 6500 documents (1598.5484934709414 docs/s, 0.7449926214459339 MB/s).\n",
            "Processed 6600 documents (1586.6661928480132 docs/s, 0.7454525445781199 MB/s).\n",
            "Processed 6700 documents (1582.9418465178512 docs/s, 0.7461349170879965 MB/s).\n",
            "Processed 6800 documents (1587.3212283762136 docs/s, 0.7472820836127162 MB/s).\n",
            "Processed 6900 documents (1592.7891867741364 docs/s, 0.7483121877411795 MB/s).\n",
            "Processed 7000 documents (1592.9301986021555 docs/s, 0.7489057062135184 MB/s).\n",
            "Processed 7100 documents (1591.4689160615958 docs/s, 0.7495438086639078 MB/s).\n",
            "Processed 7200 documents (1597.2818641559377 docs/s, 0.7506026899656186 MB/s).\n",
            "Processed 7300 documents (1592.3537279721047 docs/s, 0.7513726438955954 MB/s).\n",
            "Processed 7400 documents (1593.615566595257 docs/s, 0.752013978784139 MB/s).\n",
            "Processed 7500 documents (1589.7623792098107 docs/s, 0.7524620958478143 MB/s).\n",
            "Processed 7600 documents (1582.9401429957602 docs/s, 0.7527138704298315 MB/s).\n",
            "Processed 7700 documents (1576.675228434083 docs/s, 0.753099720904901 MB/s).\n",
            "Processed 7800 documents (1567.8299843507277 docs/s, 0.7531556011144026 MB/s).\n",
            "Processed 7900 documents (1552.7058246545118 docs/s, 0.7556521933497521 MB/s).\n",
            "Processed 8000 documents (1536.8419143433953 docs/s, 0.7552805188169337 MB/s).\n",
            "Processed 8100 documents (1528.2989442935614 docs/s, 0.7554422984514428 MB/s).\n",
            "Processed 8200 documents (1532.3124716422358 docs/s, 0.7562030660067172 MB/s).\n",
            "Processed 8300 documents (1532.417007027966 docs/s, 0.7571060334650878 MB/s).\n",
            "Processed 8400 documents (1534.0063187792477 docs/s, 0.757850852235805 MB/s).\n",
            "Processed 8500 documents (1532.174273147761 docs/s, 0.7582216084820149 MB/s).\n",
            "Processed 8600 documents (1515.5760407233536 docs/s, 0.7580319174615374 MB/s).\n",
            "Processed 8700 documents (1505.8455642762488 docs/s, 0.7578791614312794 MB/s).\n",
            "Processed 8800 documents (1500.9382908951864 docs/s, 0.7580553711061123 MB/s).\n",
            "Processed 8900 documents (1493.1162620025002 docs/s, 0.7580171895157369 MB/s).\n",
            "Processed 9000 documents (1491.488275403306 docs/s, 0.7580467400269038 MB/s).\n",
            "Processed 9100 documents (1484.034832363467 docs/s, 0.7580448602862027 MB/s).\n",
            "Processed 9200 documents (1488.6940859092097 docs/s, 0.75865843912953 MB/s).\n",
            "Processed 9300 documents (1470.9043504453662 docs/s, 0.7577756662580803 MB/s).\n",
            "Processed 9400 documents (1463.8925582315303 docs/s, 0.7579710504924196 MB/s).\n",
            "Processed 9500 documents (1460.0732992128276 docs/s, 0.7581386142067738 MB/s).\n",
            "Processed 9600 documents (1442.7850371923141 docs/s, 0.7575237697537531 MB/s).\n",
            "Processed 9700 documents (1435.0607559834573 docs/s, 0.7575715750891667 MB/s).\n",
            "Processed 9800 documents (1435.1892598702875 docs/s, 0.7578023236529011 MB/s).\n",
            "[NeMo W 2024-12-12 09:49:49 nemo_logging:349] /usr/lib/python3.10/multiprocessing/pool.py:268: ResourceWarning: unclosed running multiprocessing pool <multiprocessing.pool.Pool state=RUN pool_size=1>\n",
            "      _warn(f\"unclosed running multiprocessing pool {self!r}\",\n",
            "    \n",
            "[NeMo W 2024-12-12 09:49:49 nemo_logging:349] /opt/NeMo/scripts/nlp_language_modeling/preprocess_data_for_megatron.py:364: ResourceWarning: unclosed file <_io.TextIOWrapper name='data/custom_dataset/json/wikinews-zhtw.jsonl' mode='r' encoding='utf-8'>\n",
            "      main()\n",
            "    \n",
            "[NeMo W 2024-12-12 09:49:49 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpghy8i8rd'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "# Data preprocessing\n",
        "!mkdir -p data/custom_dataset/preprocessed\n",
        "\n",
        "!python /opt/NeMo/scripts/nlp_language_modeling/preprocess_data_for_megatron.py \\\n",
        "--input data/custom_dataset/json/wikinews-zhtw.jsonl \\\n",
        "--json-keys text \\\n",
        "--dataset-impl mmap \\\n",
        "--tokenizer-library huggingface \\\n",
        "--tokenizer-type=/workspace/tokenizer-llama31-8B-Instruct \\\n",
        "--output-prefix data/custom_dataset/preprocessed/wikinews \\\n",
        "--append-eod"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5c463fb8",
      "metadata": {
        "id": "5c463fb8"
      },
      "source": [
        "### 1.3 Download pre-trained model for continued pre-training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd1c883a-fde1-444d-bf15-277172b174d5",
      "metadata": {
        "tags": [],
        "id": "cd1c883a-fde1-444d-bf15-277172b174d5",
        "outputId": "11d1430c-e404-4dd8-a6e2-f5488288e029"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n",
            "Fetching 17 files:   0%|          | 0/17 [00:00<?, ?it/s]Downloading 'LICENSE' to '/root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/a7c3ca16cee30425ed6ad841a809590f2bcbf290.incomplete'\n",
            "Downloading 'README.md' to '/root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/bbd5630a05b65c1a8b25141bd11ec44844107d58.incomplete'\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/a7c3ca16cee30425ed6ad841a809590f2bcbf290\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/bbd5630a05b65c1a8b25141bd11ec44844107d58\n",
            "Downloading 'model-00003-of-00004.safetensors' to '/root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/fc1cdddd6bfa91128d6e94ee73d0ce62bfcdb7af29e978ddcab30c66ae9ea7fa.incomplete'\n",
            "Downloading 'model-00004-of-00004.safetensors' to '/root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/92ecfe1a2414458b4821ac8c13cf8cb70aed66b5eea8dc5ad9eeb4ff309d6d7b.incomplete'\n",
            "Downloading 'config.json' to '/root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/0bb6fd75b3ad2fe988565929f329945262c2814e.incomplete'\n",
            "Downloading 'USE_POLICY.md' to '/root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/81ebb55902285e8dd5804ccf423d17ffb2a622ee.incomplete'\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/0bb6fd75b3ad2fe988565929f329945262c2814e\n",
            "Downloading 'model-00002-of-00004.safetensors' to '/root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/09d433f650646834a83c580877bd60c6d1f88f7755305c12576b5c7058f9af15.incomplete'\n",
            "Downloading 'model-00001-of-00004.safetensors' to '/root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/2b1879f356aed350030bb40eb45ad362c89d9891096f79a3ab323d3ba5607668.incomplete'\n",
            "Downloading '.gitattributes' to '/root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/a6344aac8c09253b3b630fb776ae94478aa0275b.incomplete'\n",
            "Downloading 'generation_config.json' to '/root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/cc7276afd599de091142c6ed3005faf8a74aa257.incomplete'\n",
            "Downloading 'model.safetensors.index.json' to '/root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/0fd8120f1c6acddc268ebc2583058efaf699a771.incomplete'\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/a6344aac8c09253b3b630fb776ae94478aa0275b\n",
            "Fetching 17 files:   6%|▌         | 1/17 [00:05<01:31,  5.71s/it]Download complete. Moving file to /root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/0fd8120f1c6acddc268ebc2583058efaf699a771\n",
            "Downloading 'original/consolidated.00.pth' to '/root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/ab33d910f405204e5d388bc3521503584800461dc96808e287821dd451c1edac.incomplete'\n",
            "Downloading 'original/params.json' to '/root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/f1131204e79d0c09d2bac93f11569a8a655d68ba.incomplete'\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/81ebb55902285e8dd5804ccf423d17ffb2a622ee\n",
            "Fetching 17 files:  24%|██▎       | 4/17 [00:06<00:15,  1.18s/it]Download complete. Moving file to /root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/cc7276afd599de091142c6ed3005faf8a74aa257\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/f1131204e79d0c09d2bac93f11569a8a655d68ba\n",
            "Downloading 'special_tokens_map.json' to '/root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/02ee80b6196926a5ad790a004d9efd6ab1ba6542.incomplete'\n",
            "Downloading 'original/tokenizer.model' to '/root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/82e9d31979e92ab929cd544440f129d9ecd797b69e327f80f17e1c50d5551b55.incomplete'\n",
            "Downloading 'tokenizer.json' to '/root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/5cc5f00a5b203e90a27a3bd60d1ec393b07971e8.incomplete'\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/02ee80b6196926a5ad790a004d9efd6ab1ba6542\n",
            "Downloading 'tokenizer_config.json' to '/root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/db88166e2bc4c799fd5d1ae643b75e84d03ee70e.incomplete'\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/db88166e2bc4c799fd5d1ae643b75e84d03ee70e\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/5cc5f00a5b203e90a27a3bd60d1ec393b07971e8\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/82e9d31979e92ab929cd544440f129d9ecd797b69e327f80f17e1c50d5551b55\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/92ecfe1a2414458b4821ac8c13cf8cb70aed66b5eea8dc5ad9eeb4ff309d6d7b\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/fc1cdddd6bfa91128d6e94ee73d0ce62bfcdb7af29e978ddcab30c66ae9ea7fa\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/09d433f650646834a83c580877bd60c6d1f88f7755305c12576b5c7058f9af15\n",
            "Download complete. Moving file to /root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/2b1879f356aed350030bb40eb45ad362c89d9891096f79a3ab323d3ba5607668\n",
            "Fetching 17 files:  41%|████      | 7/17 [07:08<12:41, 76.11s/it]Download complete. Moving file to /root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/blobs/ab33d910f405204e5d388bc3521503584800461dc96808e287821dd451c1edac\n",
            "Fetching 17 files: 100%|██████████| 17/17 [11:26<00:00, 40.38s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 10:01:28 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:01:28 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:01:28 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:01:28 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:01:29 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 10:01:29 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 10:01:29 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:01:30 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 10:01:30 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:01:30 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 10:01:31 convert_llama_hf_to_nemo:128] loading checkpoint meta-llama/Llama-3.1-8B-Instruct\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 4/4 [00:03<00:00,  1.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "hf_config: {'vocab_size': 128256, 'max_position_embeddings': 131072, 'hidden_size': 4096, 'intermediate_size': 14336, 'num_hidden_layers': 32, 'num_attention_heads': 32, 'num_key_value_heads': 8, 'hidden_act': 'silu', 'initializer_range': 0.02, 'rms_norm_eps': 1e-05, 'pretraining_tp': 1, 'use_cache': True, 'rope_theta': 500000.0, 'rope_scaling': {'factor': 8.0, 'low_freq_factor': 1.0, 'high_freq_factor': 4.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}, 'attention_bias': False, 'attention_dropout': 0.0, 'mlp_bias': False, 'head_dim': 128, 'return_dict': True, 'output_hidden_states': False, 'output_attentions': False, 'torchscript': False, 'torch_dtype': torch.bfloat16, 'use_bfloat16': False, 'tf_legacy_loss': False, 'pruned_heads': {}, 'tie_word_embeddings': False, 'chunk_size_feed_forward': 0, 'is_encoder_decoder': False, 'is_decoder': False, 'cross_attention_hidden_size': None, 'add_cross_attention': False, 'tie_encoder_decoder': False, 'max_length': 20, 'min_length': 0, 'do_sample': False, 'early_stopping': False, 'num_beams': 1, 'num_beam_groups': 1, 'diversity_penalty': 0.0, 'temperature': 1.0, 'top_k': 50, 'top_p': 1.0, 'typical_p': 1.0, 'repetition_penalty': 1.0, 'length_penalty': 1.0, 'no_repeat_ngram_size': 0, 'encoder_no_repeat_ngram_size': 0, 'bad_words_ids': None, 'num_return_sequences': 1, 'output_scores': False, 'return_dict_in_generate': False, 'forced_bos_token_id': None, 'forced_eos_token_id': None, 'remove_invalid_values': False, 'exponential_decay_length_penalty': None, 'suppress_tokens': None, 'begin_suppress_tokens': None, 'architectures': ['LlamaForCausalLM'], 'finetuning_task': None, 'id2label': {0: 'LABEL_0', 1: 'LABEL_1'}, 'label2id': {'LABEL_0': 0, 'LABEL_1': 1}, 'tokenizer_class': None, 'prefix': None, 'bos_token_id': 128000, 'pad_token_id': None, 'eos_token_id': [128001, 128008, 128009], 'sep_token_id': None, 'decoder_start_token_id': None, 'task_specific_params': None, 'problem_type': None, '_name_or_path': 'meta-llama/Llama-3.1-8B-Instruct', '_commit_hash': '0e9e39f249a16976918f6564b8830bc894c89659', '_attn_implementation_internal': 'sdpa', '_attn_implementation_autoset': True, 'transformers_version': '4.42.3', 'model_type': 'llama'}\n",
            "named parameters:\n",
            "- model.embed_tokens.weight\n",
            "- model.layers.0.self_attn.q_proj.weight\n",
            "- model.layers.0.self_attn.k_proj.weight\n",
            "- model.layers.0.self_attn.v_proj.weight\n",
            "- model.layers.0.self_attn.o_proj.weight\n",
            "- model.layers.0.mlp.gate_proj.weight\n",
            "- model.layers.0.mlp.up_proj.weight\n",
            "- model.layers.0.mlp.down_proj.weight\n",
            "- model.layers.0.input_layernorm.weight\n",
            "- model.layers.0.post_attention_layernorm.weight\n",
            "- model.layers.1.self_attn.q_proj.weight\n",
            "- model.layers.1.self_attn.k_proj.weight\n",
            "- model.layers.1.self_attn.v_proj.weight\n",
            "- model.layers.1.self_attn.o_proj.weight\n",
            "- model.layers.1.mlp.gate_proj.weight\n",
            "- model.layers.1.mlp.up_proj.weight\n",
            "- model.layers.1.mlp.down_proj.weight\n",
            "- model.layers.1.input_layernorm.weight\n",
            "- model.layers.1.post_attention_layernorm.weight\n",
            "- model.layers.2.self_attn.q_proj.weight\n",
            "- model.layers.2.self_attn.k_proj.weight\n",
            "- model.layers.2.self_attn.v_proj.weight\n",
            "- model.layers.2.self_attn.o_proj.weight\n",
            "- model.layers.2.mlp.gate_proj.weight\n",
            "- model.layers.2.mlp.up_proj.weight\n",
            "- model.layers.2.mlp.down_proj.weight\n",
            "- model.layers.2.input_layernorm.weight\n",
            "- model.layers.2.post_attention_layernorm.weight\n",
            "- model.layers.3.self_attn.q_proj.weight\n",
            "- model.layers.3.self_attn.k_proj.weight\n",
            "- model.layers.3.self_attn.v_proj.weight\n",
            "- model.layers.3.self_attn.o_proj.weight\n",
            "- model.layers.3.mlp.gate_proj.weight\n",
            "- model.layers.3.mlp.up_proj.weight\n",
            "- model.layers.3.mlp.down_proj.weight\n",
            "- model.layers.3.input_layernorm.weight\n",
            "- model.layers.3.post_attention_layernorm.weight\n",
            "- model.layers.4.self_attn.q_proj.weight\n",
            "- model.layers.4.self_attn.k_proj.weight\n",
            "- model.layers.4.self_attn.v_proj.weight\n",
            "- model.layers.4.self_attn.o_proj.weight\n",
            "- model.layers.4.mlp.gate_proj.weight\n",
            "- model.layers.4.mlp.up_proj.weight\n",
            "- model.layers.4.mlp.down_proj.weight\n",
            "- model.layers.4.input_layernorm.weight\n",
            "- model.layers.4.post_attention_layernorm.weight\n",
            "- model.layers.5.self_attn.q_proj.weight\n",
            "- model.layers.5.self_attn.k_proj.weight\n",
            "- model.layers.5.self_attn.v_proj.weight\n",
            "- model.layers.5.self_attn.o_proj.weight\n",
            "- model.layers.5.mlp.gate_proj.weight\n",
            "- model.layers.5.mlp.up_proj.weight\n",
            "- model.layers.5.mlp.down_proj.weight\n",
            "- model.layers.5.input_layernorm.weight\n",
            "- model.layers.5.post_attention_layernorm.weight\n",
            "- model.layers.6.self_attn.q_proj.weight\n",
            "- model.layers.6.self_attn.k_proj.weight\n",
            "- model.layers.6.self_attn.v_proj.weight\n",
            "- model.layers.6.self_attn.o_proj.weight\n",
            "- model.layers.6.mlp.gate_proj.weight\n",
            "- model.layers.6.mlp.up_proj.weight\n",
            "- model.layers.6.mlp.down_proj.weight\n",
            "- model.layers.6.input_layernorm.weight\n",
            "- model.layers.6.post_attention_layernorm.weight\n",
            "- model.layers.7.self_attn.q_proj.weight\n",
            "- model.layers.7.self_attn.k_proj.weight\n",
            "- model.layers.7.self_attn.v_proj.weight\n",
            "- model.layers.7.self_attn.o_proj.weight\n",
            "- model.layers.7.mlp.gate_proj.weight\n",
            "- model.layers.7.mlp.up_proj.weight\n",
            "- model.layers.7.mlp.down_proj.weight\n",
            "- model.layers.7.input_layernorm.weight\n",
            "- model.layers.7.post_attention_layernorm.weight\n",
            "- model.layers.8.self_attn.q_proj.weight\n",
            "- model.layers.8.self_attn.k_proj.weight\n",
            "- model.layers.8.self_attn.v_proj.weight\n",
            "- model.layers.8.self_attn.o_proj.weight\n",
            "- model.layers.8.mlp.gate_proj.weight\n",
            "- model.layers.8.mlp.up_proj.weight\n",
            "- model.layers.8.mlp.down_proj.weight\n",
            "- model.layers.8.input_layernorm.weight\n",
            "- model.layers.8.post_attention_layernorm.weight\n",
            "- model.layers.9.self_attn.q_proj.weight\n",
            "- model.layers.9.self_attn.k_proj.weight\n",
            "- model.layers.9.self_attn.v_proj.weight\n",
            "- model.layers.9.self_attn.o_proj.weight\n",
            "- model.layers.9.mlp.gate_proj.weight\n",
            "- model.layers.9.mlp.up_proj.weight\n",
            "- model.layers.9.mlp.down_proj.weight\n",
            "- model.layers.9.input_layernorm.weight\n",
            "- model.layers.9.post_attention_layernorm.weight\n",
            "- model.layers.10.self_attn.q_proj.weight\n",
            "- model.layers.10.self_attn.k_proj.weight\n",
            "- model.layers.10.self_attn.v_proj.weight\n",
            "- model.layers.10.self_attn.o_proj.weight\n",
            "- model.layers.10.mlp.gate_proj.weight\n",
            "- model.layers.10.mlp.up_proj.weight\n",
            "- model.layers.10.mlp.down_proj.weight\n",
            "- model.layers.10.input_layernorm.weight\n",
            "- model.layers.10.post_attention_layernorm.weight\n",
            "- model.layers.11.self_attn.q_proj.weight\n",
            "- model.layers.11.self_attn.k_proj.weight\n",
            "- model.layers.11.self_attn.v_proj.weight\n",
            "- model.layers.11.self_attn.o_proj.weight\n",
            "- model.layers.11.mlp.gate_proj.weight\n",
            "- model.layers.11.mlp.up_proj.weight\n",
            "- model.layers.11.mlp.down_proj.weight\n",
            "- model.layers.11.input_layernorm.weight\n",
            "- model.layers.11.post_attention_layernorm.weight\n",
            "- model.layers.12.self_attn.q_proj.weight\n",
            "- model.layers.12.self_attn.k_proj.weight\n",
            "- model.layers.12.self_attn.v_proj.weight\n",
            "- model.layers.12.self_attn.o_proj.weight\n",
            "- model.layers.12.mlp.gate_proj.weight\n",
            "- model.layers.12.mlp.up_proj.weight\n",
            "- model.layers.12.mlp.down_proj.weight\n",
            "- model.layers.12.input_layernorm.weight\n",
            "- model.layers.12.post_attention_layernorm.weight\n",
            "- model.layers.13.self_attn.q_proj.weight\n",
            "- model.layers.13.self_attn.k_proj.weight\n",
            "- model.layers.13.self_attn.v_proj.weight\n",
            "- model.layers.13.self_attn.o_proj.weight\n",
            "- model.layers.13.mlp.gate_proj.weight\n",
            "- model.layers.13.mlp.up_proj.weight\n",
            "- model.layers.13.mlp.down_proj.weight\n",
            "- model.layers.13.input_layernorm.weight\n",
            "- model.layers.13.post_attention_layernorm.weight\n",
            "- model.layers.14.self_attn.q_proj.weight\n",
            "- model.layers.14.self_attn.k_proj.weight\n",
            "- model.layers.14.self_attn.v_proj.weight\n",
            "- model.layers.14.self_attn.o_proj.weight\n",
            "- model.layers.14.mlp.gate_proj.weight\n",
            "- model.layers.14.mlp.up_proj.weight\n",
            "- model.layers.14.mlp.down_proj.weight\n",
            "- model.layers.14.input_layernorm.weight\n",
            "- model.layers.14.post_attention_layernorm.weight\n",
            "- model.layers.15.self_attn.q_proj.weight\n",
            "- model.layers.15.self_attn.k_proj.weight\n",
            "- model.layers.15.self_attn.v_proj.weight\n",
            "- model.layers.15.self_attn.o_proj.weight\n",
            "- model.layers.15.mlp.gate_proj.weight\n",
            "- model.layers.15.mlp.up_proj.weight\n",
            "- model.layers.15.mlp.down_proj.weight\n",
            "- model.layers.15.input_layernorm.weight\n",
            "- model.layers.15.post_attention_layernorm.weight\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 10:01:37 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
            "    \n",
            "[NeMo W 2024-12-12 10:01:37 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py:1395: DeprecationWarning: torch.set_autocast_gpu_dtype(dtype) is deprecated. Please use torch.set_autocast_dtype('cuda', dtype) instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:678.)\n",
            "      torch.set_autocast_gpu_dtype(dtype)\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- model.layers.16.self_attn.q_proj.weight\n",
            "- model.layers.16.self_attn.k_proj.weight\n",
            "- model.layers.16.self_attn.v_proj.weight\n",
            "- model.layers.16.self_attn.o_proj.weight\n",
            "- model.layers.16.mlp.gate_proj.weight\n",
            "- model.layers.16.mlp.up_proj.weight\n",
            "- model.layers.16.mlp.down_proj.weight\n",
            "- model.layers.16.input_layernorm.weight\n",
            "- model.layers.16.post_attention_layernorm.weight\n",
            "- model.layers.17.self_attn.q_proj.weight\n",
            "- model.layers.17.self_attn.k_proj.weight\n",
            "- model.layers.17.self_attn.v_proj.weight\n",
            "- model.layers.17.self_attn.o_proj.weight\n",
            "- model.layers.17.mlp.gate_proj.weight\n",
            "- model.layers.17.mlp.up_proj.weight\n",
            "- model.layers.17.mlp.down_proj.weight\n",
            "- model.layers.17.input_layernorm.weight\n",
            "- model.layers.17.post_attention_layernorm.weight\n",
            "- model.layers.18.self_attn.q_proj.weight\n",
            "- model.layers.18.self_attn.k_proj.weight\n",
            "- model.layers.18.self_attn.v_proj.weight\n",
            "- model.layers.18.self_attn.o_proj.weight\n",
            "- model.layers.18.mlp.gate_proj.weight\n",
            "- model.layers.18.mlp.up_proj.weight\n",
            "- model.layers.18.mlp.down_proj.weight\n",
            "- model.layers.18.input_layernorm.weight\n",
            "- model.layers.18.post_attention_layernorm.weight\n",
            "- model.layers.19.self_attn.q_proj.weight\n",
            "- model.layers.19.self_attn.k_proj.weight\n",
            "- model.layers.19.self_attn.v_proj.weight\n",
            "- model.layers.19.self_attn.o_proj.weight\n",
            "- model.layers.19.mlp.gate_proj.weight\n",
            "- model.layers.19.mlp.up_proj.weight\n",
            "- model.layers.19.mlp.down_proj.weight\n",
            "- model.layers.19.input_layernorm.weight\n",
            "- model.layers.19.post_attention_layernorm.weight\n",
            "- model.layers.20.self_attn.q_proj.weight\n",
            "- model.layers.20.self_attn.k_proj.weight\n",
            "- model.layers.20.self_attn.v_proj.weight\n",
            "- model.layers.20.self_attn.o_proj.weight\n",
            "- model.layers.20.mlp.gate_proj.weight\n",
            "- model.layers.20.mlp.up_proj.weight\n",
            "- model.layers.20.mlp.down_proj.weight\n",
            "- model.layers.20.input_layernorm.weight\n",
            "- model.layers.20.post_attention_layernorm.weight\n",
            "- model.layers.21.self_attn.q_proj.weight\n",
            "- model.layers.21.self_attn.k_proj.weight\n",
            "- model.layers.21.self_attn.v_proj.weight\n",
            "- model.layers.21.self_attn.o_proj.weight\n",
            "- model.layers.21.mlp.gate_proj.weight\n",
            "- model.layers.21.mlp.up_proj.weight\n",
            "- model.layers.21.mlp.down_proj.weight\n",
            "- model.layers.21.input_layernorm.weight\n",
            "- model.layers.21.post_attention_layernorm.weight\n",
            "- model.layers.22.self_attn.q_proj.weight\n",
            "- model.layers.22.self_attn.k_proj.weight\n",
            "- model.layers.22.self_attn.v_proj.weight\n",
            "- model.layers.22.self_attn.o_proj.weight\n",
            "- model.layers.22.mlp.gate_proj.weight\n",
            "- model.layers.22.mlp.up_proj.weight\n",
            "- model.layers.22.mlp.down_proj.weight\n",
            "- model.layers.22.input_layernorm.weight\n",
            "- model.layers.22.post_attention_layernorm.weight\n",
            "- model.layers.23.self_attn.q_proj.weight\n",
            "- model.layers.23.self_attn.k_proj.weight\n",
            "- model.layers.23.self_attn.v_proj.weight\n",
            "- model.layers.23.self_attn.o_proj.weight\n",
            "- model.layers.23.mlp.gate_proj.weight\n",
            "- model.layers.23.mlp.up_proj.weight\n",
            "- model.layers.23.mlp.down_proj.weight\n",
            "- model.layers.23.input_layernorm.weight\n",
            "- model.layers.23.post_attention_layernorm.weight\n",
            "- model.layers.24.self_attn.q_proj.weight\n",
            "- model.layers.24.self_attn.k_proj.weight\n",
            "- model.layers.24.self_attn.v_proj.weight\n",
            "- model.layers.24.self_attn.o_proj.weight\n",
            "- model.layers.24.mlp.gate_proj.weight\n",
            "- model.layers.24.mlp.up_proj.weight\n",
            "- model.layers.24.mlp.down_proj.weight\n",
            "- model.layers.24.input_layernorm.weight\n",
            "- model.layers.24.post_attention_layernorm.weight\n",
            "- model.layers.25.self_attn.q_proj.weight\n",
            "- model.layers.25.self_attn.k_proj.weight\n",
            "- model.layers.25.self_attn.v_proj.weight\n",
            "- model.layers.25.self_attn.o_proj.weight\n",
            "- model.layers.25.mlp.gate_proj.weight\n",
            "- model.layers.25.mlp.up_proj.weight\n",
            "- model.layers.25.mlp.down_proj.weight\n",
            "- model.layers.25.input_layernorm.weight\n",
            "- model.layers.25.post_attention_layernorm.weight\n",
            "- model.layers.26.self_attn.q_proj.weight\n",
            "- model.layers.26.self_attn.k_proj.weight\n",
            "- model.layers.26.self_attn.v_proj.weight\n",
            "- model.layers.26.self_attn.o_proj.weight\n",
            "- model.layers.26.mlp.gate_proj.weight\n",
            "- model.layers.26.mlp.up_proj.weight\n",
            "- model.layers.26.mlp.down_proj.weight\n",
            "- model.layers.26.input_layernorm.weight\n",
            "- model.layers.26.post_attention_layernorm.weight\n",
            "- model.layers.27.self_attn.q_proj.weight\n",
            "- model.layers.27.self_attn.k_proj.weight\n",
            "- model.layers.27.self_attn.v_proj.weight\n",
            "- model.layers.27.self_attn.o_proj.weight\n",
            "- model.layers.27.mlp.gate_proj.weight\n",
            "- model.layers.27.mlp.up_proj.weight\n",
            "- model.layers.27.mlp.down_proj.weight\n",
            "- model.layers.27.input_layernorm.weight\n",
            "- model.layers.27.post_attention_layernorm.weight\n",
            "- model.layers.28.self_attn.q_proj.weight\n",
            "- model.layers.28.self_attn.k_proj.weight\n",
            "- model.layers.28.self_attn.v_proj.weight\n",
            "- model.layers.28.self_attn.o_proj.weight\n",
            "- model.layers.28.mlp.gate_proj.weight\n",
            "- model.layers.28.mlp.up_proj.weight\n",
            "- model.layers.28.mlp.down_proj.weight\n",
            "- model.layers.28.input_layernorm.weight\n",
            "- model.layers.28.post_attention_layernorm.weight\n",
            "- model.layers.29.self_attn.q_proj.weight\n",
            "- model.layers.29.self_attn.k_proj.weight\n",
            "- model.layers.29.self_attn.v_proj.weight\n",
            "- model.layers.29.self_attn.o_proj.weight\n",
            "- model.layers.29.mlp.gate_proj.weight\n",
            "- model.layers.29.mlp.up_proj.weight\n",
            "- model.layers.29.mlp.down_proj.weight\n",
            "- model.layers.29.input_layernorm.weight\n",
            "- model.layers.29.post_attention_layernorm.weight\n",
            "- model.layers.30.self_attn.q_proj.weight\n",
            "- model.layers.30.self_attn.k_proj.weight\n",
            "- model.layers.30.self_attn.v_proj.weight\n",
            "- model.layers.30.self_attn.o_proj.weight\n",
            "- model.layers.30.mlp.gate_proj.weight\n",
            "- model.layers.30.mlp.up_proj.weight\n",
            "- model.layers.30.mlp.down_proj.weight\n",
            "- model.layers.30.input_layernorm.weight\n",
            "- model.layers.30.post_attention_layernorm.weight\n",
            "- model.layers.31.self_attn.q_proj.weight\n",
            "- model.layers.31.self_attn.k_proj.weight\n",
            "- model.layers.31.self_attn.v_proj.weight\n",
            "- model.layers.31.self_attn.o_proj.weight\n",
            "- model.layers.31.mlp.gate_proj.weight\n",
            "- model.layers.31.mlp.up_proj.weight\n",
            "- model.layers.31.mlp.down_proj.weight\n",
            "- model.layers.31.input_layernorm.weight\n",
            "- model.layers.31.post_attention_layernorm.weight\n",
            "- model.norm.weight\n",
            "- lm_head.weight\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True (cuda), used: False\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "[NeMo W 2024-12-12 10:01:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/setup.py:177: GPU available but not used. You can set it by doing `Trainer(accelerator='gpu')`.\n",
            "    \n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "nemo_config: {'mcore_gpt': True, 'micro_batch_size': 1, 'global_batch_size': 8, 'tensor_model_parallel_size': 1, 'pipeline_model_parallel_size': 1, 'virtual_pipeline_model_parallel_size': None, 'encoder_seq_length': 131072, 'max_position_embeddings': 131072, 'num_layers': 32, 'hidden_size': 4096, 'ffn_hidden_size': 14336, 'num_attention_heads': 32, 'init_method_std': 0.02, 'use_scaled_init_method': True, 'hidden_dropout': 0.0, 'attention_dropout': 0.0, 'ffn_dropout': 0.0, 'kv_channels': None, 'apply_query_key_layer_scaling': True, 'normalization': 'rmsnorm', 'layernorm_epsilon': 1e-05, 'do_layer_norm_weight_decay': False, 'make_vocab_size_divisible_by': 128, 'pre_process': True, 'post_process': True, 'persist_layer_norm': True, 'bias': False, 'activation': 'fast-swiglu', 'headscale': False, 'transformer_block_type': 'pre_ln', 'openai_gelu': False, 'normalize_attention_scores': True, 'position_embedding_type': 'rope', 'rotary_percentage': 1.0, 'attention_type': 'multihead', 'share_embeddings_and_output_weights': False, 'overlap_p2p_comm': False, 'batch_p2p_comm': True, 'num_query_groups': 8, 'tokenizer': {'library': 'huggingface', 'type': 'meta-llama/Llama-3.1-8B-Instruct', 'use_fast': True}, 'native_amp_init_scale': 4294967296, 'native_amp_growth_interval': 1000, 'hysteresis': 2, 'fp32_residual_connection': False, 'fp16_lm_cross_entropy': False, 'megatron_amp_O2': False, 'grad_allreduce_chunk_size_mb': 125, 'grad_div_ar_fusion': True, 'gradient_accumulation_fusion': False, 'bias_activation_fusion': False, 'bias_dropout_add_fusion': False, 'masked_softmax_fusion': True, 'get_attention_mask_from_fusion': True, 'apply_rope_fusion': False, 'seed': 1234, 'resume_from_checkpoint': None, 'use_cpu_initialization': True, 'onnx_safe': False, 'apex_transformer_log_level': 30, 'gradient_as_bucket_view': True, 'sync_batch_comm': False, 'activations_checkpoint_granularity': None, 'activations_checkpoint_method': None, 'activations_checkpoint_num_layers': None, 'num_micro_batches_with_partial_activation_checkpoints': None, 'activations_checkpoint_layers_per_pipeline': None, 'sequence_parallel': False, 'transformer_engine': True, 'fp8': False, 'fp8_e4m3': False, 'fp8_hybrid': True, 'fp8_margin': 0, 'fp8_interval': 1, 'fp8_amax_history_len': 1024, 'fp8_amax_compute_algo': 'max', 'reduce_amax': True, 'use_emha': False, 'data': {'index_mapping_dir': None, 'data_impl': 'mmap', 'splits_string': '900,50,50', 'seq_length': '${model.encoder_seq_length}', 'skip_warmup': True, 'num_workers': 2, 'dataloader_type': 'single', 'reset_position_ids': False, 'reset_attention_mask': False, 'eod_mask_loss': False, 'validation_drop_last': True, 'no_seqlen_plus_one_input_tokens': False, 'pad_samples_to_global_batch_size': False, 'shuffle_documents': True}, 'nsys_profile': {'enabled': False, 'start_step': 10, 'end_step': 10, 'ranks': [0], 'gen_shape': False}, 'optim': {'name': 'fused_adam', 'lr': 0.0002, 'weight_decay': 0.01, 'betas': [0.9, 0.98], 'sched': {'name': 'CosineAnnealing', 'warmup_steps': 500, 'constant_steps': 50000, 'min_lr': 2e-05}}, 'rotary_base': 500000.0, 'seq_len_interpolation_factor': 8.0, 'scale_positional_embedding': True, 'precision': 'bf16'}\n",
            "converting layer 0\n",
            "done layer 0\n",
            "converting layer 1\n",
            "done layer 1\n",
            "converting layer 2\n",
            "done layer 2\n",
            "converting layer 3\n",
            "done layer 3\n",
            "converting layer 4\n",
            "done layer 4\n",
            "converting layer 5\n",
            "done layer 5\n",
            "converting layer 6\n",
            "done layer 6\n",
            "converting layer 7\n",
            "done layer 7\n",
            "converting layer 8\n",
            "done layer 8\n",
            "converting layer 9\n",
            "done layer 9\n",
            "converting layer 10\n",
            "done layer 10\n",
            "converting layer 11\n",
            "done layer 11\n",
            "converting layer 12\n",
            "done layer 12\n",
            "converting layer 13\n",
            "done layer 13\n",
            "converting layer 14\n",
            "done layer 14\n",
            "converting layer 15\n",
            "done layer 15\n",
            "converting layer 16\n",
            "done layer 16\n",
            "converting layer 17\n",
            "done layer 17\n",
            "converting layer 18\n",
            "done layer 18\n",
            "converting layer 19\n",
            "done layer 19\n",
            "converting layer 20\n",
            "done layer 20\n",
            "converting layer 21\n",
            "done layer 21\n",
            "converting layer 22\n",
            "done layer 22\n",
            "converting layer 23\n",
            "done layer 23\n",
            "converting layer 24\n",
            "done layer 24\n",
            "converting layer 25\n",
            "done layer 25\n",
            "converting layer 26\n",
            "done layer 26\n",
            "converting layer 27\n",
            "done layer 27\n",
            "converting layer 28\n",
            "done layer 28\n",
            "converting layer 29\n",
            "done layer 29\n",
            "converting layer 30\n",
            "done layer 30\n",
            "converting layer 31\n",
            "done layer 31\n",
            "[NeMo I 2024-12-12 10:01:43 megatron_init:314] Rank 0 has data parallel group : [0]\n",
            "[NeMo I 2024-12-12 10:01:43 megatron_init:320] Rank 0 has combined group of data parallel and context parallel : [0]\n",
            "[NeMo I 2024-12-12 10:01:43 megatron_init:325] All data parallel group ranks with context parallel combined: [[0]]\n",
            "[NeMo I 2024-12-12 10:01:43 megatron_init:328] Ranks 0 has data parallel rank: 0\n",
            "[NeMo I 2024-12-12 10:01:43 megatron_init:336] Rank 0 has context parallel group: [0]\n",
            "[NeMo I 2024-12-12 10:01:43 megatron_init:339] All context parallel group ranks: [[0]]\n",
            "[NeMo I 2024-12-12 10:01:43 megatron_init:340] Ranks 0 has context parallel rank: 0\n",
            "[NeMo I 2024-12-12 10:01:43 megatron_init:347] Rank 0 has model parallel group: [0]\n",
            "[NeMo I 2024-12-12 10:01:43 megatron_init:348] All model parallel group ranks: [[0]]\n",
            "[NeMo I 2024-12-12 10:01:43 megatron_init:357] Rank 0 has tensor model parallel group: [0]\n",
            "[NeMo I 2024-12-12 10:01:43 megatron_init:361] All tensor model parallel group ranks: [[0]]\n",
            "[NeMo I 2024-12-12 10:01:43 megatron_init:362] Rank 0 has tensor model parallel rank: 0\n",
            "[NeMo I 2024-12-12 10:01:43 megatron_init:382] Rank 0 has pipeline model parallel group: [0]\n",
            "[NeMo I 2024-12-12 10:01:43 megatron_init:394] Rank 0 has embedding group: [0]\n",
            "[NeMo I 2024-12-12 10:01:43 megatron_init:400] All pipeline model parallel group ranks: [[0]]\n",
            "[NeMo I 2024-12-12 10:01:43 megatron_init:401] Rank 0 has pipeline model parallel rank 0\n",
            "[NeMo I 2024-12-12 10:01:43 megatron_init:402] All embedding group ranks: [[0]]\n",
            "[NeMo I 2024-12-12 10:01:43 megatron_init:403] Rank 0 has embedding rank: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:43 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 10:01:43 tokenizer_utils:184] Getting HuggingFace AutoTokenizer with pretrained_model_name: meta-llama/Llama-3.1-8B-Instruct\n",
            "[NeMo I 2024-12-12 10:01:44 megatron_base_model:601] Padded vocab_size: 128256, original vocab_size: 128256, dummy tokens: 0.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: context_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: deterministic_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cross_entropy_loss_fusion in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:516] apply_query_key_layer_scaling is only enabled when using FP16, setting it to False and setting NVTE_APPLY_QK_LAYER_SCALING=0\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: first_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: last_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: tp_only_amax_red in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_router_pre_softmax in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: external_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:01:44 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: config_logger_dir in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "CPU RNG state changed within GPU RNG context\n",
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/1\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=gloo\n",
            "All distributed processes registered. Starting with 1 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 10:02:33 dist_ckpt_io:421] Using TorchDistSaveShardedStrategy(torch_dist, 1) dist-ckpt save strategy.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 10:02:37 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:02:37 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:02:37 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:02:37 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:02:38 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 10:02:38 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 10:02:39 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:02:39 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 10:02:40 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:02:40 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 10:03:20 convert_llama_hf_to_nemo:335] NeMo model saved to: Llama-3.1-8B-Instruct.nemo\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 10:03:21 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp4s2x3qdl'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:03:21 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpl9l6ce3o'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "export HF_TOKEN='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'\n",
        "HF_MODEL=meta-llama/Llama-3.1-8B-Instruct\n",
        "\n",
        "# 確保模型完整下載\n",
        "huggingface-cli download $HF_MODEL --resume\n",
        "\n",
        "# 執行轉換\n",
        "python /opt/NeMo/scripts/checkpoint_converters/convert_llama_hf_to_nemo.py \\\n",
        "--input_name_or_path $HF_MODEL \\\n",
        "--output_path Llama-3.1-8B-Instruct.nemo \\\n",
        "--llama31 True \\\n",
        "--precision bf16"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6f70251c",
      "metadata": {
        "tags": [],
        "id": "6f70251c"
      },
      "source": [
        "### 1.4 Run pre-training <a name='s1.4'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1c744db-1bbd-41ca-9c33-44595dd88550",
      "metadata": {
        "id": "a1c744db-1bbd-41ca-9c33-44595dd88550"
      },
      "source": [
        "# 8張H100測試用code (8分鐘跑完)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "95a33b58-893d-4cea-88d4-bdd7cddbbc54",
      "metadata": {
        "tags": [],
        "id": "95a33b58-893d-4cea-88d4-bdd7cddbbc54",
        "outputId": "a622f1a9-d6bc-46a9-8daa-ca7901576187"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 10:43:00 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:43:00 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:43:00 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:43:00 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:43:01 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 10:43:01 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 10:43:02 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:43:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 10:43:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:43:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:43:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "      ret = run_job(\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 10:43:03 megatron_gpt_pretraining:37] \n",
            "    \n",
            "    ************** Experiment configuration ***********\n",
            "[NeMo I 2024-12-12 10:43:03 megatron_gpt_pretraining:38] \n",
            "    run:\n",
            "      name: llama3_1_8b\n",
            "      results_dir: ${base_results_dir}/${.name}\n",
            "      time_limit: 0-01:30:00\n",
            "      dependency: singleton\n",
            "    trainer:\n",
            "      num_nodes: 1\n",
            "      devices: 8\n",
            "      accelerator: gpu\n",
            "      precision: bf16\n",
            "      logger: false\n",
            "      enable_checkpointing: false\n",
            "      use_distributed_sampler: false\n",
            "      max_epochs: null\n",
            "      max_steps: 100\n",
            "      max_time: 05:23:30:00\n",
            "      log_every_n_steps: 10\n",
            "      val_check_interval: 100\n",
            "      limit_val_batches: 1\n",
            "      limit_test_batches: 50\n",
            "      accumulate_grad_batches: 1\n",
            "      gradient_clip_val: 1.0\n",
            "    exp_manager:\n",
            "      explicit_log_dir: /workspace/results/Llama-3.1-8B/pretrain\n",
            "      exp_dir: null\n",
            "      name: megatron_llama\n",
            "      create_wandb_logger: false\n",
            "      wandb_logger_kwargs:\n",
            "        project: nemo_llama_pretrain\n",
            "        name: Llama-3.1-8B\n",
            "      resume_if_exists: true\n",
            "      resume_ignore_no_checkpoint: true\n",
            "      create_checkpoint_callback: true\n",
            "      checkpoint_callback_params:\n",
            "        monitor: val_loss\n",
            "        save_top_k: 10\n",
            "        mode: min\n",
            "        always_save_nemo: false\n",
            "        save_nemo_on_train_end: true\n",
            "        filename: megatron_llama--{val_loss:.2f}-{step}-{consumed_samples}\n",
            "        model_parallel_size: 8\n",
            "      log_step_timing: true\n",
            "      step_timing_kwargs:\n",
            "        sync_cuda: true\n",
            "        buffer_size: 5\n",
            "      seconds_to_sleep: 60\n",
            "    model:\n",
            "      mcore_gpt: true\n",
            "      micro_batch_size: 2\n",
            "      global_batch_size: 8\n",
            "      rampup_batch_size: null\n",
            "      tensor_model_parallel_size: 4\n",
            "      pipeline_model_parallel_size: 2\n",
            "      virtual_pipeline_model_parallel_size: null\n",
            "      context_parallel_size: 1\n",
            "      encoder_seq_length: 8192\n",
            "      max_position_embeddings: 8192\n",
            "      num_layers: 32\n",
            "      hidden_size: 4096\n",
            "      ffn_hidden_size: 14336\n",
            "      num_attention_heads: 32\n",
            "      num_query_groups: 8\n",
            "      init_method_std: 0.02\n",
            "      use_scaled_init_method: true\n",
            "      hidden_dropout: 0.0\n",
            "      attention_dropout: 0.0\n",
            "      ffn_dropout: 0.0\n",
            "      kv_channels: null\n",
            "      apply_query_key_layer_scaling: true\n",
            "      normalization: rmsnorm\n",
            "      layernorm_epsilon: 1.0e-05\n",
            "      do_layer_norm_weight_decay: false\n",
            "      make_vocab_size_divisible_by: 128\n",
            "      pre_process: true\n",
            "      post_process: true\n",
            "      persist_layer_norm: true\n",
            "      bias: false\n",
            "      activation: fast-swiglu\n",
            "      headscale: false\n",
            "      transformer_block_type: pre_ln\n",
            "      openai_gelu: false\n",
            "      normalize_attention_scores: true\n",
            "      position_embedding_type: rope\n",
            "      rotary_percentage: 1.0\n",
            "      apply_rope_fusion: true\n",
            "      cross_entropy_loss_fusion: true\n",
            "      attention_type: multihead\n",
            "      share_embeddings_and_output_weights: false\n",
            "      scale_positional_embedding: true\n",
            "      tokenizer:\n",
            "        library: huggingface\n",
            "        type: meta-llama/Meta-Llama-3.1-8B\n",
            "        use_fast: true\n",
            "      native_amp_init_scale: 4294967296\n",
            "      native_amp_growth_interval: 1000\n",
            "      hysteresis: 2\n",
            "      fp32_residual_connection: false\n",
            "      fp16_lm_cross_entropy: false\n",
            "      megatron_amp_O2: true\n",
            "      grad_allreduce_chunk_size_mb: 125\n",
            "      grad_div_ar_fusion: true\n",
            "      gradient_accumulation_fusion: true\n",
            "      bias_activation_fusion: true\n",
            "      bias_dropout_add_fusion: true\n",
            "      masked_softmax_fusion: true\n",
            "      seed: 1234\n",
            "      resume_from_checkpoint: null\n",
            "      use_cpu_initialization: false\n",
            "      onnx_safe: false\n",
            "      apex_transformer_log_level: 30\n",
            "      gradient_as_bucket_view: true\n",
            "      sync_batch_comm: false\n",
            "      activations_checkpoint_granularity: null\n",
            "      activations_checkpoint_method: null\n",
            "      activations_checkpoint_num_layers: null\n",
            "      num_micro_batches_with_partial_activation_checkpoints: null\n",
            "      activations_checkpoint_layers_per_pipeline: null\n",
            "      sequence_parallel: false\n",
            "      deterministic_mode: false\n",
            "      transformer_engine: true\n",
            "      fp8: false\n",
            "      fp8_e4m3: false\n",
            "      fp8_hybrid: false\n",
            "      fp8_margin: 0\n",
            "      fp8_interval: 1\n",
            "      fp8_amax_history_len: 1024\n",
            "      fp8_amax_compute_algo: max\n",
            "      ub_tp_comm_overlap: false\n",
            "      use_flash_attention: true\n",
            "      gc_interval: 100\n",
            "      nsys_profile:\n",
            "        enabled: false\n",
            "        trace:\n",
            "        - nvtx\n",
            "        - cuda\n",
            "        start_step: 10\n",
            "        end_step: 10\n",
            "        ranks:\n",
            "        - 0\n",
            "        gen_shape: false\n",
            "      optim:\n",
            "        name: distributed_fused_adam\n",
            "        lr: 0.0005\n",
            "        weight_decay: 0.1\n",
            "        betas:\n",
            "        - 0.9\n",
            "        - 0.95\n",
            "        bucket_cap_mb: 125\n",
            "        overlap_grad_sync: true\n",
            "        overlap_param_sync: true\n",
            "        contiguous_grad_buffer: true\n",
            "        contiguous_param_buffer: true\n",
            "        sched:\n",
            "          name: CosineAnnealing\n",
            "          warmup_steps: 500\n",
            "          constant_steps: 0\n",
            "          min_lr: 1.0e-05\n",
            "      data:\n",
            "        data_impl: mmap\n",
            "        splits_string: 9990,8,2\n",
            "        seq_length: 8192\n",
            "        skip_warmup: true\n",
            "        num_workers: 8\n",
            "        dataloader_type: single\n",
            "        reset_position_ids: true\n",
            "        reset_attention_mask: true\n",
            "        eod_mask_loss: false\n",
            "        index_mapping_dir: null\n",
            "        data_prefix:\n",
            "        - 1.0\n",
            "        - data/custom_dataset/preprocessed/wikinews_text_document\n",
            "      restore_from_path: Llama-3.1-8B-Instruct.nemo\n",
            "      rotary_base: 500000.0\n",
            "      seq_len_interpolation_factor: 8\n",
            "    base_results_dir: results\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 10:43:04 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
            "    \n",
            "[NeMo W 2024-12-12 10:43:04 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py:1395: DeprecationWarning: torch.set_autocast_gpu_dtype(dtype) is deprecated. Please use torch.set_autocast_dtype('cuda', dtype) instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:678.)\n",
            "      torch.set_autocast_gpu_dtype(dtype)\n",
            "    \n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(limit_val_batches=1)` was configured so 1 batch will be used.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 10:43:04 exp_manager:400] ExpManager schema\n",
            "[NeMo I 2024-12-12 10:43:04 exp_manager:401] {'explicit_log_dir': None, 'exp_dir': None, 'name': None, 'version': None, 'use_datetime_version': True, 'resume_if_exists': False, 'resume_past_end': False, 'resume_ignore_no_checkpoint': False, 'resume_from_checkpoint': None, 'create_tensorboard_logger': True, 'summary_writer_kwargs': None, 'create_wandb_logger': False, 'wandb_logger_kwargs': None, 'create_mlflow_logger': False, 'mlflow_logger_kwargs': {'experiment_name': None, 'tracking_uri': None, 'tags': None, 'save_dir': './mlruns', 'prefix': '', 'artifact_location': None, 'run_id': None, 'log_model': False}, 'create_dllogger_logger': False, 'dllogger_logger_kwargs': {'verbose': False, 'stdout': False, 'json_file': './dllogger.json'}, 'create_clearml_logger': False, 'clearml_logger_kwargs': {'project': None, 'task': None, 'connect_pytorch': False, 'model_name': None, 'tags': None, 'log_model': False, 'log_cfg': False, 'log_metrics': False}, 'create_neptune_logger': False, 'neptune_logger_kwargs': None, 'create_checkpoint_callback': True, 'checkpoint_callback_params': {'filepath': None, 'dirpath': None, 'filename': None, 'monitor': 'val_loss', 'verbose': True, 'save_last': True, 'save_top_k': 3, 'save_weights_only': False, 'mode': 'min', 'auto_insert_metric_name': True, 'every_n_epochs': 1, 'every_n_train_steps': None, 'train_time_interval': None, 'prefix': None, 'postfix': '.nemo', 'save_best_model': False, 'always_save_nemo': False, 'save_nemo_on_train_end': True, 'model_parallel_size': None, 'save_on_train_epoch_end': False, 'async_save': False}, 'create_early_stopping_callback': False, 'early_stopping_callback_params': {'monitor': 'val_loss', 'mode': 'min', 'min_delta': 0.001, 'patience': 10, 'verbose': True, 'strict': True, 'check_finite': True, 'stopping_threshold': None, 'divergence_threshold': None, 'check_on_train_epoch_end': None, 'log_rank_zero_only': False}, 'create_preemption_callback': True, 'files_to_copy': None, 'log_step_timing': True, 'step_timing_kwargs': {'reduction': 'mean', 'sync_cuda': False, 'buffer_size': 1}, 'log_local_rank_0_only': False, 'log_global_rank_0_only': False, 'disable_validation_on_resume': True, 'ema': {'enable': False, 'decay': 0.999, 'cpu_offload': False, 'validate_original_weights': False, 'every_n_steps': 1}, 'max_time_per_run': None, 'seconds_to_sleep': 5.0, 'create_straggler_detection_callback': False, 'straggler_detection_params': {'report_time_interval': 300.0, 'calc_relative_gpu_perf': True, 'calc_individual_gpu_perf': True, 'num_gpu_perf_scores_to_log': 5, 'gpu_relative_perf_threshold': 0.7, 'gpu_individual_perf_threshold': 0.7, 'stop_if_detected': False}, 'create_fault_tolerance_callback': False, 'fault_tolerance': {'workload_check_interval': 5.0, 'initial_rank_heartbeat_timeout': 3600.0, 'rank_heartbeat_timeout': 2700.0, 'calculate_timeouts': True, 'safety_factor': 5.0, 'rank_termination_signal': <Signals.SIGKILL: 9>, 'log_level': 'INFO', 'max_rank_restarts': 0, 'max_subsequent_job_failures': 0, 'additional_ft_launcher_args': '', 'simulated_fault': None}, 'log_tflops_per_sec_per_gpu': True}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 10:43:04 exp_manager:784] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/workspace/results/Llama-3.1-8B/pretrain/checkpoints. Training from scratch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 10:43:04 exp_manager:459] Experiments will be logged at /workspace/results/Llama-3.1-8B/pretrain\n",
            "[NeMo I 2024-12-12 10:43:04 exp_manager:1010] TensorboardLogger has been set up\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 10:43:04 exp_manager:1139] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 100. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 10:43:04 exp_manager:593] TFLOPs per sec per GPU will be calculated, conditioned on supported models. Defaults to -1 upon failure.\n",
            "[NeMo I 2024-12-12 10:43:04 megatron_gpt_pretraining:46] Continual training: loading weights from Llama-3.1-8B-Instruct.nemo\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 10:43:21 megatron_init:314] Rank 0 has data parallel group : [0]\n",
            "[NeMo I 2024-12-12 10:43:21 megatron_init:320] Rank 0 has combined group of data parallel and context parallel : [0]\n",
            "[NeMo I 2024-12-12 10:43:21 megatron_init:325] All data parallel group ranks with context parallel combined: [[0], [1], [2], [3], [4], [5], [6], [7]]\n",
            "[NeMo I 2024-12-12 10:43:21 megatron_init:328] Ranks 0 has data parallel rank: 0\n",
            "[NeMo I 2024-12-12 10:43:21 megatron_init:336] Rank 0 has context parallel group: [0]\n",
            "[NeMo I 2024-12-12 10:43:21 megatron_init:339] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]\n",
            "[NeMo I 2024-12-12 10:43:21 megatron_init:340] Ranks 0 has context parallel rank: 0\n",
            "[NeMo I 2024-12-12 10:43:21 megatron_init:347] Rank 0 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]\n",
            "[NeMo I 2024-12-12 10:43:21 megatron_init:348] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]\n",
            "[NeMo I 2024-12-12 10:43:21 megatron_init:357] Rank 0 has tensor model parallel group: [0, 1, 2, 3]\n",
            "[NeMo I 2024-12-12 10:43:21 megatron_init:361] All tensor model parallel group ranks: [[0, 1, 2, 3], [4, 5, 6, 7]]\n",
            "[NeMo I 2024-12-12 10:43:21 megatron_init:362] Rank 0 has tensor model parallel rank: 0\n",
            "[NeMo I 2024-12-12 10:43:21 megatron_init:382] Rank 0 has pipeline model parallel group: [0, 4]\n",
            "[NeMo I 2024-12-12 10:43:21 megatron_init:394] Rank 0 has embedding group: [0, 4]\n",
            "[NeMo I 2024-12-12 10:43:21 megatron_init:400] All pipeline model parallel group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]\n",
            "[NeMo I 2024-12-12 10:43:21 megatron_init:401] Rank 0 has pipeline model parallel rank 0\n",
            "[NeMo I 2024-12-12 10:43:21 megatron_init:402] All embedding group ranks: [[0, 4], [1, 5], [2, 6], [3, 7]]\n",
            "[NeMo I 2024-12-12 10:43:21 megatron_init:403] Rank 0 has embedding rank: 0\n",
            "[NeMo I 2024-12-12 10:43:21 num_microbatches_calculator:218] setting number of microbatches to constant 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:21 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 10:43:21 tokenizer_utils:184] Getting HuggingFace AutoTokenizer with pretrained_model_name: meta-llama/Meta-Llama-3.1-8B\n",
            "[NeMo I 2024-12-12 10:43:27 megatron_base_model:601] Padded vocab_size: 128512, original vocab_size: 128256, dummy tokens: 256.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:1186] The model: MegatronGPTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:516] apply_query_key_layer_scaling is only enabled when using FP16, setting it to False and setting NVTE_APPLY_QK_LAYER_SCALING=0\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: first_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: last_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: tp_only_amax_red in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_router_pre_softmax in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: external_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 10:43:27 megatron_base_model:574] The model: MegatronGPTModel() does not have field.name: config_logger_dir in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "apply rope scaling ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "apply rope scaling ...\n",
            "apply rope scaling ...\n",
            "apply rope scaling ...\n",
            "apply rope scaling ...\n",
            "apply rope scaling ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "apply rope scaling ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
            "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
            "Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
            "Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n",
            "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "apply rope scaling ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 8 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "[NeMo W 2024-12-12 10:45:38 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:755: FutureWarning: `load_state_dict` is deprecated and will be removed in future versions. Please use `load` instead.\n",
            "      checkpoint.load_state_dict(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:45:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/distributed/checkpoint/planner_helpers.py:311: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n",
            "      device = getattr(value, \"device\", None)\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 10:45:49 nlp_overrides:1358] Model MegatronGPTModel was successfully restored from /workspace/Llama-3.1-8B-Instruct.nemo.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 10:45:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:161: You have overridden `MegatronGPTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.\n",
            "    \n",
            "[NeMo W 2024-12-12 10:45:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:143: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.\n",
            "    \n",
            "[NeMo W 2024-12-12 10:45:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      DESCRIPTOR = _descriptor.FileDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:45:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _descriptor.FieldDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:45:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:45:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      DESCRIPTOR = _descriptor.FileDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:45:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _descriptor.EnumValueDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:45:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _DATATYPE = _descriptor.EnumDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:45:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      DESCRIPTOR = _descriptor.FileDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:45:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _descriptor.FieldDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:45:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:45:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      DESCRIPTOR = _descriptor.FileDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:45:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _descriptor.FieldDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:45:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _TENSORPROTO = _descriptor.Descriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:45:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      DESCRIPTOR = _descriptor.FileDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:45:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:35: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _descriptor.EnumValueDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:45:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:29: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _DATACLASS = _descriptor.EnumDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:45:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:74: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _descriptor.FieldDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:45:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:67: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _SUMMARYDESCRIPTION = _descriptor.Descriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:45:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:326: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "      np.bool8: (False, True),\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 10:45:50 megatron_gpt_model:1680] Pipeline model parallel rank: 0, Tensor model parallel rank: 0, Number of model parameters on device: 1.00e+09. Number of precise model parameters on device: 8033157120.\n",
            "[NeMo I 2024-12-12 10:45:50 megatron_gpt_model:1524] Building GPT datasets.\n",
            "[NeMo I 2024-12-12 10:45:50 utils:259] Let split_matrix = [(0, 0.999), (0.999, 0.9998), (0.9998, 1.0)]\n",
            "[NeMo I 2024-12-12 10:45:50 utils:259] Building dataset splits with cls=GPTDataset, sizes=[800, 16, 400], and config=GPTDatasetConfig(random_seed=1234, sequence_length=8192, blend=(['data/custom_dataset/preprocessed/wikinews_text_document'], [1.0]), blend_per_split=None, renormalize_blend_weights=False, split='9990,8,2', split_matrix=[(0, 0.999), (0.999, 0.9998), (0.9998, 1.0)], num_dataset_builder_threads=1, path_to_cache=None, mmap_bin_files=True, mock=False, tokenizer=<nemo.collections.common.tokenizers.huggingface.auto_tokenizer.AutoTokenizer object at 0x7fb1cb19c640>, reset_position_ids=True, reset_attention_mask=True, eod_mask_loss=False, create_attention_mask=False, drop_last_partial_validation_sequence=True, add_extra_token_to_sequence=True, s3_cache_path=None)\n",
            "[NeMo I 2024-12-12 10:45:50 utils:259] Load the _IndexReader from data/custom_dataset/preprocessed/wikinews_text_document.idx\n",
            "[NeMo I 2024-12-12 10:45:50 utils:259] \tExtract the sequence lengths\n",
            "[NeMo I 2024-12-12 10:45:50 utils:259] \tExtract the sequence pointers\n",
            "[NeMo I 2024-12-12 10:45:50 utils:259] \tExtract the document indices\n",
            "[NeMo I 2024-12-12 10:45:50 utils:259] > total number of sequences: 9827\n",
            "[NeMo I 2024-12-12 10:45:50 utils:259] > total number of documents: 9827\n",
            "[NeMo I 2024-12-12 10:45:50 utils:259] Build and save the GPTDataset train indices\n",
            "[NeMo I 2024-12-12 10:45:50 utils:259] > total number of samples: 959\n",
            "[NeMo I 2024-12-12 10:45:50 utils:259] > total number of epochs: 2\n",
            "[NeMo I 2024-12-12 10:45:50 utils:259] Build and save the GPTDataset valid indices\n",
            "[NeMo I 2024-12-12 10:45:50 utils:259] > total number of samples: 17\n",
            "[NeMo I 2024-12-12 10:45:50 utils:259] > total number of epochs: 21\n",
            "[NeMo I 2024-12-12 10:45:50 utils:259] Build and save the GPTDataset test indices\n",
            "[NeMo I 2024-12-12 10:45:50 utils:259] > total number of samples: 402\n",
            "[NeMo I 2024-12-12 10:45:50 utils:259] > total number of epochs: 2179\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 10:45:50 utils:259] Building a BlendedDataset for a single MegatronDataset\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 10:45:50 utils:259] Build and save the BlendedDataset indices\n",
            "[NeMo I 2024-12-12 10:45:50 utils:259] \tBuild and save the dataset and dataset sample indexes\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 10:45:50 utils:259] Unable to save the BlendedDataset indexes because path_to_cache is None\n",
            "[NeMo W 2024-12-12 10:45:50 utils:259] Building a BlendedDataset for a single MegatronDataset\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 10:45:50 utils:259] Build and save the BlendedDataset indices\n",
            "[NeMo I 2024-12-12 10:45:50 utils:259] \tBuild and save the dataset and dataset sample indexes\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 10:45:50 utils:259] Unable to save the BlendedDataset indexes because path_to_cache is None\n",
            "[NeMo W 2024-12-12 10:45:50 utils:259] Building a BlendedDataset for a single MegatronDataset\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 10:45:50 utils:259] Build and save the BlendedDataset indices\n",
            "[NeMo I 2024-12-12 10:45:50 utils:259] \tBuild and save the dataset and dataset sample indexes\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 10:45:50 utils:259] Unable to save the BlendedDataset indexes because path_to_cache is None\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 10:45:50 utils:259] Verifying NumPy indices for BlendedDataset train split\n",
            "[NeMo I 2024-12-12 10:45:50 utils:259] Verifying NumPy indices for BlendedDataset valid split\n",
            "[NeMo I 2024-12-12 10:45:50 utils:259] Verifying NumPy indices for BlendedDataset test split\n",
            "[NeMo I 2024-12-12 10:45:50 megatron_gpt_model:1613] Length of train dataset: 804\n",
            "[NeMo I 2024-12-12 10:45:50 megatron_gpt_model:1615] Length of val dataset: 17\n",
            "[NeMo I 2024-12-12 10:45:50 megatron_gpt_model:1617] Length of test dataset: 402\n",
            "[NeMo I 2024-12-12 10:45:50 megatron_gpt_model:1618] Finished building GPT datasets.\n",
            "[NeMo I 2024-12-12 10:45:50 megatron_gpt_model:1729] Setting up train dataloader with len(len(self._train_ds)): 804 and consumed samples: 0\n",
            "[NeMo I 2024-12-12 10:45:50 megatron_gpt_model:1627] Building dataloader with consumed samples: 0\n",
            "[NeMo I 2024-12-12 10:45:50 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 804 and consumed_samples: 0\n",
            "[NeMo I 2024-12-12 10:45:50 megatron_gpt_model:1737] Setting up validation dataloader with len(len(self._validation_ds)): 17 and consumed samples: 0\n",
            "[NeMo I 2024-12-12 10:45:50 megatron_gpt_model:1627] Building dataloader with consumed samples: 0\n",
            "[NeMo I 2024-12-12 10:45:50 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 17 and consumed_samples: 0\n",
            "[NeMo I 2024-12-12 10:45:50 megatron_gpt_model:1758] Setting up test dataloader with len(len(self._test_ds)): 402 and consumed samples: 0\n",
            "[NeMo I 2024-12-12 10:45:50 megatron_gpt_model:1627] Building dataloader with consumed samples: 0\n",
            "[NeMo I 2024-12-12 10:45:50 data_samplers:76] Instantiating MegatronPretrainingSampler with total_samples: 402 and consumed_samples: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
            "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
            "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
            "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
            "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
            "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
            "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 10:45:50 modelPT:787] Optimizer config = MegatronDistributedFusedAdam (\n",
            "    Parameter Group 0\n",
            "        betas: [0.9, 0.95]\n",
            "        bias_correction: True\n",
            "        eps: 1e-08\n",
            "        is_expert: False\n",
            "        lr: 0.0005\n",
            "        weight_decay: 0.1\n",
            "    )\n",
            "[NeMo I 2024-12-12 10:45:50 lr_scheduler:948] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7fb187364250>\" \n",
            "    will be used during training (effective maximum steps = 100) - \n",
            "    Parameters : \n",
            "    (warmup_steps: 500\n",
            "    constant_steps: 0\n",
            "    min_lr: 1.0e-05\n",
            "    max_steps: 100\n",
            "    )\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  | Name  | Type          | Params | Mode \n",
            "------------------------------------------------\n",
            "0 | model | Float16Module | 1.0 B  | train\n",
            "------------------------------------------------\n",
            "1.0 B     Trainable params\n",
            "0         Non-trainable params\n",
            "1.0 B     Total params\n",
            "4,016.570 Total estimated model params size (MB)\n",
            "328       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "[NeMo W 2024-12-12 10:45:50 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
            "    \n",
            "[NeMo W 2024-12-12 10:45:55 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:45:55 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:45:55 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:45:55 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:45:55 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 10:45:56 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 10:45:56 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:45:57 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 10:45:57 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:45:57 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:03 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:03 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:03 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:03 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:03 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:03 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:04 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:05 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:05 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:05 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:10 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:10 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:10 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:10 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:11 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:11 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:12 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:13 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:13 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:13 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:18 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:18 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:18 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:18 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:19 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:19 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:20 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:21 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:21 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:21 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:26 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:26 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:26 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:26 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:27 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:27 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:27 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:28 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:29 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:29 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:34 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:34 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:34 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:34 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:34 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:35 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:35 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:36 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:36 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:36 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:41 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:41 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:41 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:41 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:42 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:42 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:43 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:44 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:44 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:44 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:49 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:49 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:49 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:49 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:50 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:50 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:51 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:52 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:52 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:46:52 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:08 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:08 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:13 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:13 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:13 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:13 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:14 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:14 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:15 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:16 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:16 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:16 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:22 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:22 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:22 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:22 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:23 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:23 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:24 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:25 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:25 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:25 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:31 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:31 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:31 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:31 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:31 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:32 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:32 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:33 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:33 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:33 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:39 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:39 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:39 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:39 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:40 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:40 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:40 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:41 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:42 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:42 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:47 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:47 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:47 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:47 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:48 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:48 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:50 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:50 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:50 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:55 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:55 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:55 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:55 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:56 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:56 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:57 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:58 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:58 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:47:58 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:48:03 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:48:03 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:48:03 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:48:03 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:48:04 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 10:48:04 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 10:48:05 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:48:06 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 10:48:06 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:48:06 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:48:11 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:48:11 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:48:11 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:48:11 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:48:12 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 10:48:12 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 10:48:13 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:48:14 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 10:48:14 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:48:14 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:50:31 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: : 100%|██████████| 100/100 [02:14<00:00, reduced_train_loss=2.050, global_step=99.00, consumed_samples=800.0, train_step_timing in s=1.260]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A\n",
            "Validation:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/4 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 4/4 [00:00<00:00,  9.22it/s]\u001b[A\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 0, global step 100: 'val_loss' reached 1.82502 (best 1.82502), saving model to '/workspace/results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama--val_loss=1.83-step=100-consumed_samples=800.0.ckpt' as top 10\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: : 100%|██████████| 100/100 [02:16<00:00, reduced_train_loss=2.050, global_step=99.00, consumed_samples=800.0, train_step_timing in s=1.260, val_loss=1.830][NeMo I 2024-12-12 10:50:35 dist_ckpt_io:421] Using TorchDistSaveShardedStrategy(torch_dist, 1) dist-ckpt save strategy.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 10:50:42 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:50:42 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:50:42 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 10:50:42 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:50:43 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 10:50:43 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 10:50:43 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[NeMo W 2024-12-12 10:50:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 10:50:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 10:50:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "`Trainer.fit` stopped: `max_steps=100` reached.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: : 100%|██████████| 100/100 [02:54<00:00, reduced_train_loss=2.050, global_step=99.00, consumed_samples=800.0, train_step_timing in s=1.260, val_loss=1.830]\n",
            "[NeMo I 2024-12-12 10:51:11 perf_metrics_utils:42] train_step_timing in s: [1.26, 1.26, 1.26, 1.26, 1.26, 1.26, 1.27, 1.26, 1.26, 1.26]\n",
            "[NeMo I 2024-12-12 10:51:11 perf_metrics:86] TFLOPs per sec per GPU=375.93\n",
            "[NeMo I 2024-12-12 10:51:11 dist_ckpt_io:421] Using TorchDistSaveShardedStrategy(torch_dist, 1) dist-ckpt save strategy.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[NeMo W 2024-12-12 10:51:37 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp8o3ycumq'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:51:37 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp1wdn0ver'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:51:37 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpkykqmwoe'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:51:37 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpt_pa0wiv'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:51:37 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpd8t1_v8n'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:51:37 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpa0cimhbn'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:51:37 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpx287lc3j'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:51:37 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpuohrn19o'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:51:39 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp_ezr3gpy'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:51:39 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp8sqx1kjv'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:51:39 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp7c_amyvp'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:51:39 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpuidyrh97'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:51:39 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpugmr5etk'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:51:39 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpjldg6wwb'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:51:39 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp90pfcm60'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:51:39 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp0a43hy91'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:51:43 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpfvfofp8z'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n",
            "[NeMo W 2024-12-12 10:51:43 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpj5qxnvqd'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "MODEL_NAME=Llama-3.1-8B\n",
        "MODEL=Llama-3.1-8B-Instruct.nemo\n",
        "NUM_GPUS=8\n",
        "MAX_STEPS=100\n",
        "MBS=2\n",
        "GBS=8\n",
        "TP=4\n",
        "PP=2\n",
        "CP=1\n",
        "LR=5e-4\n",
        "DATA_SPLITS=\\'9990,8,2\\'\n",
        "DATA_PREFIX=[1.0,data/custom_dataset/preprocessed/wikinews_text_document]\n",
        "export CUDA_DEVICE_MAX_CONNECTIONS=1\n",
        "export HUGGING_FACE_HUB_TOKEN='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'\n",
        "\n",
        "python /opt/NeMo/examples/nlp/language_modeling/megatron_gpt_pretraining.py \\\n",
        "--config-path=/opt/NeMo-Framework-Launcher/launcher_scripts/conf/training/llama --config-name=llama3_1_8b \\\n",
        "+base_results_dir=results \\\n",
        "trainer.num_nodes=1 \\\n",
        "trainer.devices=$NUM_GPUS \\\n",
        "trainer.max_steps=$MAX_STEPS \\\n",
        "trainer.limit_val_batches=1 \\\n",
        "trainer.val_check_interval=100 \\\n",
        "exp_manager.explicit_log_dir=/workspace/results/$MODEL_NAME/pretrain \\\n",
        "exp_manager.wandb_logger_kwargs.name=$MODEL_NAME \\\n",
        "exp_manager.resume_if_exists=True \\\n",
        "exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True \\\n",
        "exp_manager.checkpoint_callback_params.model_parallel_size=$(($TP*$PP)) \\\n",
        "model.micro_batch_size=$MBS \\\n",
        "model.global_batch_size=$GBS \\\n",
        "model.tensor_model_parallel_size=$TP \\\n",
        "model.pipeline_model_parallel_size=$PP \\\n",
        "model.context_parallel_size=$CP \\\n",
        "model.init_method_std=0.02 \\\n",
        "model.optim.lr=$LR \\\n",
        "model.data.splits_string=${DATA_SPLITS} \\\n",
        "model.data.data_prefix=${DATA_PREFIX} \\\n",
        "model.data.num_workers=8 \\\n",
        "+model.restore_from_path=$MODEL \\\n",
        "+model.rotary_base=500000.0 \\\n",
        "+model.seq_len_interpolation_factor=8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a43cb2e3-385a-444d-a0e5-4e2e2da3ae2d",
      "metadata": {
        "id": "a43cb2e3-385a-444d-a0e5-4e2e2da3ae2d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "6ddce7fb-bc16-4448-baeb-ce7a5e9c866e",
      "metadata": {
        "id": "6ddce7fb-bc16-4448-baeb-ce7a5e9c866e"
      },
      "source": [
        "# 原始的CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25591419-518b-451e-afcc-b8a1d65f82a7",
      "metadata": {
        "id": "25591419-518b-451e-afcc-b8a1d65f82a7"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "MODEL_NAME=Llama-3.1-8B\n",
        "MODEL=Llama-3.1-8B-Instruct.nemo\n",
        "NUM_GPUS=8\n",
        "MAX_STEPS=100\n",
        "MBS=1\n",
        "GBS=2\n",
        "TP=2\n",
        "PP=1\n",
        "CP=1\n",
        "LR=1e-4\n",
        "DATA_SPLITS=\\'9990,8,2\\'\n",
        "DATA_PREFIX=[1.0,data/custom_dataset/preprocessed/wikinews_text_document]\n",
        "export CUDA_DEVICE_MAX_CONNECTIONS=1\n",
        "export HUGGING_FACE_HUB_TOKEN='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'\n",
        "\n",
        "python /opt/NeMo/examples/nlp/language_modeling/megatron_gpt_pretraining.py \\\n",
        "--config-path=/opt/NeMo-Framework-Launcher/launcher_scripts/conf/training/llama --config-name=llama3_1_8b \\\n",
        "+base_results_dir=results \\\n",
        "trainer.num_nodes=1 \\\n",
        "trainer.devices=$NUM_GPUS \\\n",
        "trainer.max_steps=$MAX_STEPS \\\n",
        "trainer.limit_val_batches=1 \\\n",
        "trainer.val_check_interval=50 \\\n",
        "exp_manager.explicit_log_dir=/workspace/results/$MODEL_NAME/pretrain \\\n",
        "exp_manager.wandb_logger_kwargs.name=$MODEL_NAME \\\n",
        "exp_manager.resume_if_exists=True \\\n",
        "exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True \\\n",
        "exp_manager.checkpoint_callback_params.model_parallel_size=$(($TP*$PP)) \\\n",
        "model.micro_batch_size=$MBS \\\n",
        "model.global_batch_size=$GBS \\\n",
        "model.tensor_model_parallel_size=$TP \\\n",
        "model.pipeline_model_parallel_size=$PP \\\n",
        "model.context_parallel_size=$CP \\\n",
        "model.init_method_std=0.02 \\\n",
        "model.optim.lr=$LR \\\n",
        "model.data.splits_string=${DATA_SPLITS} \\\n",
        "model.data.data_prefix=${DATA_PREFIX} \\\n",
        "model.data.num_workers=0 \\\n",
        "+model.restore_from_path=$MODEL \\\n",
        "+model.rotary_base=500000.0 \\\n",
        "+model.seq_len_interpolation_factor=8"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "472eed15-b2c8-4596-8e55-b738a1f5fd3c",
      "metadata": {
        "id": "472eed15-b2c8-4596-8e55-b738a1f5fd3c"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "d6217b98-0a81-4519-97dc-5490fae14709",
      "metadata": {
        "id": "d6217b98-0a81-4519-97dc-5490fae14709"
      },
      "source": [
        "# 嘗試離線運行"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8f190e7-0912-4428-aac0-996b64abfaae",
      "metadata": {
        "id": "c8f190e7-0912-4428-aac0-996b64abfaae"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "MODEL_NAME=Llama-3.1-8B\n",
        "MODEL=Llama-3.1-8B-Instruct.nemo\n",
        "NUM_GPUS=2\n",
        "MAX_STEPS=100\n",
        "MBS=1\n",
        "GBS=2\n",
        "TP=2\n",
        "PP=1\n",
        "CP=1\n",
        "LR=1e-4\n",
        "DATA_SPLITS=\"9990,8,2\"\n",
        "DATA_PREFIX=\"[1.0,data/custom_dataset/preprocessed/wikinews_text_document]\"\n",
        "TOKENIZER_DIR=tokenizer-llama3  # 本地 Tokenizer 路徑\n",
        "\n",
        "# 禁用 CUDA 多連接\n",
        "export CUDA_DEVICE_MAX_CONNECTIONS=1\n",
        "\n",
        "# 執行 NeMo 訓練程式\n",
        "python /opt/NeMo/examples/nlp/language_modeling/megatron_gpt_pretraining.py \\\n",
        "--config-path=/opt/NeMo-Framework-Launcher/launcher_scripts/conf/training/llama --config-name=llama3_1_8b \\\n",
        "+base_results_dir=results \\\n",
        "trainer.num_nodes=1 \\\n",
        "trainer.devices=$NUM_GPUS \\\n",
        "trainer.max_steps=$MAX_STEPS \\\n",
        "trainer.limit_val_batches=1 \\\n",
        "trainer.val_check_interval=50 \\\n",
        "exp_manager.explicit_log_dir=/workspace/results/$MODEL_NAME/pretrain \\\n",
        "exp_manager.wandb_logger_kwargs.name=$MODEL_NAME \\\n",
        "exp_manager.resume_if_exists=True \\\n",
        "exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True \\\n",
        "exp_manager.checkpoint_callback_params.model_parallel_size=$(($TP*$PP)) \\\n",
        "model.micro_batch_size=$MBS \\\n",
        "model.global_batch_size=$GBS \\\n",
        "model.tensor_model_parallel_size=$TP \\\n",
        "model.pipeline_model_parallel_size=$PP \\\n",
        "model.context_parallel_size=$CP \\\n",
        "model.init_method_std=0.02 \\\n",
        "model.optim.lr=$LR \\\n",
        "model.data.splits_string=${DATA_SPLITS} \\\n",
        "model.data.data_prefix=${DATA_PREFIX} \\\n",
        "model.data.num_workers=0 \\\n",
        "+model.restore_from_path=$MODEL \\\n",
        "+model.rotary_base=500000.0 \\\n",
        "+model.seq_len_interpolation_factor=8 \\\n",
        "model.tokenizer.library=megatron  \\\n",
        "model.tokenizer.type=$TOKENIZER_DIR \\\n",
        "model.tokenizer.vocab_file=$TOKENIZER_DIR/vocab.json \\\n",
        "model.tokenizer.merge_file=$TOKENIZER_DIR/merges.txt \\\n",
        "model.tokenizer.model_file=$TOKENIZER_DIR/tokenizer.model \\\n",
        "+model.tokenizer.use_fast=false"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45abf191-0464-4847-a6b3-a436dcf701a7",
      "metadata": {
        "id": "45abf191-0464-4847-a6b3-a436dcf701a7"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "936fc9de-ab32-4341-856c-29b774a36361",
      "metadata": {
        "id": "936fc9de-ab32-4341-856c-29b774a36361"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "60bdaf30",
      "metadata": {
        "id": "60bdaf30"
      },
      "source": [
        "## 2. Instruction Tuning <a name='s2'></a>\n",
        "\n",
        "We will be using the [erhwenkuo/alpaca-data-gpt4-chinese-zhtw](https://huggingface.co/datasets/erhwenkuo/alpaca-data-gpt4-chinese-zhtw) is a dataset that contains Chinese (zh-tw) Instruction-Following generated by GPT-4 using Alpaca prompts for fine-tuning LLMs.\n",
        "\n",
        "The dataset was originaly shared in [this repository](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM). This dataset is a translation from English to Chinese.\n",
        "\n",
        "### 2.1 Download dataset: erhwenkuo/alpaca-data-gpt4-chinese-zhtw <a name='s2.1'></a>\n",
        "Let's download dataset and save it as json first."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46e8df35",
      "metadata": {
        "tags": [],
        "id": "46e8df35",
        "outputId": "3151270e-03da-4940-f24a-c87fd10666d4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Generating train split: 100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 52049/52049 [00:00<00:00, 515662.50 examples/s]\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset('erhwenkuo/alpaca-data-gpt4-chinese-zhtw')['train']\n",
        "output_path = 'data/alpaca/gpt4-chinese-zhtw.jsonl'\n",
        "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
        "\n",
        "with open(output_path, 'w') as f:\n",
        "    for human_instruction, human_input, assistant_output in zip(dataset['instruction'], dataset['input'], dataset['output']):\n",
        "        f.write(json.dumps({'input': '\\n'.join([human_instruction.strip(),human_input.strip()]).strip(), 'output': assistant_output.strip()}, ensure_ascii=False)+ '\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74f42d91",
      "metadata": {
        "tags": [],
        "id": "74f42d91",
        "outputId": "f7467b01-1d03-44fe-a7d3-a286e376e918"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"input\": \"給出三個保持健康的小貼士。\", \"output\": \"1. 飲食要均衡且富有營養：確保你的餐食包含各種水果、蔬菜、瘦肉、全穀物和健康脂肪。這有助於為身體提供必要的營養，使其發揮最佳功能，並有助於預防慢性疾病。2. 經常參加體育鍛煉：鍛鍊對於保持強壯的骨骼、肌肉和心血管健康至關重要。每週至少要進行150分鐘的中等有氧運動或75分鐘的劇烈運動。3. 獲得足夠的睡眠：獲得足夠的高質量睡眠對身體和心理健康至關重要。它有助於調節情緒，提高認知功能，並支援健康的生長和免疫功能。每晚睡眠目標為7-9小時。\"}\n"
          ]
        }
      ],
      "source": [
        "!head -n 1 data/alpaca/gpt4-chinese-zhtw.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ba864f07",
      "metadata": {
        "id": "ba864f07"
      },
      "source": [
        "### 2.2 Split the data into train, validation and test. <a name='s2.2'></a>\n",
        "\n",
        "Generate the train, test and validation splits- you may use your own script to do this or create a new script and use the following sample split_train_val.py by copying it over in the alpaca directory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a058d68a",
      "metadata": {
        "tags": [],
        "id": "a058d68a"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "input_file = \"data/alpaca/gpt4-chinese-zhtw.jsonl\"\n",
        "training_output_file = \"data/alpaca/training.jsonl\"\n",
        "validation_output_file = \"data/alpaca/validation.jsonl\"\n",
        "test_output_file = \"data/alpaca/test.jsonl\"\n",
        "\n",
        "# Specify the proportion of data for training and validation\n",
        "train_proportion = 0.98\n",
        "validation_proportion = 0.01\n",
        "test_proportion = 0.01\n",
        "\n",
        "# Read the JSONL file and shuffle the JSON objects\n",
        "with open(input_file, \"r\") as f:\n",
        "    lines = f.readlines()\n",
        "    random.shuffle(lines)\n",
        "\n",
        "# Calculate split indices\n",
        "total_lines = len(lines)\n",
        "train_index = int(total_lines * train_proportion)\n",
        "val_index = int(total_lines * validation_proportion)\n",
        "\n",
        "# Distribute JSON objects into training and validation sets\n",
        "train_data = lines[:train_index]\n",
        "validation_data = lines[train_index:train_index+val_index]\n",
        "test_data = lines[train_index+val_index:]\n",
        "\n",
        "# Write JSON objects to training file\n",
        "with open(training_output_file, \"w\") as f:\n",
        "    for line in train_data:\n",
        "        f.write(line.strip() + \"\\n\")\n",
        "\n",
        "# Write JSON objects to validation file\n",
        "with open(validation_output_file, \"w\") as f:\n",
        "    for line in validation_data:\n",
        "        f.write(line.strip() + \"\\n\")\n",
        "\n",
        "# Write JSON objects to training file\n",
        "with open(test_output_file, \"w\") as f:\n",
        "    for line in test_data:\n",
        "        f.write(line.strip() + \"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31cafbce",
      "metadata": {
        "tags": [],
        "id": "31cafbce",
        "outputId": "8751f7bd-23de-4f61-d402-9ab211966043"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\"input\": \"生成一句關於從失敗中學習的重要性的句子。\", \"output\": \"“接受和從失敗中學習對於成長和成功至關重要，因為它教給我們寶貴的經驗教訓，使我們能夠培養韌性，並提供改進的機會。”\"}\n"
          ]
        }
      ],
      "source": [
        "# What the dataset looks like after spliting\n",
        "!head -1 data/alpaca/training.jsonl"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0fb1542a",
      "metadata": {
        "id": "0fb1542a"
      },
      "source": [
        "### 2.3 Full parameter fine-tuning  <a name='s2.3'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "343e4944-4ad7-4011-b74b-5e52972cac1d",
      "metadata": {
        "id": "343e4944-4ad7-4011-b74b-5e52972cac1d"
      },
      "source": [
        "# 8張H100訓練"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "290290c0-98e0-4e68-9e11-7ba796c560b7",
      "metadata": {
        "tags": [],
        "id": "290290c0-98e0-4e68-9e11-7ba796c560b7",
        "outputId": "3ec01bea-cc3a-4a53-e409-0d0866ff0344"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 11:23:33 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:23:33 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:23:33 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:23:33 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:23:34 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 11:23:34 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 11:23:35 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:23:36 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 11:23:36 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:23:36 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 11:23:36 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "      ret = run_job(\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 11:23:36 megatron_gpt_finetuning:56] \n",
            "    \n",
            "    ************** Experiment configuration ***********\n",
            "[NeMo I 2024-12-12 11:23:36 megatron_gpt_finetuning:57] \n",
            "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
            "    trainer:\n",
            "      devices: 8\n",
            "      accelerator: gpu\n",
            "      num_nodes: 1\n",
            "      precision: 16\n",
            "      logger: false\n",
            "      enable_checkpointing: false\n",
            "      use_distributed_sampler: false\n",
            "      max_epochs: null\n",
            "      max_steps: 100\n",
            "      log_every_n_steps: 10\n",
            "      val_check_interval: 1.0\n",
            "      gradient_clip_val: 1.0\n",
            "    exp_manager:\n",
            "      explicit_log_dir: /workspace/results/Llama-3.1-8B/SFT\n",
            "      exp_dir: null\n",
            "      name: ${name}\n",
            "      create_wandb_logger: false\n",
            "      wandb_logger_kwargs:\n",
            "        project: null\n",
            "        name: null\n",
            "      resume_if_exists: true\n",
            "      resume_ignore_no_checkpoint: true\n",
            "      create_checkpoint_callback: true\n",
            "      checkpoint_callback_params:\n",
            "        monitor: validation_${model.data.validation_ds.metric.name}\n",
            "        save_top_k: 1\n",
            "        mode: min\n",
            "        save_nemo_on_train_end: true\n",
            "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
            "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
            "        always_save_nemo: false\n",
            "        save_best_model: true\n",
            "      create_early_stopping_callback: true\n",
            "      early_stopping_callback_params:\n",
            "        monitor: val_loss\n",
            "        mode: min\n",
            "        min_delta: 0.001\n",
            "        patience: 10\n",
            "        verbose: true\n",
            "        strict: false\n",
            "    model:\n",
            "      seed: 1234\n",
            "      tensor_model_parallel_size: 8\n",
            "      pipeline_model_parallel_size: 1\n",
            "      global_batch_size: 32\n",
            "      micro_batch_size: 2\n",
            "      restore_from_path: results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama.nemo\n",
            "      resume_from_checkpoint: null\n",
            "      save_nemo_on_validation_end: false\n",
            "      sync_batch_comm: false\n",
            "      megatron_amp_O2: false\n",
            "      sequence_parallel: false\n",
            "      activations_checkpoint_granularity: null\n",
            "      activations_checkpoint_method: null\n",
            "      activations_checkpoint_num_layers: null\n",
            "      activations_checkpoint_layers_per_pipeline: null\n",
            "      answer_only_loss: true\n",
            "      gradient_as_bucket_view: false\n",
            "      hidden_dropout: 0.0\n",
            "      attention_dropout: 0.0\n",
            "      ffn_dropout: 0.0\n",
            "      fsdp: false\n",
            "      fsdp_sharding_strategy: full\n",
            "      fsdp_grad_reduce_dtype: fp32\n",
            "      fsdp_sharded_checkpoint: false\n",
            "      fsdp_use_orig_params: false\n",
            "      peft:\n",
            "        peft_scheme: null\n",
            "        restore_from_path: null\n",
            "        adapter_tuning:\n",
            "          type: parallel_adapter\n",
            "          adapter_dim: 32\n",
            "          adapter_dropout: 0.0\n",
            "          norm_position: pre\n",
            "          column_init_method: xavier\n",
            "          row_init_method: zero\n",
            "          norm_type: mixedfusedlayernorm\n",
            "          layer_selection: null\n",
            "          weight_tying: false\n",
            "          position_embedding_strategy: null\n",
            "        lora_tuning:\n",
            "          variant: nemo\n",
            "          target_modules:\n",
            "          - attention_qkv\n",
            "          adapter_dim: 32\n",
            "          alpha: ${model.peft.lora_tuning.adapter_dim}\n",
            "          adapter_dropout: 0.0\n",
            "          column_init_method: xavier\n",
            "          row_init_method: zero\n",
            "          layer_selection: null\n",
            "          weight_tying: false\n",
            "          position_embedding_strategy: null\n",
            "        p_tuning:\n",
            "          virtual_tokens: 10\n",
            "          bottleneck_dim: 1024\n",
            "          embedding_dim: 1024\n",
            "          init_std: 0.023\n",
            "        ia3_tuning:\n",
            "          layer_selection: null\n",
            "        selective_tuning:\n",
            "          tunable_base_param_names:\n",
            "          - self_attention\n",
            "          - word_embeddings\n",
            "      data:\n",
            "        train_ds:\n",
            "          file_names:\n",
            "          - data/alpaca/training.jsonl\n",
            "          global_batch_size: ${model.global_batch_size}\n",
            "          micro_batch_size: ${model.micro_batch_size}\n",
            "          shuffle: true\n",
            "          num_workers: 8\n",
            "          memmap_workers: 2\n",
            "          pin_memory: true\n",
            "          max_seq_length: 8192\n",
            "          min_seq_length: 1\n",
            "          drop_last: true\n",
            "          concat_sampling_probabilities:\n",
            "          - 1.0\n",
            "          label_key: output\n",
            "          add_eos: true\n",
            "          add_sep: false\n",
            "          add_bos: false\n",
            "          truncation_field: input\n",
            "          index_mapping_dir: null\n",
            "          prompt_template: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\\nYou\n",
            "            are a knowledgeable assistant trained to provide accurate and helpful information.\n",
            "            Please respond to the user's queries promptly and politely.<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\\n{input}<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n{output}\n",
            "          truncation_method: right\n",
            "          global_sample_mapping: false\n",
            "        validation_ds:\n",
            "          file_names:\n",
            "          - data/alpaca/validation.jsonl\n",
            "          names: null\n",
            "          global_batch_size: ${model.global_batch_size}\n",
            "          micro_batch_size: ${model.micro_batch_size}\n",
            "          shuffle: false\n",
            "          num_workers: 8\n",
            "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
            "          pin_memory: true\n",
            "          max_seq_length: 8192\n",
            "          min_seq_length: 1\n",
            "          drop_last: false\n",
            "          label_key: ${model.data.train_ds.label_key}\n",
            "          add_eos: ${model.data.train_ds.add_eos}\n",
            "          add_sep: ${model.data.train_ds.add_sep}\n",
            "          add_bos: ${model.data.train_ds.add_bos}\n",
            "          write_predictions_to_file: false\n",
            "          output_file_path_prefix: null\n",
            "          truncation_field: ${model.data.train_ds.truncation_field}\n",
            "          index_mapping_dir: null\n",
            "          prompt_template: ${model.data.train_ds.prompt_template}\n",
            "          tokens_to_generate: 32\n",
            "          truncation_method: right\n",
            "          global_sample_mapping: false\n",
            "          metric:\n",
            "            name: loss\n",
            "            average: null\n",
            "            num_classes: null\n",
            "        test_ds:\n",
            "          file_names:\n",
            "          - data/alpaca/test.jsonl\n",
            "          names: null\n",
            "          global_batch_size: ${model.global_batch_size}\n",
            "          micro_batch_size: ${model.micro_batch_size}\n",
            "          shuffle: false\n",
            "          num_workers: 8\n",
            "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
            "          pin_memory: true\n",
            "          max_seq_length: 8192\n",
            "          min_seq_length: 1\n",
            "          drop_last: false\n",
            "          label_key: ${model.data.train_ds.label_key}\n",
            "          add_eos: ${model.data.train_ds.add_eos}\n",
            "          add_sep: ${model.data.train_ds.add_sep}\n",
            "          add_bos: ${model.data.train_ds.add_bos}\n",
            "          write_predictions_to_file: false\n",
            "          output_file_path_prefix: null\n",
            "          truncation_field: ${model.data.train_ds.truncation_field}\n",
            "          index_mapping_dir: null\n",
            "          prompt_template: ${model.data.train_ds.prompt_template}\n",
            "          tokens_to_generate: 32\n",
            "          truncation_method: right\n",
            "          global_sample_mapping: false\n",
            "          metric:\n",
            "            name: loss\n",
            "            average: null\n",
            "            num_classes: null\n",
            "      optim:\n",
            "        name: fused_adam\n",
            "        lr: 0.0002\n",
            "        weight_decay: 0.01\n",
            "        betas:\n",
            "        - 0.9\n",
            "        - 0.98\n",
            "        sched:\n",
            "          name: CosineAnnealing\n",
            "          warmup_steps: 50\n",
            "          min_lr: 0.0\n",
            "          constant_steps: 0\n",
            "          monitor: val_loss\n",
            "          reduce_on_plateau: false\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 11:23:36 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py:1451: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "      super().__init__(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:23:36 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
            "    \n",
            "[NeMo W 2024-12-12 11:23:36 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py:1395: DeprecationWarning: torch.set_autocast_gpu_dtype(dtype) is deprecated. Please use torch.set_autocast_dtype('cuda', dtype) instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:678.)\n",
            "      torch.set_autocast_gpu_dtype(dtype)\n",
            "    \n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 11:23:37 exp_manager:400] ExpManager schema\n",
            "[NeMo I 2024-12-12 11:23:37 exp_manager:401] {'explicit_log_dir': None, 'exp_dir': None, 'name': None, 'version': None, 'use_datetime_version': True, 'resume_if_exists': False, 'resume_past_end': False, 'resume_ignore_no_checkpoint': False, 'resume_from_checkpoint': None, 'create_tensorboard_logger': True, 'summary_writer_kwargs': None, 'create_wandb_logger': False, 'wandb_logger_kwargs': None, 'create_mlflow_logger': False, 'mlflow_logger_kwargs': {'experiment_name': None, 'tracking_uri': None, 'tags': None, 'save_dir': './mlruns', 'prefix': '', 'artifact_location': None, 'run_id': None, 'log_model': False}, 'create_dllogger_logger': False, 'dllogger_logger_kwargs': {'verbose': False, 'stdout': False, 'json_file': './dllogger.json'}, 'create_clearml_logger': False, 'clearml_logger_kwargs': {'project': None, 'task': None, 'connect_pytorch': False, 'model_name': None, 'tags': None, 'log_model': False, 'log_cfg': False, 'log_metrics': False}, 'create_neptune_logger': False, 'neptune_logger_kwargs': None, 'create_checkpoint_callback': True, 'checkpoint_callback_params': {'filepath': None, 'dirpath': None, 'filename': None, 'monitor': 'val_loss', 'verbose': True, 'save_last': True, 'save_top_k': 3, 'save_weights_only': False, 'mode': 'min', 'auto_insert_metric_name': True, 'every_n_epochs': 1, 'every_n_train_steps': None, 'train_time_interval': None, 'prefix': None, 'postfix': '.nemo', 'save_best_model': False, 'always_save_nemo': False, 'save_nemo_on_train_end': True, 'model_parallel_size': None, 'save_on_train_epoch_end': False, 'async_save': False}, 'create_early_stopping_callback': False, 'early_stopping_callback_params': {'monitor': 'val_loss', 'mode': 'min', 'min_delta': 0.001, 'patience': 10, 'verbose': True, 'strict': True, 'check_finite': True, 'stopping_threshold': None, 'divergence_threshold': None, 'check_on_train_epoch_end': None, 'log_rank_zero_only': False}, 'create_preemption_callback': True, 'files_to_copy': None, 'log_step_timing': True, 'step_timing_kwargs': {'reduction': 'mean', 'sync_cuda': False, 'buffer_size': 1}, 'log_local_rank_0_only': False, 'log_global_rank_0_only': False, 'disable_validation_on_resume': True, 'ema': {'enable': False, 'decay': 0.999, 'cpu_offload': False, 'validate_original_weights': False, 'every_n_steps': 1}, 'max_time_per_run': None, 'seconds_to_sleep': 5.0, 'create_straggler_detection_callback': False, 'straggler_detection_params': {'report_time_interval': 300.0, 'calc_relative_gpu_perf': True, 'calc_individual_gpu_perf': True, 'num_gpu_perf_scores_to_log': 5, 'gpu_relative_perf_threshold': 0.7, 'gpu_individual_perf_threshold': 0.7, 'stop_if_detected': False}, 'create_fault_tolerance_callback': False, 'fault_tolerance': {'workload_check_interval': 5.0, 'initial_rank_heartbeat_timeout': 3600.0, 'rank_heartbeat_timeout': 2700.0, 'calculate_timeouts': True, 'safety_factor': 5.0, 'rank_termination_signal': <Signals.SIGKILL: 9>, 'log_level': 'INFO', 'max_rank_restarts': 0, 'max_subsequent_job_failures': 0, 'additional_ft_launcher_args': '', 'simulated_fault': None}, 'log_tflops_per_sec_per_gpu': True}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 11:23:37 exp_manager:862] Exp_manager is logging to /workspace/results/Llama-3.1-8B/SFT, but it already exists.\n",
            "[NeMo W 2024-12-12 11:23:37 exp_manager:784] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/workspace/results/Llama-3.1-8B/SFT/checkpoints. Training from scratch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 11:23:37 exp_manager:459] Experiments will be logged at /workspace/results/Llama-3.1-8B/SFT\n",
            "[NeMo I 2024-12-12 11:23:37 exp_manager:1010] TensorboardLogger has been set up\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 11:23:37 exp_manager:1139] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 100. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 11:23:37 exp_manager:593] TFLOPs per sec per GPU will be calculated, conditioned on supported models. Defaults to -1 upon failure.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 11:23:48 megatron_init:314] Rank 0 has data parallel group : [0]\n",
            "[NeMo I 2024-12-12 11:23:48 megatron_init:320] Rank 0 has combined group of data parallel and context parallel : [0]\n",
            "[NeMo I 2024-12-12 11:23:48 megatron_init:325] All data parallel group ranks with context parallel combined: [[0], [1], [2], [3], [4], [5], [6], [7]]\n",
            "[NeMo I 2024-12-12 11:23:48 megatron_init:328] Ranks 0 has data parallel rank: 0\n",
            "[NeMo I 2024-12-12 11:23:48 megatron_init:336] Rank 0 has context parallel group: [0]\n",
            "[NeMo I 2024-12-12 11:23:48 megatron_init:339] All context parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]\n",
            "[NeMo I 2024-12-12 11:23:48 megatron_init:340] Ranks 0 has context parallel rank: 0\n",
            "[NeMo I 2024-12-12 11:23:48 megatron_init:347] Rank 0 has model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]\n",
            "[NeMo I 2024-12-12 11:23:48 megatron_init:348] All model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]\n",
            "[NeMo I 2024-12-12 11:23:48 megatron_init:357] Rank 0 has tensor model parallel group: [0, 1, 2, 3, 4, 5, 6, 7]\n",
            "[NeMo I 2024-12-12 11:23:48 megatron_init:361] All tensor model parallel group ranks: [[0, 1, 2, 3, 4, 5, 6, 7]]\n",
            "[NeMo I 2024-12-12 11:23:48 megatron_init:362] Rank 0 has tensor model parallel rank: 0\n",
            "[NeMo I 2024-12-12 11:23:48 megatron_init:382] Rank 0 has pipeline model parallel group: [0]\n",
            "[NeMo I 2024-12-12 11:23:48 megatron_init:394] Rank 0 has embedding group: [0]\n",
            "[NeMo I 2024-12-12 11:23:48 megatron_init:400] All pipeline model parallel group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]\n",
            "[NeMo I 2024-12-12 11:23:48 megatron_init:401] Rank 0 has pipeline model parallel rank 0\n",
            "[NeMo I 2024-12-12 11:23:48 megatron_init:402] All embedding group ranks: [[0], [1], [2], [3], [4], [5], [6], [7]]\n",
            "[NeMo I 2024-12-12 11:23:48 megatron_init:403] Rank 0 has embedding rank: 0\n",
            "[NeMo I 2024-12-12 11:23:48 num_microbatches_calculator:218] setting number of microbatches to constant 16\n",
            "[NeMo I 2024-12-12 11:23:48 megatron_base_model:985] Gradient accumulation fusion can only be used with megatron amp O2 mixed precision.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:48 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 11:23:48 tokenizer_utils:184] Getting HuggingFace AutoTokenizer with pretrained_model_name: meta-llama/Meta-Llama-3.1-8B\n",
            "[NeMo I 2024-12-12 11:23:49 megatron_base_model:601] Padded vocab_size: 129024, original vocab_size: 128256, dummy tokens: 768.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: first_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: last_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: tp_only_amax_red in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_router_pre_softmax in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: external_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: config_logger_dir in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 11:23:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:718: UserWarning: To guarantee overlapping TP and SP collectives with the backwardGEMMs, set environment variable CUDA_DEVICE_MAX_CONNECTIONS = 1\n",
            "      warnings.warn(\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "apply rope scaling ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "apply rope scaling ...\n",
            "apply rope scaling ...\n",
            "apply rope scaling ...\n",
            "apply rope scaling ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Initializing distributed: GLOBAL_RANK: 6, MEMBER: 7/8\n",
            "Initializing distributed: GLOBAL_RANK: 4, MEMBER: 5/8\n",
            "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/8\n",
            "Initializing distributed: GLOBAL_RANK: 5, MEMBER: 6/8\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "apply rope scaling ...\n",
            "apply rope scaling ...\n",
            "apply rope scaling ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Initializing distributed: GLOBAL_RANK: 7, MEMBER: 8/8\n",
            "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/8\n",
            "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/8\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 8 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "[NeMo W 2024-12-12 11:24:50 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:755: FutureWarning: `load_state_dict` is deprecated and will be removed in future versions. Please use `load` instead.\n",
            "      checkpoint.load_state_dict(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:24:50 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/distributed/checkpoint/planner_helpers.py:311: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n",
            "      device = getattr(value, \"device\", None)\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 11:24:56 nlp_overrides:1358] Model MegatronGPTSFTModel was successfully restored from /workspace/results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama.nemo.\n",
            "[NeMo I 2024-12-12 11:24:56 megatron_gpt_finetuning:75] Running full finetuning since no peft scheme is given.\n",
            "      | Name  | Type     | Params | Mode \n",
            "    -------------------------------------------\n",
            "    0 | model | GPTModel | 1.0 B  | train\n",
            "    -------------------------------------------\n",
            "    1.0 B     Trainable params\n",
            "    0         Non-trainable params\n",
            "    1.0 B     Total params\n",
            "    4,019.208 Total estimated model params size (MB)\n",
            "    649       Modules in train mode\n",
            "    0         Modules in eval mode\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 11:24:56 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:161: You have overridden `MegatronGPTSFTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.\n",
            "    \n",
            "[NeMo W 2024-12-12 11:24:56 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:143: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.\n",
            "    \n",
            "[NeMo W 2024-12-12 11:24:56 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      DESCRIPTOR = _descriptor.FileDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:24:56 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _descriptor.FieldDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:24:56 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:24:56 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      DESCRIPTOR = _descriptor.FileDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:24:56 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _descriptor.EnumValueDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:24:56 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _DATATYPE = _descriptor.EnumDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:24:56 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      DESCRIPTOR = _descriptor.FileDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:24:56 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _descriptor.FieldDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:24:56 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:24:56 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      DESCRIPTOR = _descriptor.FileDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:24:56 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _descriptor.FieldDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:24:56 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _TENSORPROTO = _descriptor.Descriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:24:56 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      DESCRIPTOR = _descriptor.FileDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:24:56 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:35: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _descriptor.EnumValueDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:24:56 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:29: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _DATACLASS = _descriptor.EnumDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:24:56 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:74: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _descriptor.FieldDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:24:56 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:67: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _SUMMARYDESCRIPTION = _descriptor.Descriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:24:56 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:326: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "      np.bool8: (False, True),\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 11:24:56 megatron_gpt_sft_model:836] Building GPT SFT validation datasets.\n",
            "[NeMo I 2024-12-12 11:24:56 text_memmap_dataset:116] Building data files\n",
            "[NeMo I 2024-12-12 11:24:56 text_memmap_dataset:527] Processing 1 data files using 2 workers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 11:24:56 text_memmap_dataset:542] Time building 0 / 1 mem-mapped files: 0:00:00.079305\n",
            "[NeMo I 2024-12-12 11:24:57 text_memmap_dataset:527] Processing 1 data files using 2 workers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 11:24:57 text_memmap_dataset:542] Time building 0 / 1 mem-mapped files: 0:00:00.077116\n",
            "[NeMo I 2024-12-12 11:24:57 text_memmap_dataset:158] Loading data files\n",
            "[NeMo I 2024-12-12 11:24:57 text_memmap_dataset:249] Loading data/alpaca/validation.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 11:24:57 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/text_memmap_dataset.py:263: ResourceWarning: unclosed file <_io.BufferedReader name='data/alpaca/validation.jsonl.idx.info'>\n",
            "      idx_info_dict = pickle.load(open(idx_fn + \".info\", \"rb\"))\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 11:24:57 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.000836\n",
            "[NeMo I 2024-12-12 11:24:57 text_memmap_dataset:165] Computing global indices\n",
            "[NeMo I 2024-12-12 11:24:57 megatron_gpt_sft_model:840] Length of val dataset: 520\n",
            "[NeMo I 2024-12-12 11:24:57 megatron_gpt_sft_model:828] Building GPT SFT test datasets.\n",
            "[NeMo I 2024-12-12 11:24:57 text_memmap_dataset:116] Building data files\n",
            "[NeMo I 2024-12-12 11:24:57 text_memmap_dataset:527] Processing 1 data files using 2 workers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 11:24:57 text_memmap_dataset:542] Time building 0 / 1 mem-mapped files: 0:00:00.048851\n",
            "[NeMo I 2024-12-12 11:24:57 text_memmap_dataset:527] Processing 1 data files using 2 workers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 11:24:57 text_memmap_dataset:542] Time building 0 / 1 mem-mapped files: 0:00:00.045071\n",
            "[NeMo I 2024-12-12 11:24:57 text_memmap_dataset:158] Loading data files\n",
            "[NeMo I 2024-12-12 11:24:57 text_memmap_dataset:249] Loading data/alpaca/test.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 11:24:57 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/text_memmap_dataset.py:263: ResourceWarning: unclosed file <_io.BufferedReader name='data/alpaca/test.jsonl.idx.info'>\n",
            "      idx_info_dict = pickle.load(open(idx_fn + \".info\", \"rb\"))\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 11:24:57 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.000649\n",
            "[NeMo I 2024-12-12 11:24:57 text_memmap_dataset:165] Computing global indices\n",
            "[NeMo I 2024-12-12 11:24:57 megatron_gpt_sft_model:831] Length of test dataset: 521\n",
            "[NeMo I 2024-12-12 11:24:57 megatron_gpt_sft_model:847] Building GPT SFT traing datasets.\n",
            "[NeMo I 2024-12-12 11:24:57 text_memmap_dataset:116] Building data files\n",
            "[NeMo I 2024-12-12 11:24:57 text_memmap_dataset:527] Processing 1 data files using 2 workers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 11:24:57 text_memmap_dataset:542] Time building 0 / 1 mem-mapped files: 0:00:00.055687\n",
            "[NeMo I 2024-12-12 11:24:57 text_memmap_dataset:527] Processing 1 data files using 2 workers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 11:24:57 text_memmap_dataset:542] Time building 0 / 1 mem-mapped files: 0:00:00.080003\n",
            "[NeMo I 2024-12-12 11:24:57 text_memmap_dataset:158] Loading data files\n",
            "[NeMo I 2024-12-12 11:24:57 text_memmap_dataset:249] Loading data/alpaca/training.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 11:24:57 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/text_memmap_dataset.py:263: ResourceWarning: unclosed file <_io.BufferedReader name='data/alpaca/training.jsonl.idx.info'>\n",
            "      idx_info_dict = pickle.load(open(idx_fn + \".info\", \"rb\"))\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 11:24:57 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.000667\n",
            "[NeMo I 2024-12-12 11:24:57 text_memmap_dataset:165] Computing global indices\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 11:24:57 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron/dataset_utils.py:1332: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)\n",
            "      counts = torch.cuda.LongTensor([1])\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "make: Entering directory '/opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron'\n",
            "make: Nothing to be done for 'default'.\n",
            "make: Leaving directory '/opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron'\n",
            "> building indices for blendable datasets ...\n",
            " > sample ratios:\n",
            "   dataset 0, input: 1, achieved: 1\n",
            "[NeMo I 2024-12-12 11:24:57 blendable_dataset:67] > elapsed time for building blendable dataset indices: 0.03 (sec)\n",
            "[NeMo I 2024-12-12 11:24:57 megatron_gpt_sft_model:849] Length of train dataset: 3216\n",
            "[NeMo I 2024-12-12 11:24:57 megatron_gpt_sft_model:854] Building dataloader with consumed samples: 0\n",
            "[NeMo I 2024-12-12 11:24:57 megatron_gpt_sft_model:854] Building dataloader with consumed samples: 0\n",
            "[NeMo I 2024-12-12 11:24:57 megatron_gpt_sft_model:854] Building dataloader with consumed samples: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
            "LOCAL_RANK: 4 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
            "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
            "LOCAL_RANK: 6 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
            "LOCAL_RANK: 7 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
            "LOCAL_RANK: 5 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
            "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 11:24:57 nlp_overrides:274] Configuring DDP for model parallelism.\n",
            "[NeMo I 2024-12-12 11:24:57 modelPT:787] Optimizer config = FusedAdam (\n",
            "    Parameter Group 0\n",
            "        betas: [0.9, 0.98]\n",
            "        bias_correction: True\n",
            "        eps: 1e-08\n",
            "        is_expert: False\n",
            "        lr: 0.0002\n",
            "        weight_decay: 0.01\n",
            "    )\n",
            "[NeMo I 2024-12-12 11:24:57 lr_scheduler:948] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f2f70f61de0>\" \n",
            "    will be used during training (effective maximum steps = 100) - \n",
            "    Parameters : \n",
            "    (warmup_steps: 50\n",
            "    min_lr: 0.0\n",
            "    constant_steps: 0\n",
            "    max_steps: 100\n",
            "    )\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  | Name  | Type     | Params | Mode \n",
            "-------------------------------------------\n",
            "0 | model | GPTModel | 1.0 B  | train\n",
            "-------------------------------------------\n",
            "1.0 B     Trainable params\n",
            "0         Non-trainable params\n",
            "1.0 B     Total params\n",
            "4,019.208 Total estimated model params size (MB)\n",
            "649       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "[NeMo W 2024-12-12 11:24:57 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:03 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:03 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:03 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:03 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:04 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:04 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:05 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:06 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:06 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:06 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:12 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:12 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:12 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:12 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:13 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:13 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:13 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:14 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:14 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:14 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:20 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:20 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:20 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:20 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:21 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:21 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:21 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:23 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:23 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:28 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:28 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:28 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:28 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:29 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:29 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:29 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:30 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:31 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:31 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:36 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:36 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:36 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:36 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:37 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:37 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:39 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:39 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:39 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:45 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:45 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:45 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:45 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:45 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:45 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:46 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:47 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:47 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:47 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:53 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:53 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:53 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:53 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:54 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:54 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:54 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:55 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:55 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:25:55 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:01 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:01 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:01 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:01 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:02 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:02 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:02 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:04 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:04 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sanity Checking: |          | 0/? [00:00<?, ?it/s][NeMo I 2024-12-12 11:26:05 num_microbatches_calculator:218] setting number of microbatches to constant 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 11:26:07 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:10 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:10 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:11 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:11 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:11 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:636: UserWarning: When using async grad allreduce it is recommended to set the environment variable CUDA_DEVICE_MAX_CONNECTIONS to 1 for maximum speedup\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:13 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:13 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:15 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:16 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:16 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:16 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:19 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:19 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
            "      warnings.warn(\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:14<00:00,  0.14it/s][NeMo I 2024-12-12 11:26:20 num_microbatches_calculator:218] setting number of microbatches to constant 16\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 11:26:20 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:20 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('validation_loss_dataloader0', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:20 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('validation_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:20 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:26 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:26 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:26 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:26 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:27 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:27 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:27 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:29 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:29 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:29 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:35 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:35 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:35 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:35 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:36 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:36 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:36 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:37 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:43 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:43 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:43 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:43 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:44 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:44 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:46 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:46 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:46 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:52 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:52 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:52 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:52 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:53 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:53 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:53 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:54 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:54 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:26:54 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:00 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:00 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:00 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:00 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:01 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:01 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:02 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:03 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:08 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:08 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:08 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:08 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:09 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:09 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:10 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:11 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:11 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:11 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:17 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:17 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:17 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:17 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:18 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:18 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:18 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:19 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:19 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:19 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:25 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:25 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:25 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:25 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:26 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:26 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:27 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:28 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:28 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:28 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:31 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:31 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:32 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:32 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:36 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:36 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:215: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:40 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:40 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:40 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:42 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:42 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:48 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:48 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:52 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:52 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:55 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:55 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:58 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 11:27:58 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
            "      warnings.warn(\n",
            "    \n",
            "[rank4]:W1212 11:28:01.346000 139707341711168 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
            "[rank4]:W1212 11:28:01.346000 139707341711168 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
            "[rank4]:W1212 11:28:01.346000 139707341711168 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 624, actual 656\n",
            "[rank4]:W1212 11:28:01.346000 139707341711168 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
            "[rank4]:W1212 11:28:01.346000 139707341711168 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
            "[rank6]:W1212 11:28:01.347000 140691748554560 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
            "[rank6]:W1212 11:28:01.347000 140691748554560 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
            "[rank6]:W1212 11:28:01.347000 140691748554560 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 624, actual 656\n",
            "[rank6]:W1212 11:28:01.347000 140691748554560 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
            "[rank6]:W1212 11:28:01.347000 140691748554560 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
            "[rank2]:W1212 11:28:01.346000 139798245107520 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
            "[rank2]:W1212 11:28:01.346000 139798245107520 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
            "[rank2]:W1212 11:28:01.346000 139798245107520 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 624, actual 656\n",
            "[rank2]:W1212 11:28:01.346000 139798245107520 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
            "[rank2]:W1212 11:28:01.346000 139798245107520 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
            "[rank7]:W1212 11:28:01.346000 139751646332736 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
            "[rank7]:W1212 11:28:01.346000 139751646332736 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
            "[rank7]:W1212 11:28:01.346000 139751646332736 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 624, actual 656\n",
            "[rank7]:W1212 11:28:01.346000 139751646332736 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
            "[rank7]:W1212 11:28:01.346000 139751646332736 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
            "[rank5]:W1212 11:28:01.346000 140428061738816 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
            "[rank5]:W1212 11:28:01.346000 140428061738816 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
            "[rank5]:W1212 11:28:01.346000 140428061738816 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 624, actual 656\n",
            "[rank5]:W1212 11:28:01.346000 140428061738816 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
            "[rank5]:W1212 11:28:01.346000 140428061738816 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
            "[rank3]:W1212 11:28:01.347000 140070200276800 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
            "[rank3]:W1212 11:28:01.347000 140070200276800 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
            "[rank3]:W1212 11:28:01.347000 140070200276800 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 624, actual 656\n",
            "[rank3]:W1212 11:28:01.347000 140070200276800 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
            "[rank3]:W1212 11:28:01.347000 140070200276800 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
            "[rank1]:W1212 11:28:01.347000 140154373449536 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
            "[rank1]:W1212 11:28:01.347000 140154373449536 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
            "[rank1]:W1212 11:28:01.347000 140154373449536 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 624, actual 656\n",
            "[rank1]:W1212 11:28:01.347000 140154373449536 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
            "[rank1]:W1212 11:28:01.347000 140154373449536 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
            "[rank0]:W1212 11:28:01.347000 139846555354944 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
            "[rank0]:W1212 11:28:01.347000 139846555354944 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
            "[rank0]:W1212 11:28:01.347000 139846555354944 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 624, actual 656\n",
            "[rank0]:W1212 11:28:01.347000 139846555354944 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
            "[rank0]:W1212 11:28:01.347000 139846555354944 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
            "[NeMo W 2024-12-12 11:28:02 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 11:28:02 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:28:05 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 11:28:05 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:31:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: : 100%|██████████| 100/100 [03:51<00:00, reduced_train_loss=2.900, global_step=99.00, consumed_samples=3200.0, train_step_timing in s=1.940]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-12-12 11:31:22 num_microbatches_calculator:218] setting number of microbatches to constant 16\n",
            "\n",
            "Validation:   0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/17 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   6%|▌         | 1/17 [00:01<00:16,  0.94it/s]\u001b[A\n",
            "Validation DataLoader 0:  12%|█▏        | 2/17 [00:02<00:15,  0.96it/s]\u001b[A\n",
            "Validation DataLoader 0:  18%|█▊        | 3/17 [00:03<00:16,  0.83it/s]\u001b[A\n",
            "Validation DataLoader 0:  24%|██▎       | 4/17 [00:05<00:16,  0.78it/s]\u001b[A\n",
            "Validation DataLoader 0:  29%|██▉       | 5/17 [00:06<00:15,  0.75it/s]\u001b[A\n",
            "Validation DataLoader 0:  35%|███▌      | 6/17 [00:07<00:14,  0.78it/s]\u001b[A\n",
            "Validation DataLoader 0:  41%|████      | 7/17 [00:09<00:13,  0.76it/s]\u001b[A\n",
            "Validation DataLoader 0:  47%|████▋     | 8/17 [00:10<00:11,  0.75it/s]\u001b[A\n",
            "Validation DataLoader 0:  53%|█████▎    | 9/17 [00:11<00:10,  0.77it/s]\u001b[A\n",
            "Validation DataLoader 0:  59%|█████▉    | 10/17 [00:13<00:09,  0.76it/s]\u001b[A\n",
            "Validation DataLoader 0:  65%|██████▍   | 11/17 [00:14<00:08,  0.75it/s]\u001b[A\n",
            "Validation DataLoader 0:  71%|███████   | 12/17 [00:15<00:06,  0.76it/s]\u001b[A\n",
            "Validation DataLoader 0:  76%|███████▋  | 13/17 [00:17<00:05,  0.76it/s]\u001b[A\n",
            "Validation DataLoader 0:  82%|████████▏ | 14/17 [00:18<00:04,  0.75it/s]\u001b[A\n",
            "Validation DataLoader 0:  88%|████████▊ | 15/17 [00:20<00:02,  0.74it/s]\u001b[A\n",
            "Validation DataLoader 0:  94%|█████████▍| 16/17 [00:21<00:01,  0.74it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 17/17 [00:23<00:00,  0.73it/s]\u001b[A[NeMo I 2024-12-12 11:31:45 num_microbatches_calculator:218] setting number of microbatches to constant 16\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 0, global step 100: 'validation_loss' reached 2.63934 (best 2.63934), saving model to '/workspace/results/Llama-3.1-8B/SFT/checkpoints/megatron_gpt_peft_None_tuning--validation_loss=2.639-step=100-consumed_samples=3200.0.ckpt' as top 1\n",
            "[NeMo W 2024-12-12 11:31:45 nlp_overrides:610] Distributed checkpoints requires DistributedCheckpointIO plugin to be used. Setting up a default now.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: : 100%|██████████| 100/100 [04:16<00:00, reduced_train_loss=2.900, global_step=99.00, consumed_samples=3200.0, train_step_timing in s=1.940, val_loss=2.640][NeMo I 2024-12-12 11:31:45 dist_ckpt_io:421] Using TorchDistSaveShardedStrategy(torch_dist, 1) dist-ckpt save strategy.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 11:31:56 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:31:56 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:31:56 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 11:31:56 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:31:57 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 11:31:57 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 11:31:58 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[NeMo W 2024-12-12 11:31:59 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 11:31:59 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 11:31:59 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "[NeMo E 2024-12-12 11:32:09 filesystem_async:220] Local process 1 encountered an error: [enforce fail at inline_container.cc:603] . unexpected pos 448 vs 342\n",
            "[NeMo W 2024-12-12 11:32:18 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpfp97udkp'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n",
            "[NeMo W 2024-12-12 11:32:18 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpz0o66_8q'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n",
            "[NeMo W 2024-12-12 11:32:18 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp3131hoyn'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n",
            "[NeMo W 2024-12-12 11:32:18 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp9fnzpoko'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n",
            "[NeMo W 2024-12-12 11:32:18 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpcmowiwyn'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n",
            "[NeMo W 2024-12-12 11:32:18 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp_dq10jzu'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n",
            "[NeMo W 2024-12-12 11:32:18 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp67d2d_m9'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n",
            "[NeMo W 2024-12-12 11:32:18 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmphhxp___x'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n",
            "Error executing job with overrides: ['trainer.devices=8', 'trainer.max_epochs=null', 'trainer.max_steps=100', 'trainer.val_check_interval=1.0', 'exp_manager.explicit_log_dir=/workspace/results/Llama-3.1-8B/SFT', 'exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True', 'model.tensor_model_parallel_size=8', 'model.pipeline_model_parallel_size=1', 'model.restore_from_path=results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama.nemo', 'model.global_batch_size=32', 'model.micro_batch_size=2', 'model.data.train_ds.file_names=[data/alpaca/training.jsonl]', 'model.data.validation_ds.file_names=[data/alpaca/validation.jsonl]', 'model.data.test_ds.file_names=[data/alpaca/test.jsonl]', 'model.data.train_ds.max_seq_length=8192', 'model.data.validation_ds.max_seq_length=8192', 'model.data.test_ds.max_seq_length=8192', 'model.data.train_ds.num_workers=8', 'model.data.validation_ds.num_workers=8', 'model.data.test_ds.num_workers=8', 'model.data.train_ds.concat_sampling_probabilities=[1.0]', 'model.data.train_ds.prompt_template=\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\\\nYou are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user\\'s queries promptly and politely.<|eot_id|>\\\\n<|start_header_id|>user<|end_header_id|>\\\\n{input}<|eot_id|>\\\\n<|start_header_id|>assistant<|end_header_id|>\\\\n{output}\"', 'model.optim.lr=2e-4', 'model.peft.peft_scheme=null']\n",
            "RuntimeError: [enforce fail at inline_container.cc:603] . unexpected pos 448 vs 342\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py\", line 77, in main\n",
            "    trainer.fit(model)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 538, in fit\n",
            "    call._call_and_handle_interrupt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 46, in _call_and_handle_interrupt\n",
            "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/launchers/subprocess_script.py\", line 105, in launch\n",
            "    return function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n",
            "    self._run(model, ckpt_path=ckpt_path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 981, in _run\n",
            "    results = self._run_stage()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1025, in _run_stage\n",
            "    self.fit_loop.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 205, in run\n",
            "    self.advance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 363, in advance\n",
            "    self.epoch_loop.run(self._data_fetcher)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 141, in run\n",
            "    self.on_advance_end(data_fetcher)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 295, in on_advance_end\n",
            "    self.val_loop.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py\", line 178, in _decorator\n",
            "    return loop_run(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 142, in run\n",
            "    return self.on_run_end()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 268, in on_run_end\n",
            "    self._on_evaluation_end()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 313, in _on_evaluation_end\n",
            "    call._call_callback_hooks(trainer, hook_name, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 218, in _call_callback_hooks\n",
            "    fn(trainer, trainer.lightning_module, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 335, in on_validation_end\n",
            "    self._save_last_checkpoint(trainer, monitor_candidates)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 696, in _save_last_checkpoint\n",
            "    self._save_checkpoint(trainer, filepath)\n",
            "  File \"/opt/NeMo/nemo/utils/callbacks/nemo_model_checkpoint.py\", line 470, in _save_checkpoint\n",
            "    trainer.save_checkpoint(filepath, self.save_weights_only, storage_options=storage_options)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1365, in save_checkpoint\n",
            "    self.strategy.save_checkpoint(checkpoint, filepath, storage_options=storage_options)\n",
            "  File \"/opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py\", line 398, in save_checkpoint\n",
            "    self.checkpoint_io.save_checkpoint(checkpoint, ckpt_to_dir(filepath), storage_options=storage_options)\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/opt/NeMo/nemo/utils/callbacks/dist_ckpt_io.py\", line 271, in save_checkpoint\n",
            "    return dist_checkpointing.save(\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/serialization.py\", line 382, in save\n",
            "    sharded_strategy.save(sharded_state_dict, checkpoint_dir)\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/base.py\", line 223, in save\n",
            "    async_calls.maybe_finalize_async_calls(blocking=True)\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/async_utils.py\", line 209, in maybe_finalize_async_calls\n",
            "    finalize_fn()\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py\", line 680, in finalize_fn\n",
            "    save_state_dict_async_finalize(*save_state_dict_ret)\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/state_dict_saver.py\", line 144, in save_state_dict_async_finalize\n",
            "    write_results = storage_writer.retrieve_write_results()\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/filesystem_async.py\", line 308, in retrieve_write_results\n",
            "    raise RuntimeError(f'Worker failure: {write_results_or_exc}') from write_results_or_exc\n",
            "RuntimeError: Worker failure: [enforce fail at inline_container.cc:603] . unexpected pos 448 vs 342\n",
            "\n",
            "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n",
            "Error executing job with overrides: ['trainer.devices=8', 'trainer.max_epochs=null', 'trainer.max_steps=100', 'trainer.val_check_interval=1.0', 'exp_manager.explicit_log_dir=/workspace/results/Llama-3.1-8B/SFT', 'exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True', 'model.tensor_model_parallel_size=8', 'model.pipeline_model_parallel_size=1', 'model.restore_from_path=results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama.nemo', 'model.global_batch_size=32', 'model.micro_batch_size=2', 'model.data.train_ds.file_names=[data/alpaca/training.jsonl]', 'model.data.validation_ds.file_names=[data/alpaca/validation.jsonl]', 'model.data.test_ds.file_names=[data/alpaca/test.jsonl]', 'model.data.train_ds.max_seq_length=8192', 'model.data.validation_ds.max_seq_length=8192', 'model.data.test_ds.max_seq_length=8192', 'model.data.train_ds.num_workers=8', 'model.data.validation_ds.num_workers=8', 'model.data.test_ds.num_workers=8', 'model.data.train_ds.concat_sampling_probabilities=[1.0]', 'model.data.train_ds.prompt_template=\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\\\nYou are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user\\'s queries promptly and politely.<|eot_id|>\\\\n<|start_header_id|>user<|end_header_id|>\\\\n{input}<|eot_id|>\\\\n<|start_header_id|>assistant<|end_header_id|>\\\\n{output}\"', 'model.optim.lr=2e-4', 'model.peft.peft_scheme=null']\n",
            "RuntimeError: [enforce fail at inline_container.cc:603] . unexpected pos 448 vs 342\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py\", line 77, in main\n",
            "    trainer.fit(model)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 538, in fit\n",
            "    call._call_and_handle_interrupt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 46, in _call_and_handle_interrupt\n",
            "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/launchers/subprocess_script.py\", line 105, in launch\n",
            "    return function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n",
            "    self._run(model, ckpt_path=ckpt_path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 981, in _run\n",
            "    results = self._run_stage()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1025, in _run_stage\n",
            "    self.fit_loop.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 205, in run\n",
            "    self.advance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 363, in advance\n",
            "    self.epoch_loop.run(self._data_fetcher)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 141, in run\n",
            "    self.on_advance_end(data_fetcher)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 295, in on_advance_end\n",
            "    self.val_loop.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py\", line 178, in _decorator\n",
            "    return loop_run(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 142, in run\n",
            "    return self.on_run_end()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 268, in on_run_end\n",
            "    self._on_evaluation_end()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 313, in _on_evaluation_end\n",
            "    call._call_callback_hooks(trainer, hook_name, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 218, in _call_callback_hooks\n",
            "    fn(trainer, trainer.lightning_module, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 335, in on_validation_end\n",
            "    self._save_last_checkpoint(trainer, monitor_candidates)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 696, in _save_last_checkpoint\n",
            "    self._save_checkpoint(trainer, filepath)\n",
            "  File \"/opt/NeMo/nemo/utils/callbacks/nemo_model_checkpoint.py\", line 470, in _save_checkpoint\n",
            "    trainer.save_checkpoint(filepath, self.save_weights_only, storage_options=storage_options)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1365, in save_checkpoint\n",
            "    self.strategy.save_checkpoint(checkpoint, filepath, storage_options=storage_options)\n",
            "  File \"/opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py\", line 398, in save_checkpoint\n",
            "    self.checkpoint_io.save_checkpoint(checkpoint, ckpt_to_dir(filepath), storage_options=storage_options)\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/opt/NeMo/nemo/utils/callbacks/dist_ckpt_io.py\", line 271, in save_checkpoint\n",
            "    return dist_checkpointing.save(\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/serialization.py\", line 382, in save\n",
            "    sharded_strategy.save(sharded_state_dict, checkpoint_dir)\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/base.py\", line 223, in save\n",
            "    async_calls.maybe_finalize_async_calls(blocking=True)\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/async_utils.py\", line 209, in maybe_finalize_async_calls\n",
            "    finalize_fn()\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py\", line 680, in finalize_fn\n",
            "    save_state_dict_async_finalize(*save_state_dict_ret)\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/state_dict_saver.py\", line 144, in save_state_dict_async_finalize\n",
            "    write_results = storage_writer.retrieve_write_results()\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/filesystem_async.py\", line 308, in retrieve_write_results\n",
            "    raise RuntimeError(f'Worker failure: {write_results_or_exc}') from write_results_or_exc\n",
            "RuntimeError: Worker failure: [enforce fail at inline_container.cc:603] . unexpected pos 448 vs 342\n",
            "\n",
            "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n",
            "Error executing job with overrides: ['trainer.devices=8', 'trainer.max_epochs=null', 'trainer.max_steps=100', 'trainer.val_check_interval=1.0', 'exp_manager.explicit_log_dir=/workspace/results/Llama-3.1-8B/SFT', 'exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True', 'model.tensor_model_parallel_size=8', 'model.pipeline_model_parallel_size=1', 'model.restore_from_path=results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama.nemo', 'model.global_batch_size=32', 'model.micro_batch_size=2', 'model.data.train_ds.file_names=[data/alpaca/training.jsonl]', 'model.data.validation_ds.file_names=[data/alpaca/validation.jsonl]', 'model.data.test_ds.file_names=[data/alpaca/test.jsonl]', 'model.data.train_ds.max_seq_length=8192', 'model.data.validation_ds.max_seq_length=8192', 'model.data.test_ds.max_seq_length=8192', 'model.data.train_ds.num_workers=8', 'model.data.validation_ds.num_workers=8', 'model.data.test_ds.num_workers=8', 'model.data.train_ds.concat_sampling_probabilities=[1.0]', 'model.data.train_ds.prompt_template=\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\\\nYou are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user\\'s queries promptly and politely.<|eot_id|>\\\\n<|start_header_id|>user<|end_header_id|>\\\\n{input}<|eot_id|>\\\\n<|start_header_id|>assistant<|end_header_id|>\\\\n{output}\"', 'model.optim.lr=2e-4', 'model.peft.peft_scheme=null']\n",
            "RuntimeError: [enforce fail at inline_container.cc:603] . unexpected pos 448 vs 342\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py\", line 77, in main\n",
            "    trainer.fit(model)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 538, in fit\n",
            "    call._call_and_handle_interrupt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 46, in _call_and_handle_interrupt\n",
            "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/launchers/subprocess_script.py\", line 105, in launch\n",
            "    return function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n",
            "    self._run(model, ckpt_path=ckpt_path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 981, in _run\n",
            "    results = self._run_stage()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1025, in _run_stage\n",
            "    self.fit_loop.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 205, in run\n",
            "    self.advance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 363, in advance\n",
            "    self.epoch_loop.run(self._data_fetcher)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 141, in run\n",
            "    self.on_advance_end(data_fetcher)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 295, in on_advance_end\n",
            "    self.val_loop.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py\", line 178, in _decorator\n",
            "    return loop_run(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 142, in run\n",
            "    return self.on_run_end()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 268, in on_run_end\n",
            "    self._on_evaluation_end()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 313, in _on_evaluation_end\n",
            "    call._call_callback_hooks(trainer, hook_name, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 218, in _call_callback_hooks\n",
            "    fn(trainer, trainer.lightning_module, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 335, in on_validation_end\n",
            "    self._save_last_checkpoint(trainer, monitor_candidates)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 696, in _save_last_checkpoint\n",
            "    self._save_checkpoint(trainer, filepath)\n",
            "  File \"/opt/NeMo/nemo/utils/callbacks/nemo_model_checkpoint.py\", line 470, in _save_checkpoint\n",
            "    trainer.save_checkpoint(filepath, self.save_weights_only, storage_options=storage_options)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1365, in save_checkpoint\n",
            "    self.strategy.save_checkpoint(checkpoint, filepath, storage_options=storage_options)\n",
            "  File \"/opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py\", line 398, in save_checkpoint\n",
            "    self.checkpoint_io.save_checkpoint(checkpoint, ckpt_to_dir(filepath), storage_options=storage_options)\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/opt/NeMo/nemo/utils/callbacks/dist_ckpt_io.py\", line 271, in save_checkpoint\n",
            "    return dist_checkpointing.save(\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/serialization.py\", line 382, in save\n",
            "    sharded_strategy.save(sharded_state_dict, checkpoint_dir)\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/base.py\", line 223, in save\n",
            "    async_calls.maybe_finalize_async_calls(blocking=True)\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/async_utils.py\", line 209, in maybe_finalize_async_calls\n",
            "    finalize_fn()\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py\", line 680, in finalize_fn\n",
            "    save_state_dict_async_finalize(*save_state_dict_ret)\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/state_dict_saver.py\", line 144, in save_state_dict_async_finalize\n",
            "    write_results = storage_writer.retrieve_write_results()\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/filesystem_async.py\", line 308, in retrieve_write_results\n",
            "    raise RuntimeError(f'Worker failure: {write_results_or_exc}') from write_results_or_exc\n",
            "RuntimeError: Worker failure: [enforce fail at inline_container.cc:603] . unexpected pos 448 vs 342\n",
            "\n",
            "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n",
            "Error executing job with overrides: ['trainer.devices=8', 'trainer.max_epochs=null', 'trainer.max_steps=100', 'trainer.val_check_interval=1.0', 'exp_manager.explicit_log_dir=/workspace/results/Llama-3.1-8B/SFT', 'exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True', 'model.tensor_model_parallel_size=8', 'model.pipeline_model_parallel_size=1', 'model.restore_from_path=results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama.nemo', 'model.global_batch_size=32', 'model.micro_batch_size=2', 'model.data.train_ds.file_names=[data/alpaca/training.jsonl]', 'model.data.validation_ds.file_names=[data/alpaca/validation.jsonl]', 'model.data.test_ds.file_names=[data/alpaca/test.jsonl]', 'model.data.train_ds.max_seq_length=8192', 'model.data.validation_ds.max_seq_length=8192', 'model.data.test_ds.max_seq_length=8192', 'model.data.train_ds.num_workers=8', 'model.data.validation_ds.num_workers=8', 'model.data.test_ds.num_workers=8', 'model.data.train_ds.concat_sampling_probabilities=[1.0]', 'model.data.train_ds.prompt_template=\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\\\nYou are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user\\'s queries promptly and politely.<|eot_id|>\\\\n<|start_header_id|>user<|end_header_id|>\\\\n{input}<|eot_id|>\\\\n<|start_header_id|>assistant<|end_header_id|>\\\\n{output}\"', 'model.optim.lr=2e-4', 'model.peft.peft_scheme=null']\n",
            "RuntimeError: [enforce fail at inline_container.cc:603] . unexpected pos 448 vs 342\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py\", line 77, in main\n",
            "    trainer.fit(model)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 538, in fit\n",
            "    call._call_and_handle_interrupt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 46, in _call_and_handle_interrupt\n",
            "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/launchers/subprocess_script.py\", line 105, in launch\n",
            "    return function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n",
            "    self._run(model, ckpt_path=ckpt_path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 981, in _run\n",
            "    results = self._run_stage()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1025, in _run_stage\n",
            "    self.fit_loop.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 205, in run\n",
            "    self.advance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 363, in advance\n",
            "    self.epoch_loop.run(self._data_fetcher)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 141, in run\n",
            "    self.on_advance_end(data_fetcher)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 295, in on_advance_end\n",
            "    self.val_loop.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py\", line 178, in _decorator\n",
            "    return loop_run(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 142, in run\n",
            "    return self.on_run_end()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 268, in on_run_end\n",
            "    self._on_evaluation_end()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 313, in _on_evaluation_end\n",
            "    call._call_callback_hooks(trainer, hook_name, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 218, in _call_callback_hooks\n",
            "    fn(trainer, trainer.lightning_module, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 335, in on_validation_end\n",
            "    self._save_last_checkpoint(trainer, monitor_candidates)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 696, in _save_last_checkpoint\n",
            "    self._save_checkpoint(trainer, filepath)\n",
            "  File \"/opt/NeMo/nemo/utils/callbacks/nemo_model_checkpoint.py\", line 470, in _save_checkpoint\n",
            "    trainer.save_checkpoint(filepath, self.save_weights_only, storage_options=storage_options)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1365, in save_checkpoint\n",
            "    self.strategy.save_checkpoint(checkpoint, filepath, storage_options=storage_options)\n",
            "  File \"/opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py\", line 398, in save_checkpoint\n",
            "    self.checkpoint_io.save_checkpoint(checkpoint, ckpt_to_dir(filepath), storage_options=storage_options)\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/opt/NeMo/nemo/utils/callbacks/dist_ckpt_io.py\", line 271, in save_checkpoint\n",
            "    return dist_checkpointing.save(\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/serialization.py\", line 382, in save\n",
            "    sharded_strategy.save(sharded_state_dict, checkpoint_dir)\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/base.py\", line 223, in save\n",
            "    async_calls.maybe_finalize_async_calls(blocking=True)\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/async_utils.py\", line 209, in maybe_finalize_async_calls\n",
            "    finalize_fn()\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py\", line 680, in finalize_fn\n",
            "    save_state_dict_async_finalize(*save_state_dict_ret)\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/state_dict_saver.py\", line 144, in save_state_dict_async_finalize\n",
            "    write_results = storage_writer.retrieve_write_results()\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/filesystem_async.py\", line 308, in retrieve_write_results\n",
            "    raise RuntimeError(f'Worker failure: {write_results_or_exc}') from write_results_or_exc\n",
            "RuntimeError: Worker failure: [enforce fail at inline_container.cc:603] . unexpected pos 448 vs 342\n",
            "\n",
            "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n",
            "Error executing job with overrides: ['trainer.devices=8', 'trainer.max_epochs=null', 'trainer.max_steps=100', 'trainer.val_check_interval=1.0', 'exp_manager.explicit_log_dir=/workspace/results/Llama-3.1-8B/SFT', 'exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True', 'model.tensor_model_parallel_size=8', 'model.pipeline_model_parallel_size=1', 'model.restore_from_path=results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama.nemo', 'model.global_batch_size=32', 'model.micro_batch_size=2', 'model.data.train_ds.file_names=[data/alpaca/training.jsonl]', 'model.data.validation_ds.file_names=[data/alpaca/validation.jsonl]', 'model.data.test_ds.file_names=[data/alpaca/test.jsonl]', 'model.data.train_ds.max_seq_length=8192', 'model.data.validation_ds.max_seq_length=8192', 'model.data.test_ds.max_seq_length=8192', 'model.data.train_ds.num_workers=8', 'model.data.validation_ds.num_workers=8', 'model.data.test_ds.num_workers=8', 'model.data.train_ds.concat_sampling_probabilities=[1.0]', 'model.data.train_ds.prompt_template=\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\\\nYou are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user\\'s queries promptly and politely.<|eot_id|>\\\\n<|start_header_id|>user<|end_header_id|>\\\\n{input}<|eot_id|>\\\\n<|start_header_id|>assistant<|end_header_id|>\\\\n{output}\"', 'model.optim.lr=2e-4', 'model.peft.peft_scheme=null']\n",
            "RuntimeError: [enforce fail at inline_container.cc:603] . unexpected pos 448 vs 342\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py\", line 77, in main\n",
            "    trainer.fit(model)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 538, in fit\n",
            "    call._call_and_handle_interrupt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 46, in _call_and_handle_interrupt\n",
            "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/launchers/subprocess_script.py\", line 105, in launch\n",
            "    return function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n",
            "    self._run(model, ckpt_path=ckpt_path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 981, in _run\n",
            "    results = self._run_stage()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1025, in _run_stage\n",
            "    self.fit_loop.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 205, in run\n",
            "    self.advance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 363, in advance\n",
            "    self.epoch_loop.run(self._data_fetcher)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 141, in run\n",
            "    self.on_advance_end(data_fetcher)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 295, in on_advance_end\n",
            "    self.val_loop.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py\", line 178, in _decorator\n",
            "    return loop_run(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 142, in run\n",
            "    return self.on_run_end()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 268, in on_run_end\n",
            "    self._on_evaluation_end()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 313, in _on_evaluation_end\n",
            "    call._call_callback_hooks(trainer, hook_name, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 218, in _call_callback_hooks\n",
            "    fn(trainer, trainer.lightning_module, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 335, in on_validation_end\n",
            "    self._save_last_checkpoint(trainer, monitor_candidates)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 696, in _save_last_checkpoint\n",
            "    self._save_checkpoint(trainer, filepath)\n",
            "  File \"/opt/NeMo/nemo/utils/callbacks/nemo_model_checkpoint.py\", line 470, in _save_checkpoint\n",
            "    trainer.save_checkpoint(filepath, self.save_weights_only, storage_options=storage_options)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1365, in save_checkpoint\n",
            "    self.strategy.save_checkpoint(checkpoint, filepath, storage_options=storage_options)\n",
            "  File \"/opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py\", line 398, in save_checkpoint\n",
            "    self.checkpoint_io.save_checkpoint(checkpoint, ckpt_to_dir(filepath), storage_options=storage_options)\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/opt/NeMo/nemo/utils/callbacks/dist_ckpt_io.py\", line 271, in save_checkpoint\n",
            "    return dist_checkpointing.save(\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/serialization.py\", line 382, in save\n",
            "    sharded_strategy.save(sharded_state_dict, checkpoint_dir)\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/base.py\", line 223, in save\n",
            "    async_calls.maybe_finalize_async_calls(blocking=True)\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/async_utils.py\", line 209, in maybe_finalize_async_calls\n",
            "    finalize_fn()\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py\", line 680, in finalize_fn\n",
            "    save_state_dict_async_finalize(*save_state_dict_ret)\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/state_dict_saver.py\", line 144, in save_state_dict_async_finalize\n",
            "    write_results = storage_writer.retrieve_write_results()\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/filesystem_async.py\", line 308, in retrieve_write_results\n",
            "    raise RuntimeError(f'Worker failure: {write_results_or_exc}') from write_results_or_exc\n",
            "RuntimeError: Worker failure: [enforce fail at inline_container.cc:603] . unexpected pos 448 vs 342\n",
            "\n",
            "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n",
            "Error executing job with overrides: ['trainer.devices=8', 'trainer.max_epochs=null', 'trainer.max_steps=100', 'trainer.val_check_interval=1.0', 'exp_manager.explicit_log_dir=/workspace/results/Llama-3.1-8B/SFT', 'exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True', 'model.tensor_model_parallel_size=8', 'model.pipeline_model_parallel_size=1', 'model.restore_from_path=results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama.nemo', 'model.global_batch_size=32', 'model.micro_batch_size=2', 'model.data.train_ds.file_names=[data/alpaca/training.jsonl]', 'model.data.validation_ds.file_names=[data/alpaca/validation.jsonl]', 'model.data.test_ds.file_names=[data/alpaca/test.jsonl]', 'model.data.train_ds.max_seq_length=8192', 'model.data.validation_ds.max_seq_length=8192', 'model.data.test_ds.max_seq_length=8192', 'model.data.train_ds.num_workers=8', 'model.data.validation_ds.num_workers=8', 'model.data.test_ds.num_workers=8', 'model.data.train_ds.concat_sampling_probabilities=[1.0]', 'model.data.train_ds.prompt_template=\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\\\nYou are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user\\'s queries promptly and politely.<|eot_id|>\\\\n<|start_header_id|>user<|end_header_id|>\\\\n{input}<|eot_id|>\\\\n<|start_header_id|>assistant<|end_header_id|>\\\\n{output}\"', 'model.optim.lr=2e-4', 'model.peft.peft_scheme=null']\n",
            "RuntimeError: [enforce fail at inline_container.cc:603] . unexpected pos 448 vs 342\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py\", line 77, in main\n",
            "    trainer.fit(model)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 538, in fit\n",
            "    call._call_and_handle_interrupt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 46, in _call_and_handle_interrupt\n",
            "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/launchers/subprocess_script.py\", line 105, in launch\n",
            "    return function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n",
            "    self._run(model, ckpt_path=ckpt_path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 981, in _run\n",
            "    results = self._run_stage()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1025, in _run_stage\n",
            "    self.fit_loop.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 205, in run\n",
            "    self.advance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 363, in advance\n",
            "    self.epoch_loop.run(self._data_fetcher)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 141, in run\n",
            "    self.on_advance_end(data_fetcher)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 295, in on_advance_end\n",
            "    self.val_loop.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py\", line 178, in _decorator\n",
            "    return loop_run(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 142, in run\n",
            "    return self.on_run_end()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 268, in on_run_end\n",
            "    self._on_evaluation_end()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 313, in _on_evaluation_end\n",
            "    call._call_callback_hooks(trainer, hook_name, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 218, in _call_callback_hooks\n",
            "    fn(trainer, trainer.lightning_module, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 335, in on_validation_end\n",
            "    self._save_last_checkpoint(trainer, monitor_candidates)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 696, in _save_last_checkpoint\n",
            "    self._save_checkpoint(trainer, filepath)\n",
            "  File \"/opt/NeMo/nemo/utils/callbacks/nemo_model_checkpoint.py\", line 470, in _save_checkpoint\n",
            "    trainer.save_checkpoint(filepath, self.save_weights_only, storage_options=storage_options)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1365, in save_checkpoint\n",
            "    self.strategy.save_checkpoint(checkpoint, filepath, storage_options=storage_options)\n",
            "  File \"/opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py\", line 398, in save_checkpoint\n",
            "    self.checkpoint_io.save_checkpoint(checkpoint, ckpt_to_dir(filepath), storage_options=storage_options)\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/opt/NeMo/nemo/utils/callbacks/dist_ckpt_io.py\", line 271, in save_checkpoint\n",
            "    return dist_checkpointing.save(\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/serialization.py\", line 382, in save\n",
            "    sharded_strategy.save(sharded_state_dict, checkpoint_dir)\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/base.py\", line 223, in save\n",
            "    async_calls.maybe_finalize_async_calls(blocking=True)\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/async_utils.py\", line 209, in maybe_finalize_async_calls\n",
            "    finalize_fn()\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py\", line 680, in finalize_fn\n",
            "    save_state_dict_async_finalize(*save_state_dict_ret)\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/state_dict_saver.py\", line 144, in save_state_dict_async_finalize\n",
            "    write_results = storage_writer.retrieve_write_results()\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/filesystem_async.py\", line 308, in retrieve_write_results\n",
            "    raise RuntimeError(f'Worker failure: {write_results_or_exc}') from write_results_or_exc\n",
            "RuntimeError: Worker failure: [enforce fail at inline_container.cc:603] . unexpected pos 448 vs 342\n",
            "\n",
            "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n",
            "Error executing job with overrides: ['trainer.devices=8', 'trainer.max_epochs=null', 'trainer.max_steps=100', 'trainer.val_check_interval=1.0', 'exp_manager.explicit_log_dir=/workspace/results/Llama-3.1-8B/SFT', 'exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True', 'model.tensor_model_parallel_size=8', 'model.pipeline_model_parallel_size=1', 'model.restore_from_path=results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama.nemo', 'model.global_batch_size=32', 'model.micro_batch_size=2', 'model.data.train_ds.file_names=[data/alpaca/training.jsonl]', 'model.data.validation_ds.file_names=[data/alpaca/validation.jsonl]', 'model.data.test_ds.file_names=[data/alpaca/test.jsonl]', 'model.data.train_ds.max_seq_length=8192', 'model.data.validation_ds.max_seq_length=8192', 'model.data.test_ds.max_seq_length=8192', 'model.data.train_ds.num_workers=8', 'model.data.validation_ds.num_workers=8', 'model.data.test_ds.num_workers=8', 'model.data.train_ds.concat_sampling_probabilities=[1.0]', 'model.data.train_ds.prompt_template=\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\\\nYou are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user\\'s queries promptly and politely.<|eot_id|>\\\\n<|start_header_id|>user<|end_header_id|>\\\\n{input}<|eot_id|>\\\\n<|start_header_id|>assistant<|end_header_id|>\\\\n{output}\"', 'model.optim.lr=2e-4', 'model.peft.peft_scheme=null']\n",
            "RuntimeError: [enforce fail at inline_container.cc:603] . unexpected pos 448 vs 342\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py\", line 77, in main\n",
            "    trainer.fit(model)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 538, in fit\n",
            "    call._call_and_handle_interrupt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 46, in _call_and_handle_interrupt\n",
            "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/launchers/subprocess_script.py\", line 105, in launch\n",
            "    return function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n",
            "    self._run(model, ckpt_path=ckpt_path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 981, in _run\n",
            "    results = self._run_stage()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1025, in _run_stage\n",
            "    self.fit_loop.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 205, in run\n",
            "    self.advance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 363, in advance\n",
            "    self.epoch_loop.run(self._data_fetcher)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 141, in run\n",
            "    self.on_advance_end(data_fetcher)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 295, in on_advance_end\n",
            "    self.val_loop.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py\", line 178, in _decorator\n",
            "    return loop_run(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 142, in run\n",
            "    return self.on_run_end()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 268, in on_run_end\n",
            "    self._on_evaluation_end()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 313, in _on_evaluation_end\n",
            "    call._call_callback_hooks(trainer, hook_name, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 218, in _call_callback_hooks\n",
            "    fn(trainer, trainer.lightning_module, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 335, in on_validation_end\n",
            "    self._save_last_checkpoint(trainer, monitor_candidates)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 696, in _save_last_checkpoint\n",
            "    self._save_checkpoint(trainer, filepath)\n",
            "  File \"/opt/NeMo/nemo/utils/callbacks/nemo_model_checkpoint.py\", line 470, in _save_checkpoint\n",
            "    trainer.save_checkpoint(filepath, self.save_weights_only, storage_options=storage_options)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1365, in save_checkpoint\n",
            "    self.strategy.save_checkpoint(checkpoint, filepath, storage_options=storage_options)\n",
            "  File \"/opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py\", line 398, in save_checkpoint\n",
            "    self.checkpoint_io.save_checkpoint(checkpoint, ckpt_to_dir(filepath), storage_options=storage_options)\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/opt/NeMo/nemo/utils/callbacks/dist_ckpt_io.py\", line 271, in save_checkpoint\n",
            "    return dist_checkpointing.save(\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/serialization.py\", line 382, in save\n",
            "    sharded_strategy.save(sharded_state_dict, checkpoint_dir)\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/base.py\", line 223, in save\n",
            "    async_calls.maybe_finalize_async_calls(blocking=True)\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/async_utils.py\", line 209, in maybe_finalize_async_calls\n",
            "    finalize_fn()\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py\", line 680, in finalize_fn\n",
            "    save_state_dict_async_finalize(*save_state_dict_ret)\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/state_dict_saver.py\", line 144, in save_state_dict_async_finalize\n",
            "    write_results = storage_writer.retrieve_write_results()\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/filesystem_async.py\", line 308, in retrieve_write_results\n",
            "    raise RuntimeError(f'Worker failure: {write_results_or_exc}') from write_results_or_exc\n",
            "RuntimeError: Worker failure: [enforce fail at inline_container.cc:603] . unexpected pos 448 vs 342\n",
            "\n",
            "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n",
            "Error executing job with overrides: ['trainer.devices=8', 'trainer.max_epochs=null', 'trainer.max_steps=100', 'trainer.val_check_interval=1.0', 'exp_manager.explicit_log_dir=/workspace/results/Llama-3.1-8B/SFT', 'exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True', 'model.tensor_model_parallel_size=8', 'model.pipeline_model_parallel_size=1', 'model.restore_from_path=results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama.nemo', 'model.global_batch_size=32', 'model.micro_batch_size=2', 'model.data.train_ds.file_names=[data/alpaca/training.jsonl]', 'model.data.validation_ds.file_names=[data/alpaca/validation.jsonl]', 'model.data.test_ds.file_names=[data/alpaca/test.jsonl]', 'model.data.train_ds.max_seq_length=8192', 'model.data.validation_ds.max_seq_length=8192', 'model.data.test_ds.max_seq_length=8192', 'model.data.train_ds.num_workers=8', 'model.data.validation_ds.num_workers=8', 'model.data.test_ds.num_workers=8', 'model.data.train_ds.concat_sampling_probabilities=[1.0]', 'model.data.train_ds.prompt_template=\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\\\nYou are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user\\'s queries promptly and politely.<|eot_id|>\\\\n<|start_header_id|>user<|end_header_id|>\\\\n{input}<|eot_id|>\\\\n<|start_header_id|>assistant<|end_header_id|>\\\\n{output}\"', 'model.optim.lr=2e-4', 'model.peft.peft_scheme=null']\n",
            "RuntimeError: [enforce fail at inline_container.cc:603] . unexpected pos 448 vs 342\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py\", line 77, in main\n",
            "    trainer.fit(model)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 538, in fit\n",
            "    call._call_and_handle_interrupt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 46, in _call_and_handle_interrupt\n",
            "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/launchers/subprocess_script.py\", line 105, in launch\n",
            "    return function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n",
            "    self._run(model, ckpt_path=ckpt_path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 981, in _run\n",
            "    results = self._run_stage()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1025, in _run_stage\n",
            "    self.fit_loop.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 205, in run\n",
            "    self.advance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 363, in advance\n",
            "    self.epoch_loop.run(self._data_fetcher)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 141, in run\n",
            "    self.on_advance_end(data_fetcher)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 295, in on_advance_end\n",
            "    self.val_loop.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py\", line 178, in _decorator\n",
            "    return loop_run(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 142, in run\n",
            "    return self.on_run_end()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 268, in on_run_end\n",
            "    self._on_evaluation_end()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 313, in _on_evaluation_end\n",
            "    call._call_callback_hooks(trainer, hook_name, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 218, in _call_callback_hooks\n",
            "    fn(trainer, trainer.lightning_module, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 335, in on_validation_end\n",
            "    self._save_last_checkpoint(trainer, monitor_candidates)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 696, in _save_last_checkpoint\n",
            "    self._save_checkpoint(trainer, filepath)\n",
            "  File \"/opt/NeMo/nemo/utils/callbacks/nemo_model_checkpoint.py\", line 470, in _save_checkpoint\n",
            "    trainer.save_checkpoint(filepath, self.save_weights_only, storage_options=storage_options)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1365, in save_checkpoint\n",
            "    self.strategy.save_checkpoint(checkpoint, filepath, storage_options=storage_options)\n",
            "  File \"/opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py\", line 398, in save_checkpoint\n",
            "    self.checkpoint_io.save_checkpoint(checkpoint, ckpt_to_dir(filepath), storage_options=storage_options)\n",
            "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
            "    return func(*args, **kwds)\n",
            "  File \"/opt/NeMo/nemo/utils/callbacks/dist_ckpt_io.py\", line 271, in save_checkpoint\n",
            "    return dist_checkpointing.save(\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/serialization.py\", line 382, in save\n",
            "    sharded_strategy.save(sharded_state_dict, checkpoint_dir)\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/base.py\", line 223, in save\n",
            "    async_calls.maybe_finalize_async_calls(blocking=True)\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/async_utils.py\", line 209, in maybe_finalize_async_calls\n",
            "    finalize_fn()\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py\", line 680, in finalize_fn\n",
            "    save_state_dict_async_finalize(*save_state_dict_ret)\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/state_dict_saver.py\", line 144, in save_state_dict_async_finalize\n",
            "    write_results = storage_writer.retrieve_write_results()\n",
            "  File \"/opt/megatron-lm/megatron/core/dist_checkpointing/strategies/filesystem_async.py\", line 308, in retrieve_write_results\n",
            "    raise RuntimeError(f'Worker failure: {write_results_or_exc}') from write_results_or_exc\n",
            "RuntimeError: Worker failure: [enforce fail at inline_container.cc:603] . unexpected pos 448 vs 342\n",
            "\n",
            "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n",
            "[NeMo W 2024-12-12 11:33:03 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpo_ai6zd_'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n",
            "[NeMo W 2024-12-12 11:33:03 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmpvf8qem98'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n",
            "/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 8 leaked semaphore objects to clean up at shutdown\n",
            "  warnings.warn('resource_tracker: There appear to be %d '\n",
            "/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 8 leaked semaphore objects to clean up at shutdown\n",
            "  warnings.warn('resource_tracker: There appear to be %d '\n",
            "/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 8 leaked semaphore objects to clean up at shutdown\n",
            "  warnings.warn('resource_tracker: There appear to be %d '\n",
            "/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 8 leaked semaphore objects to clean up at shutdown\n",
            "  warnings.warn('resource_tracker: There appear to be %d '\n",
            "/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 8 leaked semaphore objects to clean up at shutdown\n",
            "  warnings.warn('resource_tracker: There appear to be %d '\n",
            "/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 8 leaked semaphore objects to clean up at shutdown\n",
            "  warnings.warn('resource_tracker: There appear to be %d '\n",
            "/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 8 leaked semaphore objects to clean up at shutdown\n",
            "  warnings.warn('resource_tracker: There appear to be %d '\n",
            "/usr/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 8 leaked semaphore objects to clean up at shutdown\n",
            "  warnings.warn('resource_tracker: There appear to be %d '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: : 100%|██████████| 100/100 [05:35<00:00, reduced_train_loss=2.900, global_step=99.00, consumed_samples=3200.0, train_step_timing in s=1.940, val_loss=2.640]"
          ]
        },
        {
          "ename": "CalledProcessError",
          "evalue": "Command 'b'\\nMODEL_NAME=Llama-3.1-8B\\nMODEL=results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama.nemo\\nNUM_GPUS=8\\nMAX_STEPS=100\\nVAL_INTERVAL=1.0\\nGBS=32\\nMBS=2\\nTP=8\\nPP=1\\nLR=2e-4\\nSEQ_LEN=8192\\nTRAIN_DS=[data/alpaca/training.jsonl]\\nVALID_DS=[data/alpaca/validation.jsonl]\\nTEST_DS=[data/alpaca/test.jsonl]\\nCONCAT_SAMPLING_PROBS=[1.0]\\nPROMPT_TEMPLATE=\"\\\\\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\\\n\\\\\\nYou are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user\\'s queries promptly and politely.<|eot_id|>\\\\n\\\\\\n<|start_header_id|>user<|end_header_id|>\\\\n\\\\\\n{input}<|eot_id|>\\\\n\\\\\\n<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\\\n{output}\\\\\"\"\\n\\npython /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\\\\n--config-path=/opt/NeMo/examples/nlp/language_modeling/tuning/conf --config-name=megatron_gpt_finetuning_config \\\\\\ntrainer.devices=$NUM_GPUS \\\\\\ntrainer.max_epochs=null \\\\\\ntrainer.max_steps=$MAX_STEPS \\\\\\ntrainer.val_check_interval=$VAL_INTERVAL \\\\\\nexp_manager.explicit_log_dir=/workspace/results/$MODEL_NAME/SFT \\\\\\nexp_manager.checkpoint_callback_params.save_nemo_on_train_end=True \\\\\\nmodel.tensor_model_parallel_size=$TP \\\\\\nmodel.pipeline_model_parallel_size=$PP \\\\\\nmodel.restore_from_path=$MODEL \\\\\\nmodel.global_batch_size=$GBS \\\\\\nmodel.micro_batch_size=$MBS \\\\\\nmodel.data.train_ds.file_names=${TRAIN_DS} \\\\\\nmodel.data.validation_ds.file_names=${VALID_DS} \\\\\\nmodel.data.test_ds.file_names=${TEST_DS} \\\\\\nmodel.data.train_ds.max_seq_length=$SEQ_LEN \\\\\\nmodel.data.validation_ds.max_seq_length=$SEQ_LEN \\\\\\nmodel.data.test_ds.max_seq_length=$SEQ_LEN \\\\\\nmodel.data.train_ds.num_workers=8 \\\\\\nmodel.data.validation_ds.num_workers=8 \\\\\\nmodel.data.test_ds.num_workers=8 \\\\\\nmodel.data.train_ds.concat_sampling_probabilities=${CONCAT_SAMPLING_PROBS} \\\\\\nmodel.data.train_ds.prompt_template=\"$PROMPT_TEMPLATE\" \\\\\\nmodel.optim.lr=$LR \\\\\\nmodel.peft.peft_scheme=null\\n'' returned non-zero exit status 1.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mMODEL_NAME=Llama-3.1-8B\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mMODEL=results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama.nemo\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mNUM_GPUS=8\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mMAX_STEPS=100\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mVAL_INTERVAL=1.0\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mGBS=32\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mMBS=2\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mTP=8\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mPP=1\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mLR=2e-4\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mSEQ_LEN=8192\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mTRAIN_DS=[data/alpaca/training.jsonl]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mVALID_DS=[data/alpaca/validation.jsonl]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mTEST_DS=[data/alpaca/test.jsonl]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mCONCAT_SAMPLING_PROBS=[1.0]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mPROMPT_TEMPLATE=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<|begin_of_text|><|start_header_id|>system<|end_header_id|>\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mYou are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43ms queries promptly and politely.<|eot_id|>\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m<|start_header_id|>user<|end_header_id|>\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;132;43;01m{input}\u001b[39;49;00m\u001b[38;5;124;43m<|eot_id|>\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m<|start_header_id|>assistant<|end_header_id|>\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;132;43;01m{output}\u001b[39;49;00m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mpython /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m--config-path=/opt/NeMo/examples/nlp/language_modeling/tuning/conf --config-name=megatron_gpt_finetuning_config \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mtrainer.devices=$NUM_GPUS \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mtrainer.max_epochs=null \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mtrainer.max_steps=$MAX_STEPS \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mtrainer.val_check_interval=$VAL_INTERVAL \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mexp_manager.explicit_log_dir=/workspace/results/$MODEL_NAME/SFT \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mexp_manager.checkpoint_callback_params.save_nemo_on_train_end=True \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.tensor_model_parallel_size=$TP \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.pipeline_model_parallel_size=$PP \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.restore_from_path=$MODEL \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.global_batch_size=$GBS \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.micro_batch_size=$MBS \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.data.train_ds.file_names=$\u001b[39;49m\u001b[38;5;132;43;01m{TRAIN_DS}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.data.validation_ds.file_names=$\u001b[39;49m\u001b[38;5;132;43;01m{VALID_DS}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.data.test_ds.file_names=$\u001b[39;49m\u001b[38;5;132;43;01m{TEST_DS}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.data.train_ds.max_seq_length=$SEQ_LEN \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.data.validation_ds.max_seq_length=$SEQ_LEN \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.data.test_ds.max_seq_length=$SEQ_LEN \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.data.train_ds.num_workers=8 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.data.validation_ds.num_workers=8 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.data.test_ds.num_workers=8 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.data.train_ds.concat_sampling_probabilities=$\u001b[39;49m\u001b[38;5;132;43;01m{CONCAT_SAMPLING_PROBS}\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.data.train_ds.prompt_template=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m$PROMPT_TEMPLATE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.optim.lr=$LR \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.peft.peft_scheme=null\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:2517\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2515\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2516\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2517\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2519\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/script.py:154\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/script.py:314\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\nMODEL_NAME=Llama-3.1-8B\\nMODEL=results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama.nemo\\nNUM_GPUS=8\\nMAX_STEPS=100\\nVAL_INTERVAL=1.0\\nGBS=32\\nMBS=2\\nTP=8\\nPP=1\\nLR=2e-4\\nSEQ_LEN=8192\\nTRAIN_DS=[data/alpaca/training.jsonl]\\nVALID_DS=[data/alpaca/validation.jsonl]\\nTEST_DS=[data/alpaca/test.jsonl]\\nCONCAT_SAMPLING_PROBS=[1.0]\\nPROMPT_TEMPLATE=\"\\\\\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\\\n\\\\\\nYou are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user\\'s queries promptly and politely.<|eot_id|>\\\\n\\\\\\n<|start_header_id|>user<|end_header_id|>\\\\n\\\\\\n{input}<|eot_id|>\\\\n\\\\\\n<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\\\n{output}\\\\\"\"\\n\\npython /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\\\\n--config-path=/opt/NeMo/examples/nlp/language_modeling/tuning/conf --config-name=megatron_gpt_finetuning_config \\\\\\ntrainer.devices=$NUM_GPUS \\\\\\ntrainer.max_epochs=null \\\\\\ntrainer.max_steps=$MAX_STEPS \\\\\\ntrainer.val_check_interval=$VAL_INTERVAL \\\\\\nexp_manager.explicit_log_dir=/workspace/results/$MODEL_NAME/SFT \\\\\\nexp_manager.checkpoint_callback_params.save_nemo_on_train_end=True \\\\\\nmodel.tensor_model_parallel_size=$TP \\\\\\nmodel.pipeline_model_parallel_size=$PP \\\\\\nmodel.restore_from_path=$MODEL \\\\\\nmodel.global_batch_size=$GBS \\\\\\nmodel.micro_batch_size=$MBS \\\\\\nmodel.data.train_ds.file_names=${TRAIN_DS} \\\\\\nmodel.data.validation_ds.file_names=${VALID_DS} \\\\\\nmodel.data.test_ds.file_names=${TEST_DS} \\\\\\nmodel.data.train_ds.max_seq_length=$SEQ_LEN \\\\\\nmodel.data.validation_ds.max_seq_length=$SEQ_LEN \\\\\\nmodel.data.test_ds.max_seq_length=$SEQ_LEN \\\\\\nmodel.data.train_ds.num_workers=8 \\\\\\nmodel.data.validation_ds.num_workers=8 \\\\\\nmodel.data.test_ds.num_workers=8 \\\\\\nmodel.data.train_ds.concat_sampling_probabilities=${CONCAT_SAMPLING_PROBS} \\\\\\nmodel.data.train_ds.prompt_template=\"$PROMPT_TEMPLATE\" \\\\\\nmodel.optim.lr=$LR \\\\\\nmodel.peft.peft_scheme=null\\n'' returned non-zero exit status 1."
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "MODEL_NAME=Llama-3.1-8B\n",
        "MODEL=results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama.nemo\n",
        "NUM_GPUS=8\n",
        "MAX_STEPS=100\n",
        "VAL_INTERVAL=1.0\n",
        "GBS=32\n",
        "MBS=2\n",
        "TP=8\n",
        "PP=1\n",
        "LR=2e-4\n",
        "SEQ_LEN=8192\n",
        "TRAIN_DS=[data/alpaca/training.jsonl]\n",
        "VALID_DS=[data/alpaca/validation.jsonl]\n",
        "TEST_DS=[data/alpaca/test.jsonl]\n",
        "CONCAT_SAMPLING_PROBS=[1.0]\n",
        "PROMPT_TEMPLATE=\"\\\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\\n",
        "You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\\n\\\n",
        "<|start_header_id|>user<|end_header_id|>\\n\\\n",
        "{input}<|eot_id|>\\n\\\n",
        "<|start_header_id|>assistant<|end_header_id|>\\n\\\n",
        "{output}\\\"\"\n",
        "\n",
        "python /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\n",
        "--config-path=/opt/NeMo/examples/nlp/language_modeling/tuning/conf --config-name=megatron_gpt_finetuning_config \\\n",
        "trainer.devices=$NUM_GPUS \\\n",
        "trainer.max_epochs=null \\\n",
        "trainer.max_steps=$MAX_STEPS \\\n",
        "trainer.val_check_interval=$VAL_INTERVAL \\\n",
        "exp_manager.explicit_log_dir=/workspace/results/$MODEL_NAME/SFT \\\n",
        "exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True \\\n",
        "model.tensor_model_parallel_size=$TP \\\n",
        "model.pipeline_model_parallel_size=$PP \\\n",
        "model.restore_from_path=$MODEL \\\n",
        "model.global_batch_size=$GBS \\\n",
        "model.micro_batch_size=$MBS \\\n",
        "model.data.train_ds.file_names=${TRAIN_DS} \\\n",
        "model.data.validation_ds.file_names=${VALID_DS} \\\n",
        "model.data.test_ds.file_names=${TEST_DS} \\\n",
        "model.data.train_ds.max_seq_length=$SEQ_LEN \\\n",
        "model.data.validation_ds.max_seq_length=$SEQ_LEN \\\n",
        "model.data.test_ds.max_seq_length=$SEQ_LEN \\\n",
        "model.data.train_ds.num_workers=8 \\\n",
        "model.data.validation_ds.num_workers=8 \\\n",
        "model.data.test_ds.num_workers=8 \\\n",
        "model.data.train_ds.concat_sampling_probabilities=${CONCAT_SAMPLING_PROBS} \\\n",
        "model.data.train_ds.prompt_template=\"$PROMPT_TEMPLATE\" \\\n",
        "model.optim.lr=$LR \\\n",
        "model.peft.peft_scheme=null"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "04043ac7-52ae-4961-b2ab-d55fb235e252",
      "metadata": {
        "id": "04043ac7-52ae-4961-b2ab-d55fb235e252"
      },
      "source": [
        "# 原始的code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a62ef2e3-27cf-481b-925f-ef6732c1a9fb",
      "metadata": {
        "id": "a62ef2e3-27cf-481b-925f-ef6732c1a9fb"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "MODEL_NAME=Llama-3.1-8B\n",
        "MODEL=results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama.nemo\n",
        "NUM_GPUS=4\n",
        "MAX_STEPS=100\n",
        "VAL_INTERVAL=1.0\n",
        "GBS=16\n",
        "MBS=1\n",
        "TP=4\n",
        "PP=1\n",
        "LR=1e-4\n",
        "SEQ_LEN=8192\n",
        "TRAIN_DS=[data/alpaca/training.jsonl]\n",
        "VALID_DS=[data/alpaca/validation.jsonl]\n",
        "TEST_DS=[data/alpaca/test.jsonl]\n",
        "CONCAT_SAMPLING_PROBS=[1.0]\n",
        "PROMPT_TEMPLATE=\"\\\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\\n",
        "You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\\n\\\n",
        "<|start_header_id|>user<|end_header_id|>\\n\\\n",
        "{input}<|eot_id|>\\n\\\n",
        "<|start_header_id|>assistant<|end_header_id|>\\n\\\n",
        "{output}\\\"\"\n",
        "\n",
        "python /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\n",
        "--config-path=/opt/NeMo/examples/nlp/language_modeling/tuning/conf --config-name=megatron_gpt_finetuning_config \\\n",
        "trainer.devices=$NUM_GPUS \\\n",
        "trainer.max_epochs=null \\\n",
        "trainer.max_steps=$MAX_STEPS \\\n",
        "trainer.val_check_interval=$VAL_INTERVAL \\\n",
        "exp_manager.explicit_log_dir=/workspace/results/$MODEL_NAME/SFT \\\n",
        "exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True \\\n",
        "model.tensor_model_parallel_size=$TP \\\n",
        "model.pipeline_model_parallel_size=$PP \\\n",
        "model.restore_from_path=$MODEL \\\n",
        "model.global_batch_size=$GBS \\\n",
        "model.micro_batch_size=$MBS \\\n",
        "model.data.train_ds.file_names=${TRAIN_DS} \\\n",
        "model.data.validation_ds.file_names=${VALID_DS} \\\n",
        "model.data.test_ds.file_names=${TEST_DS} \\\n",
        "model.data.train_ds.max_seq_length=$SEQ_LEN \\\n",
        "model.data.validation_ds.max_seq_length=$SEQ_LEN \\\n",
        "model.data.test_ds.max_seq_length=$SEQ_LEN \\\n",
        "model.data.train_ds.num_workers=0 \\\n",
        "model.data.validation_ds.num_workers=0 \\\n",
        "model.data.test_ds.num_workers=0 \\\n",
        "model.data.train_ds.concat_sampling_probabilities=${CONCAT_SAMPLING_PROBS} \\\n",
        "model.data.train_ds.prompt_template=\"$PROMPT_TEMPLATE\" \\\n",
        "model.optim.lr=$LR \\\n",
        "model.peft.peft_scheme=null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92d18124-c08d-4d3f-b9c3-9fc8a217af8a",
      "metadata": {
        "id": "92d18124-c08d-4d3f-b9c3-9fc8a217af8a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "d361e97e",
      "metadata": {
        "id": "d361e97e"
      },
      "source": [
        "### 2.4. Parameter Efficient Fine-tuning <a name='s2.4'></a>\n",
        "Fine-tuning language model can be computationally expensive and risk overfitting, especially with small, specialized datasets. Parameter-efficient fine-tuning methods like LoRA offer a solution. These techniques adapt the model to specific tasks by modifying only a subset of parameters, reducing computational costs and mitigating overfitting risks. In essence, LoRA enable a more efficient and targeted adaptation of large language models for specialized tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "71cb6ccc-18ad-4fd9-90db-f9513a16e065",
      "metadata": {
        "id": "71cb6ccc-18ad-4fd9-90db-f9513a16e065"
      },
      "source": [
        "# 原始的code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cc9c5257-4ad2-4453-aff5-4b9712b634d1",
      "metadata": {
        "tags": [],
        "id": "cc9c5257-4ad2-4453-aff5-4b9712b634d1",
        "outputId": "cafb0977-9f75-450f-b43a-bf8fadf618e2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 12:03:59 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 12:03:59 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 12:03:59 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:03:59 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 12:03:59 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 12:03:59 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 12:04:00 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:04:01 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 12:04:01 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 12:04:01 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 12:04:02 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "      ret = run_job(\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 12:04:02 megatron_gpt_finetuning:56] \n",
            "    \n",
            "    ************** Experiment configuration ***********\n",
            "[NeMo I 2024-12-12 12:04:02 megatron_gpt_finetuning:57] \n",
            "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
            "    trainer:\n",
            "      devices: 4\n",
            "      accelerator: gpu\n",
            "      num_nodes: 1\n",
            "      precision: 16\n",
            "      logger: false\n",
            "      enable_checkpointing: false\n",
            "      use_distributed_sampler: false\n",
            "      max_epochs: null\n",
            "      max_steps: 100\n",
            "      log_every_n_steps: 10\n",
            "      val_check_interval: 1.0\n",
            "      gradient_clip_val: 1.0\n",
            "    exp_manager:\n",
            "      explicit_log_dir: /workspace/results/Llama-3.1-8B/PEFT\n",
            "      exp_dir: null\n",
            "      name: ${name}\n",
            "      create_wandb_logger: false\n",
            "      wandb_logger_kwargs:\n",
            "        project: null\n",
            "        name: null\n",
            "      resume_if_exists: true\n",
            "      resume_ignore_no_checkpoint: true\n",
            "      create_checkpoint_callback: true\n",
            "      checkpoint_callback_params:\n",
            "        monitor: validation_${model.data.validation_ds.metric.name}\n",
            "        save_top_k: 1\n",
            "        mode: min\n",
            "        save_nemo_on_train_end: true\n",
            "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
            "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
            "        always_save_nemo: false\n",
            "        save_best_model: true\n",
            "      create_early_stopping_callback: true\n",
            "      early_stopping_callback_params:\n",
            "        monitor: val_loss\n",
            "        mode: min\n",
            "        min_delta: 0.001\n",
            "        patience: 10\n",
            "        verbose: true\n",
            "        strict: false\n",
            "    model:\n",
            "      seed: 1234\n",
            "      tensor_model_parallel_size: 1\n",
            "      pipeline_model_parallel_size: 1\n",
            "      global_batch_size: 16\n",
            "      micro_batch_size: 1\n",
            "      restore_from_path: results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama.nemo\n",
            "      resume_from_checkpoint: null\n",
            "      save_nemo_on_validation_end: false\n",
            "      sync_batch_comm: false\n",
            "      megatron_amp_O2: false\n",
            "      sequence_parallel: false\n",
            "      activations_checkpoint_granularity: null\n",
            "      activations_checkpoint_method: null\n",
            "      activations_checkpoint_num_layers: null\n",
            "      activations_checkpoint_layers_per_pipeline: null\n",
            "      answer_only_loss: true\n",
            "      gradient_as_bucket_view: false\n",
            "      hidden_dropout: 0.0\n",
            "      attention_dropout: 0.0\n",
            "      ffn_dropout: 0.0\n",
            "      fsdp: false\n",
            "      fsdp_sharding_strategy: full\n",
            "      fsdp_grad_reduce_dtype: fp32\n",
            "      fsdp_sharded_checkpoint: false\n",
            "      fsdp_use_orig_params: false\n",
            "      peft:\n",
            "        peft_scheme: lora\n",
            "        restore_from_path: null\n",
            "        adapter_tuning:\n",
            "          type: parallel_adapter\n",
            "          adapter_dim: 32\n",
            "          adapter_dropout: 0.0\n",
            "          norm_position: pre\n",
            "          column_init_method: xavier\n",
            "          row_init_method: zero\n",
            "          norm_type: mixedfusedlayernorm\n",
            "          layer_selection: null\n",
            "          weight_tying: false\n",
            "          position_embedding_strategy: null\n",
            "        lora_tuning:\n",
            "          variant: nemo\n",
            "          target_modules:\n",
            "          - attention_qkv\n",
            "          adapter_dim: 32\n",
            "          alpha: ${model.peft.lora_tuning.adapter_dim}\n",
            "          adapter_dropout: 0.0\n",
            "          column_init_method: xavier\n",
            "          row_init_method: zero\n",
            "          layer_selection: null\n",
            "          weight_tying: false\n",
            "          position_embedding_strategy: null\n",
            "        p_tuning:\n",
            "          virtual_tokens: 10\n",
            "          bottleneck_dim: 1024\n",
            "          embedding_dim: 1024\n",
            "          init_std: 0.023\n",
            "        ia3_tuning:\n",
            "          layer_selection: null\n",
            "        selective_tuning:\n",
            "          tunable_base_param_names:\n",
            "          - self_attention\n",
            "          - word_embeddings\n",
            "      data:\n",
            "        train_ds:\n",
            "          file_names:\n",
            "          - data/alpaca/training.jsonl\n",
            "          global_batch_size: ${model.global_batch_size}\n",
            "          micro_batch_size: ${model.micro_batch_size}\n",
            "          shuffle: true\n",
            "          num_workers: 0\n",
            "          memmap_workers: 2\n",
            "          pin_memory: true\n",
            "          max_seq_length: 8192\n",
            "          min_seq_length: 1\n",
            "          drop_last: true\n",
            "          concat_sampling_probabilities:\n",
            "          - 1.0\n",
            "          label_key: output\n",
            "          add_eos: true\n",
            "          add_sep: false\n",
            "          add_bos: false\n",
            "          truncation_field: input\n",
            "          index_mapping_dir: null\n",
            "          prompt_template: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\\nYou\n",
            "            are a knowledgeable assistant trained to provide accurate and helpful information.\n",
            "            Please respond to the user's queries promptly and politely.<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\\n{input}<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n{output}\n",
            "          truncation_method: right\n",
            "          global_sample_mapping: false\n",
            "        validation_ds:\n",
            "          file_names:\n",
            "          - data/alpaca/validation.jsonl\n",
            "          names: null\n",
            "          global_batch_size: ${model.global_batch_size}\n",
            "          micro_batch_size: ${model.micro_batch_size}\n",
            "          shuffle: false\n",
            "          num_workers: 0\n",
            "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
            "          pin_memory: true\n",
            "          max_seq_length: 8192\n",
            "          min_seq_length: 1\n",
            "          drop_last: false\n",
            "          label_key: ${model.data.train_ds.label_key}\n",
            "          add_eos: ${model.data.train_ds.add_eos}\n",
            "          add_sep: ${model.data.train_ds.add_sep}\n",
            "          add_bos: ${model.data.train_ds.add_bos}\n",
            "          write_predictions_to_file: false\n",
            "          output_file_path_prefix: null\n",
            "          truncation_field: ${model.data.train_ds.truncation_field}\n",
            "          index_mapping_dir: null\n",
            "          prompt_template: ${model.data.train_ds.prompt_template}\n",
            "          tokens_to_generate: 32\n",
            "          truncation_method: right\n",
            "          global_sample_mapping: false\n",
            "          metric:\n",
            "            name: loss\n",
            "            average: null\n",
            "            num_classes: null\n",
            "        test_ds:\n",
            "          file_names:\n",
            "          - data/alpaca/test.jsonl\n",
            "          names: null\n",
            "          global_batch_size: ${model.global_batch_size}\n",
            "          micro_batch_size: ${model.micro_batch_size}\n",
            "          shuffle: false\n",
            "          num_workers: 0\n",
            "          memmap_workers: ${model.data.train_ds.memmap_workers}\n",
            "          pin_memory: true\n",
            "          max_seq_length: 8192\n",
            "          min_seq_length: 1\n",
            "          drop_last: false\n",
            "          label_key: ${model.data.train_ds.label_key}\n",
            "          add_eos: ${model.data.train_ds.add_eos}\n",
            "          add_sep: ${model.data.train_ds.add_sep}\n",
            "          add_bos: ${model.data.train_ds.add_bos}\n",
            "          write_predictions_to_file: false\n",
            "          output_file_path_prefix: null\n",
            "          truncation_field: ${model.data.train_ds.truncation_field}\n",
            "          index_mapping_dir: null\n",
            "          prompt_template: ${model.data.train_ds.prompt_template}\n",
            "          tokens_to_generate: 32\n",
            "          truncation_method: right\n",
            "          global_sample_mapping: false\n",
            "          metric:\n",
            "            name: loss\n",
            "            average: null\n",
            "            num_classes: null\n",
            "      optim:\n",
            "        name: fused_adam\n",
            "        lr: 0.0001\n",
            "        weight_decay: 0.01\n",
            "        betas:\n",
            "        - 0.9\n",
            "        - 0.98\n",
            "        sched:\n",
            "          name: CosineAnnealing\n",
            "          warmup_steps: 50\n",
            "          min_lr: 0.0\n",
            "          constant_steps: 0\n",
            "          monitor: val_loss\n",
            "          reduce_on_plateau: false\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 12:04:02 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py:1451: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "      super().__init__(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:04:02 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
            "    \n",
            "[NeMo W 2024-12-12 12:04:02 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py:1395: DeprecationWarning: torch.set_autocast_gpu_dtype(dtype) is deprecated. Please use torch.set_autocast_dtype('cuda', dtype) instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:678.)\n",
            "      torch.set_autocast_gpu_dtype(dtype)\n",
            "    \n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "`Trainer(val_check_interval=1.0)` was configured so validation will run at the end of the training epoch..\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 12:04:03 exp_manager:400] ExpManager schema\n",
            "[NeMo I 2024-12-12 12:04:03 exp_manager:401] {'explicit_log_dir': None, 'exp_dir': None, 'name': None, 'version': None, 'use_datetime_version': True, 'resume_if_exists': False, 'resume_past_end': False, 'resume_ignore_no_checkpoint': False, 'resume_from_checkpoint': None, 'create_tensorboard_logger': True, 'summary_writer_kwargs': None, 'create_wandb_logger': False, 'wandb_logger_kwargs': None, 'create_mlflow_logger': False, 'mlflow_logger_kwargs': {'experiment_name': None, 'tracking_uri': None, 'tags': None, 'save_dir': './mlruns', 'prefix': '', 'artifact_location': None, 'run_id': None, 'log_model': False}, 'create_dllogger_logger': False, 'dllogger_logger_kwargs': {'verbose': False, 'stdout': False, 'json_file': './dllogger.json'}, 'create_clearml_logger': False, 'clearml_logger_kwargs': {'project': None, 'task': None, 'connect_pytorch': False, 'model_name': None, 'tags': None, 'log_model': False, 'log_cfg': False, 'log_metrics': False}, 'create_neptune_logger': False, 'neptune_logger_kwargs': None, 'create_checkpoint_callback': True, 'checkpoint_callback_params': {'filepath': None, 'dirpath': None, 'filename': None, 'monitor': 'val_loss', 'verbose': True, 'save_last': True, 'save_top_k': 3, 'save_weights_only': False, 'mode': 'min', 'auto_insert_metric_name': True, 'every_n_epochs': 1, 'every_n_train_steps': None, 'train_time_interval': None, 'prefix': None, 'postfix': '.nemo', 'save_best_model': False, 'always_save_nemo': False, 'save_nemo_on_train_end': True, 'model_parallel_size': None, 'save_on_train_epoch_end': False, 'async_save': False}, 'create_early_stopping_callback': False, 'early_stopping_callback_params': {'monitor': 'val_loss', 'mode': 'min', 'min_delta': 0.001, 'patience': 10, 'verbose': True, 'strict': True, 'check_finite': True, 'stopping_threshold': None, 'divergence_threshold': None, 'check_on_train_epoch_end': None, 'log_rank_zero_only': False}, 'create_preemption_callback': True, 'files_to_copy': None, 'log_step_timing': True, 'step_timing_kwargs': {'reduction': 'mean', 'sync_cuda': False, 'buffer_size': 1}, 'log_local_rank_0_only': False, 'log_global_rank_0_only': False, 'disable_validation_on_resume': True, 'ema': {'enable': False, 'decay': 0.999, 'cpu_offload': False, 'validate_original_weights': False, 'every_n_steps': 1}, 'max_time_per_run': None, 'seconds_to_sleep': 5.0, 'create_straggler_detection_callback': False, 'straggler_detection_params': {'report_time_interval': 300.0, 'calc_relative_gpu_perf': True, 'calc_individual_gpu_perf': True, 'num_gpu_perf_scores_to_log': 5, 'gpu_relative_perf_threshold': 0.7, 'gpu_individual_perf_threshold': 0.7, 'stop_if_detected': False}, 'create_fault_tolerance_callback': False, 'fault_tolerance': {'workload_check_interval': 5.0, 'initial_rank_heartbeat_timeout': 3600.0, 'rank_heartbeat_timeout': 2700.0, 'calculate_timeouts': True, 'safety_factor': 5.0, 'rank_termination_signal': <Signals.SIGKILL: 9>, 'log_level': 'INFO', 'max_rank_restarts': 0, 'max_subsequent_job_failures': 0, 'additional_ft_launcher_args': '', 'simulated_fault': None}, 'log_tflops_per_sec_per_gpu': True}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 12:04:03 exp_manager:862] Exp_manager is logging to /workspace/results/Llama-3.1-8B/PEFT, but it already exists.\n",
            "[NeMo W 2024-12-12 12:04:03 exp_manager:784] There were no checkpoints found in checkpoint_dir or no checkpoint folder at checkpoint_dir :/workspace/results/Llama-3.1-8B/PEFT/checkpoints. Training from scratch.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 12:04:03 exp_manager:459] Experiments will be logged at /workspace/results/Llama-3.1-8B/PEFT\n",
            "[NeMo I 2024-12-12 12:04:03 exp_manager:1010] TensorboardLogger has been set up\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 12:04:03 exp_manager:1139] The checkpoint callback was told to monitor a validation value and trainer's max_steps was set to 100. Please ensure that max_steps will run for at least 1 epochs to ensure that checkpointing will not error out.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 12:04:03 exp_manager:593] TFLOPs per sec per GPU will be calculated, conditioned on supported models. Defaults to -1 upon failure.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 12:04:13 megatron_init:314] Rank 0 has data parallel group : [0, 1, 2, 3]\n",
            "[NeMo I 2024-12-12 12:04:13 megatron_init:320] Rank 0 has combined group of data parallel and context parallel : [0, 1, 2, 3]\n",
            "[NeMo I 2024-12-12 12:04:13 megatron_init:325] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3]]\n",
            "[NeMo I 2024-12-12 12:04:13 megatron_init:328] Ranks 0 has data parallel rank: 0\n",
            "[NeMo I 2024-12-12 12:04:13 megatron_init:336] Rank 0 has context parallel group: [0]\n",
            "[NeMo I 2024-12-12 12:04:13 megatron_init:339] All context parallel group ranks: [[0], [1], [2], [3]]\n",
            "[NeMo I 2024-12-12 12:04:13 megatron_init:340] Ranks 0 has context parallel rank: 0\n",
            "[NeMo I 2024-12-12 12:04:13 megatron_init:347] Rank 0 has model parallel group: [0]\n",
            "[NeMo I 2024-12-12 12:04:13 megatron_init:348] All model parallel group ranks: [[0], [1], [2], [3]]\n",
            "[NeMo I 2024-12-12 12:04:13 megatron_init:357] Rank 0 has tensor model parallel group: [0]\n",
            "[NeMo I 2024-12-12 12:04:13 megatron_init:361] All tensor model parallel group ranks: [[0], [1], [2], [3]]\n",
            "[NeMo I 2024-12-12 12:04:13 megatron_init:362] Rank 0 has tensor model parallel rank: 0\n",
            "[NeMo I 2024-12-12 12:04:13 megatron_init:382] Rank 0 has pipeline model parallel group: [0]\n",
            "[NeMo I 2024-12-12 12:04:13 megatron_init:394] Rank 0 has embedding group: [0]\n",
            "[NeMo I 2024-12-12 12:04:13 megatron_init:400] All pipeline model parallel group ranks: [[0], [1], [2], [3]]\n",
            "[NeMo I 2024-12-12 12:04:13 megatron_init:401] Rank 0 has pipeline model parallel rank 0\n",
            "[NeMo I 2024-12-12 12:04:13 megatron_init:402] All embedding group ranks: [[0], [1], [2], [3]]\n",
            "[NeMo I 2024-12-12 12:04:13 megatron_init:403] Rank 0 has embedding rank: 0\n",
            "[NeMo I 2024-12-12 12:04:13 num_microbatches_calculator:218] setting number of microbatches to constant 4\n",
            "[NeMo I 2024-12-12 12:04:13 megatron_base_model:975] When not using pipeline model parallel, gradient accumulation fusion can only be used with distributed_fused_adam.\n",
            "[NeMo I 2024-12-12 12:04:13 megatron_base_model:985] Gradient accumulation fusion can only be used with megatron amp O2 mixed precision.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:13 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 12:04:13 tokenizer_utils:184] Getting HuggingFace AutoTokenizer with pretrained_model_name: meta-llama/Meta-Llama-3.1-8B\n",
            "[NeMo I 2024-12-12 12:04:14 megatron_base_model:601] Padded vocab_size: 128256, original vocab_size: 128256, dummy tokens: 0.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: first_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: last_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: tp_only_amax_red in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_router_pre_softmax in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: external_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-12 12:04:14 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: config_logger_dir in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "apply rope scaling ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "apply rope scaling ...\n",
            "apply rope scaling ...\n",
            "apply rope scaling ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
            "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
            "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 4 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "[NeMo W 2024-12-12 12:05:05 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:755: FutureWarning: `load_state_dict` is deprecated and will be removed in future versions. Please use `load` instead.\n",
            "      checkpoint.load_state_dict(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:05 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/distributed/checkpoint/planner_helpers.py:311: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n",
            "      device = getattr(value, \"device\", None)\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 12:05:20 nlp_overrides:1358] Model MegatronGPTSFTModel was successfully restored from /workspace/results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama.nemo.\n",
            "[NeMo I 2024-12-12 12:05:20 megatron_gpt_finetuning:72] Adding adapter weights to the model for PEFT\n",
            "[NeMo I 2024-12-12 12:05:20 nlp_adapter_mixins:249] Before adding PEFT params:\n",
            "      | Name  | Type     | Params | Mode \n",
            "    -------------------------------------------\n",
            "    0 | model | GPTModel | 8.0 B  | train\n",
            "    -------------------------------------------\n",
            "    0         Trainable params\n",
            "    8.0 B     Non-trainable params\n",
            "    8.0 B     Total params\n",
            "    32,121.045Total estimated model params size (MB)\n",
            "    649       Modules in train mode\n",
            "    0         Modules in eval mode\n",
            "[NeMo I 2024-12-12 12:05:22 nlp_adapter_mixins:254] After adding PEFT params:\n",
            "      | Name  | Type     | Params | Mode \n",
            "    -------------------------------------------\n",
            "    0 | model | GPTModel | 8.0 B  | train\n",
            "    -------------------------------------------\n",
            "    10.5 M    Trainable params\n",
            "    8.0 B     Non-trainable params\n",
            "    8.0 B     Total params\n",
            "    32,162.988Total estimated model params size (MB)\n",
            "    809       Modules in train mode\n",
            "    0         Modules in eval mode\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 12:05:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:161: You have overridden `MegatronGPTSFTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:143: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_shape_pb2.py:18: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      DESCRIPTOR = _descriptor.FileDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_shape_pb2.py:36: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _descriptor.FieldDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_shape_pb2.py:29: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _TENSORSHAPEPROTO_DIM = _descriptor.Descriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/types_pb2.py:19: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      DESCRIPTOR = _descriptor.FileDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/types_pb2.py:33: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _descriptor.EnumValueDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/types_pb2.py:27: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _DATATYPE = _descriptor.EnumDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/resource_handle_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      DESCRIPTOR = _descriptor.FileDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/resource_handle_pb2.py:39: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _descriptor.FieldDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/resource_handle_pb2.py:32: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _RESOURCEHANDLEPROTO_DTYPEANDSHAPE = _descriptor.Descriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_pb2.py:21: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      DESCRIPTOR = _descriptor.FileDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_pb2.py:40: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _descriptor.FieldDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/tensor_pb2.py:33: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _TENSORPROTO = _descriptor.Descriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:20: DeprecationWarning: Call to deprecated create function FileDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      DESCRIPTOR = _descriptor.FileDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:35: DeprecationWarning: Call to deprecated create function EnumValueDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _descriptor.EnumValueDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:29: DeprecationWarning: Call to deprecated create function EnumDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _DATACLASS = _descriptor.EnumDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:74: DeprecationWarning: Call to deprecated create function FieldDescriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _descriptor.FieldDescriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/proto/summary_pb2.py:67: DeprecationWarning: Call to deprecated create function Descriptor(). Note: Create unlinked descriptors is going to go away. Please use get/find descriptors from generated code or query the descriptor_pool.\n",
            "      _SUMMARYDESCRIPTION = _descriptor.Descriptor(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:326: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
            "      np.bool8: (False, True),\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 12:05:23 megatron_gpt_sft_model:836] Building GPT SFT validation datasets.\n",
            "[NeMo I 2024-12-12 12:05:23 text_memmap_dataset:116] Building data files\n",
            "[NeMo I 2024-12-12 12:05:23 text_memmap_dataset:527] Processing 1 data files using 2 workers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 12:05:23 text_memmap_dataset:542] Time building 0 / 1 mem-mapped files: 0:00:00.098930\n",
            "[NeMo I 2024-12-12 12:05:23 text_memmap_dataset:527] Processing 1 data files using 2 workers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 12:05:23 text_memmap_dataset:542] Time building 0 / 1 mem-mapped files: 0:00:00.067703\n",
            "[NeMo I 2024-12-12 12:05:23 text_memmap_dataset:158] Loading data files\n",
            "[NeMo I 2024-12-12 12:05:23 text_memmap_dataset:249] Loading data/alpaca/validation.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 12:05:23 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/text_memmap_dataset.py:263: ResourceWarning: unclosed file <_io.BufferedReader name='data/alpaca/validation.jsonl.idx.info'>\n",
            "      idx_info_dict = pickle.load(open(idx_fn + \".info\", \"rb\"))\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 12:05:23 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.000806\n",
            "[NeMo I 2024-12-12 12:05:23 text_memmap_dataset:165] Computing global indices\n",
            "[NeMo I 2024-12-12 12:05:23 megatron_gpt_sft_model:840] Length of val dataset: 520\n",
            "[NeMo I 2024-12-12 12:05:23 megatron_gpt_sft_model:828] Building GPT SFT test datasets.\n",
            "[NeMo I 2024-12-12 12:05:23 text_memmap_dataset:116] Building data files\n",
            "[NeMo I 2024-12-12 12:05:23 text_memmap_dataset:527] Processing 1 data files using 2 workers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 12:05:23 text_memmap_dataset:542] Time building 0 / 1 mem-mapped files: 0:00:00.048978\n",
            "[NeMo I 2024-12-12 12:05:23 text_memmap_dataset:527] Processing 1 data files using 2 workers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 12:05:23 text_memmap_dataset:542] Time building 0 / 1 mem-mapped files: 0:00:00.046652\n",
            "[NeMo I 2024-12-12 12:05:23 text_memmap_dataset:158] Loading data files\n",
            "[NeMo I 2024-12-12 12:05:23 text_memmap_dataset:249] Loading data/alpaca/test.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 12:05:23 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/text_memmap_dataset.py:263: ResourceWarning: unclosed file <_io.BufferedReader name='data/alpaca/test.jsonl.idx.info'>\n",
            "      idx_info_dict = pickle.load(open(idx_fn + \".info\", \"rb\"))\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 12:05:23 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.000631\n",
            "[NeMo I 2024-12-12 12:05:23 text_memmap_dataset:165] Computing global indices\n",
            "[NeMo I 2024-12-12 12:05:23 megatron_gpt_sft_model:831] Length of test dataset: 521\n",
            "[NeMo I 2024-12-12 12:05:23 megatron_gpt_sft_model:847] Building GPT SFT traing datasets.\n",
            "[NeMo I 2024-12-12 12:05:23 text_memmap_dataset:116] Building data files\n",
            "[NeMo I 2024-12-12 12:05:23 text_memmap_dataset:527] Processing 1 data files using 2 workers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 12:05:23 text_memmap_dataset:542] Time building 0 / 1 mem-mapped files: 0:00:00.047996\n",
            "[NeMo I 2024-12-12 12:05:23 text_memmap_dataset:527] Processing 1 data files using 2 workers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 12:05:23 text_memmap_dataset:542] Time building 0 / 1 mem-mapped files: 0:00:00.047688\n",
            "[NeMo I 2024-12-12 12:05:23 text_memmap_dataset:158] Loading data files\n",
            "[NeMo I 2024-12-12 12:05:23 text_memmap_dataset:249] Loading data/alpaca/training.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 12:05:23 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/text_memmap_dataset.py:263: ResourceWarning: unclosed file <_io.BufferedReader name='data/alpaca/training.jsonl.idx.info'>\n",
            "      idx_info_dict = pickle.load(open(idx_fn + \".info\", \"rb\"))\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 12:05:23 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.000648\n",
            "[NeMo I 2024-12-12 12:05:23 text_memmap_dataset:165] Computing global indices\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 12:05:23 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron/dataset_utils.py:1332: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)\n",
            "      counts = torch.cuda.LongTensor([1])\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "make: Entering directory '/opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron'\n",
            "make: Nothing to be done for 'default'.\n",
            "make: Leaving directory '/opt/NeMo/nemo/collections/nlp/data/language_modeling/megatron'\n",
            "> building indices for blendable datasets ...\n",
            " > sample ratios:\n",
            "   dataset 0, input: 1, achieved: 1\n",
            "[NeMo I 2024-12-12 12:05:24 blendable_dataset:67] > elapsed time for building blendable dataset indices: 0.03 (sec)\n",
            "[NeMo I 2024-12-12 12:05:24 megatron_gpt_sft_model:849] Length of train dataset: 1608\n",
            "[NeMo I 2024-12-12 12:05:24 megatron_gpt_sft_model:854] Building dataloader with consumed samples: 0\n",
            "[NeMo I 2024-12-12 12:05:24 megatron_gpt_sft_model:854] Building dataloader with consumed samples: 0\n",
            "[NeMo I 2024-12-12 12:05:24 megatron_gpt_sft_model:854] Building dataloader with consumed samples: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
            "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n",
            "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3,4,5,6,7]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 12:05:24 nlp_overrides:274] Configuring DDP for model parallelism.\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 adapter_mixins:495] Unfrozen adapter : lora_kqv_adapter\n",
            "[NeMo I 2024-12-12 12:05:24 nlp_adapter_mixins:335] Optimizer groups set:\n",
            "      | Name  | Type     | Params | Mode \n",
            "    -------------------------------------------\n",
            "    0 | model | GPTModel | 8.0 B  | train\n",
            "    -------------------------------------------\n",
            "    10.5 M    Trainable params\n",
            "    8.0 B     Non-trainable params\n",
            "    8.0 B     Total params\n",
            "    32,162.988Total estimated model params size (MB)\n",
            "    809       Modules in train mode\n",
            "    0         Modules in eval mode\n",
            "[NeMo I 2024-12-12 12:05:24 modelPT:787] Optimizer config = FusedAdam (\n",
            "    Parameter Group 0\n",
            "        betas: [0.9, 0.98]\n",
            "        bias_correction: True\n",
            "        eps: 1e-08\n",
            "        lr: 0.0001\n",
            "        weight_decay: 0.01\n",
            "    )\n",
            "[NeMo I 2024-12-12 12:05:24 lr_scheduler:948] Scheduler \"<nemo.core.optim.lr_scheduler.CosineAnnealing object at 0x7f78f0fcd0c0>\" \n",
            "    will be used during training (effective maximum steps = 100) - \n",
            "    Parameters : \n",
            "    (warmup_steps: 50\n",
            "    min_lr: 0.0\n",
            "    constant_steps: 0\n",
            "    max_steps: 100\n",
            "    )\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "  | Name  | Type     | Params | Mode \n",
            "-------------------------------------------\n",
            "0 | model | GPTModel | 8.0 B  | train\n",
            "-------------------------------------------\n",
            "10.5 M    Trainable params\n",
            "8.0 B     Non-trainable params\n",
            "8.0 B     Total params\n",
            "32,162.988Total estimated model params size (MB)\n",
            "809       Modules in train mode\n",
            "0         Modules in eval mode\n",
            "[NeMo W 2024-12-12 12:05:24 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:24 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sanity Checking: |          | 0/? [00:00<?, ?it/s][NeMo I 2024-12-12 12:05:25 num_microbatches_calculator:218] setting number of microbatches to constant 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 12:05:25 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:25 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:27 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:28 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:28 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:28 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:32 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:32 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:34 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:34 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:35 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:35 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:38 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
            "      warnings.warn(\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:13<00:00,  0.15it/s][NeMo I 2024-12-12 12:05:38 num_microbatches_calculator:218] setting number of microbatches to constant 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 12:05:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('validation_loss_dataloader0', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('validation_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: Found `dataloader_iter` argument in the `training_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:40 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:40 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:41 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:41 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:46 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:46 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:47 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:215: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:48 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:48 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:48 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:51 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:51 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:54 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:54 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:56 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:56 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:58 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 12:05:58 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:06:00 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 12:06:00 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
            "      warnings.warn(\n",
            "    \n",
            "[rank1]:W1212 12:06:01.589000 139849441511232 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
            "[rank1]:W1212 12:06:01.589000 139849441511232 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
            "[rank1]:W1212 12:06:01.589000 139849441511232 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 672, actual 432\n",
            "[rank1]:W1212 12:06:01.589000 139849441511232 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
            "[rank1]:W1212 12:06:01.589000 139849441511232 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
            "[rank0]:W1212 12:06:03.679000 140162368804672 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
            "[rank0]:W1212 12:06:03.679000 140162368804672 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
            "[rank0]:W1212 12:06:03.679000 140162368804672 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 128, actual 272\n",
            "[rank0]:W1212 12:06:03.679000 140162368804672 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
            "[rank0]:W1212 12:06:03.679000 140162368804672 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
            "[rank3]:W1212 12:06:03.680000 140299892418368 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
            "[rank3]:W1212 12:06:03.680000 140299892418368 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
            "[rank3]:W1212 12:06:03.680000 140299892418368 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 432, actual 448\n",
            "[rank3]:W1212 12:06:03.680000 140299892418368 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
            "[rank3]:W1212 12:06:03.680000 140299892418368 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
            "[NeMo W 2024-12-12 12:06:04 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 12:06:04 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
            "      warnings.warn(\n",
            "    \n",
            "[rank2]:W1212 12:06:05.761000 140644075476800 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
            "[rank2]:W1212 12:06:05.761000 140644075476800 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
            "[rank2]:W1212 12:06:05.761000 140644075476800 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 592, actual 624\n",
            "[rank2]:W1212 12:06:05.761000 140644075476800 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
            "[rank2]:W1212 12:06:05.761000 140644075476800 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
            "[NeMo W 2024-12-12 12:06:06 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-12 12:06:06 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:609: UserWarning: async_grad_allreduce is deprecated, not in use anymore and will be fully removed with 0.10.0. Please use allreduce_dgrad instead.\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:07:57 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: Found `dataloader_iter` argument in the `validation_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 0: : 100%|██████████| 100/100 [02:18<00:00, reduced_train_loss=1.590, global_step=99.00, consumed_samples=1600.0, train_step_timing in s=1.550]\n",
            "Validation: |          | 0/? [00:00<?, ?it/s]\u001b[A[NeMo I 2024-12-12 12:07:57 num_microbatches_calculator:218] setting number of microbatches to constant 4\n",
            "\n",
            "Validation:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   0%|          | 0/33 [00:00<?, ?it/s]\u001b[A\n",
            "Validation DataLoader 0:   3%|▎         | 1/33 [00:00<00:08,  3.59it/s]\u001b[A\n",
            "Validation DataLoader 0:   6%|▌         | 2/33 [00:00<00:08,  3.58it/s]\u001b[A\n",
            "Validation DataLoader 0:   9%|▉         | 3/33 [00:01<00:13,  2.25it/s]\u001b[A\n",
            "Validation DataLoader 0:  12%|█▏        | 4/33 [00:02<00:15,  1.90it/s]\u001b[A\n",
            "Validation DataLoader 0:  15%|█▌        | 5/33 [00:02<00:16,  1.73it/s]\u001b[A\n",
            "Validation DataLoader 0:  18%|█▊        | 6/33 [00:03<00:16,  1.64it/s]\u001b[A\n",
            "Validation DataLoader 0:  21%|██        | 7/33 [00:04<00:16,  1.59it/s]\u001b[A\n",
            "Validation DataLoader 0:  24%|██▍       | 8/33 [00:05<00:16,  1.55it/s]\u001b[A\n",
            "Validation DataLoader 0:  27%|██▋       | 9/33 [00:05<00:15,  1.51it/s]\u001b[A\n",
            "Validation DataLoader 0:  30%|███       | 10/33 [00:06<00:15,  1.49it/s]\u001b[A\n",
            "Validation DataLoader 0:  33%|███▎      | 11/33 [00:07<00:14,  1.48it/s]\u001b[A\n",
            "Validation DataLoader 0:  36%|███▋      | 12/33 [00:08<00:14,  1.46it/s]\u001b[A\n",
            "Validation DataLoader 0:  39%|███▉      | 13/33 [00:08<00:13,  1.45it/s]\u001b[A\n",
            "Validation DataLoader 0:  42%|████▏     | 14/33 [00:09<00:13,  1.44it/s]\u001b[A\n",
            "Validation DataLoader 0:  45%|████▌     | 15/33 [00:10<00:12,  1.43it/s]\u001b[A\n",
            "Validation DataLoader 0:  48%|████▊     | 16/33 [00:11<00:11,  1.43it/s]\u001b[A\n",
            "Validation DataLoader 0:  52%|█████▏    | 17/33 [00:11<00:11,  1.42it/s]\u001b[A\n",
            "Validation DataLoader 0:  55%|█████▍    | 18/33 [00:12<00:10,  1.41it/s]\u001b[A\n",
            "Validation DataLoader 0:  58%|█████▊    | 19/33 [00:13<00:09,  1.41it/s]\u001b[A\n",
            "Validation DataLoader 0:  61%|██████    | 20/33 [00:14<00:09,  1.40it/s]\u001b[A\n",
            "Validation DataLoader 0:  64%|██████▎   | 21/33 [00:14<00:08,  1.40it/s]\u001b[A\n",
            "Validation DataLoader 0:  67%|██████▋   | 22/33 [00:15<00:07,  1.40it/s]\u001b[A\n",
            "Validation DataLoader 0:  70%|██████▉   | 23/33 [00:16<00:07,  1.39it/s]\u001b[A\n",
            "Validation DataLoader 0:  73%|███████▎  | 24/33 [00:16<00:06,  1.43it/s]\u001b[A\n",
            "Validation DataLoader 0:  76%|███████▌  | 25/33 [00:17<00:05,  1.43it/s]\u001b[A\n",
            "Validation DataLoader 0:  79%|███████▉  | 26/33 [00:18<00:04,  1.42it/s]\u001b[A\n",
            "Validation DataLoader 0:  82%|████████▏ | 27/33 [00:19<00:04,  1.42it/s]\u001b[A\n",
            "Validation DataLoader 0:  85%|████████▍ | 28/33 [00:19<00:03,  1.42it/s]\u001b[A\n",
            "Validation DataLoader 0:  88%|████████▊ | 29/33 [00:20<00:02,  1.41it/s]\u001b[A\n",
            "Validation DataLoader 0:  91%|█████████ | 30/33 [00:21<00:02,  1.41it/s]\u001b[A\n",
            "Validation DataLoader 0:  94%|█████████▍| 31/33 [00:21<00:01,  1.41it/s]\u001b[A\n",
            "Validation DataLoader 0:  97%|█████████▋| 32/33 [00:22<00:00,  1.41it/s]\u001b[A\n",
            "Validation DataLoader 0: 100%|██████████| 33/33 [00:23<00:00,  1.40it/s]\u001b[A[NeMo I 2024-12-12 12:08:21 num_microbatches_calculator:218] setting number of microbatches to constant 4\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 0, global step 100: 'validation_loss' reached 1.79125 (best 1.79125), saving model to '/workspace/results/Llama-3.1-8B/PEFT/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=1.791-step=100-consumed_samples=1600.0.ckpt' as top 1\n",
            "Error executing job with overrides: ['trainer.devices=4', 'trainer.max_epochs=null', 'trainer.max_steps=100', 'trainer.val_check_interval=1.0', 'exp_manager.explicit_log_dir=/workspace/results/Llama-3.1-8B/PEFT', 'exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True', 'model.tensor_model_parallel_size=1', 'model.restore_from_path=results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama.nemo', 'model.global_batch_size=16', 'model.micro_batch_size=1', 'model.data.train_ds.file_names=[data/alpaca/training.jsonl]', 'model.data.validation_ds.file_names=[data/alpaca/validation.jsonl]', 'model.data.test_ds.file_names=[data/alpaca/test.jsonl]', 'model.data.train_ds.max_seq_length=8192', 'model.data.validation_ds.max_seq_length=8192', 'model.data.test_ds.max_seq_length=8192', 'model.data.train_ds.num_workers=0', 'model.data.validation_ds.num_workers=0', 'model.data.test_ds.num_workers=0', 'model.data.train_ds.concat_sampling_probabilities=[1.0]', 'model.data.train_ds.prompt_template=\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\\\nYou are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user\\'s queries promptly and politely.<|eot_id|>\\\\n<|start_header_id|>user<|end_header_id|>\\\\n{input}<|eot_id|>\\\\n<|start_header_id|>assistant<|end_header_id|>\\\\n{output}\"', 'model.optim.lr=1e-4', 'model.peft.peft_scheme=lora', 'model.peft.lora_tuning.adapter_dim=32']\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py\", line 77, in main\n",
            "    trainer.fit(model)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 538, in fit\n",
            "    call._call_and_handle_interrupt(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 46, in _call_and_handle_interrupt\n",
            "    return trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/strategies/launchers/subprocess_script.py\", line 105, in launch\n",
            "    return function(*args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 574, in _fit_impl\n",
            "    self._run(model, ckpt_path=ckpt_path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 981, in _run\n",
            "    results = self._run_stage()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1025, in _run_stage\n",
            "    self.fit_loop.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 205, in run\n",
            "    self.advance()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/fit_loop.py\", line 363, in advance\n",
            "    self.epoch_loop.run(self._data_fetcher)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 141, in run\n",
            "    self.on_advance_end(data_fetcher)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/training_epoch_loop.py\", line 295, in on_advance_end\n",
            "    self.val_loop.run()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py\", line 178, in _decorator\n",
            "    return loop_run(self, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 142, in run\n",
            "    return self.on_run_end()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 268, in on_run_end\n",
            "    self._on_evaluation_end()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/evaluation_loop.py\", line 313, in _on_evaluation_end\n",
            "    call._call_callback_hooks(trainer, hook_name, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/call.py\", line 218, in _call_callback_hooks\n",
            "    fn(trainer, trainer.lightning_module, *args, **kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 334, in on_validation_end\n",
            "    self._save_topk_checkpoint(trainer, monitor_candidates)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 385, in _save_topk_checkpoint\n",
            "    self._save_monitor_checkpoint(trainer, monitor_candidates)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 705, in _save_monitor_checkpoint\n",
            "    self._update_best_and_save(current, trainer, monitor_candidates)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/callbacks/model_checkpoint.py\", line 757, in _update_best_and_save\n",
            "    self._save_checkpoint(trainer, filepath)\n",
            "  File \"/opt/NeMo/nemo/utils/callbacks/nemo_model_checkpoint.py\", line 470, in _save_checkpoint\n",
            "    trainer.save_checkpoint(filepath, self.save_weights_only, storage_options=storage_options)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/trainer.py\", line 1365, in save_checkpoint\n",
            "    self.strategy.save_checkpoint(checkpoint, filepath, storage_options=storage_options)\n",
            "  File \"/opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py\", line 411, in save_checkpoint\n",
            "    self.checkpoint_io.save_checkpoint(checkpoint, filepath, storage_options=storage_options)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning_fabric/plugins/io/torch_io.py\", line 58, in save_checkpoint\n",
            "    _atomic_save(checkpoint, path)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/lightning_fabric/utilities/cloud_io.py\", line 89, in _atomic_save\n",
            "    with fs.transaction, fs.open(urlpath, \"wb\") as f:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fsspec/transaction.py\", line 28, in __exit__\n",
            "    self.complete(commit=exc_type is None)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fsspec/transaction.py\", line 44, in complete\n",
            "    f.commit()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/fsspec/implementations/local.py\", line 405, in commit\n",
            "    shutil.move(self.temp, self.path)\n",
            "  File \"/usr/lib/python3.10/shutil.py\", line 836, in move\n",
            "    copy_function(src, real_dst)\n",
            "  File \"/usr/lib/python3.10/shutil.py\", line 434, in copy2\n",
            "    copyfile(src, dst, follow_symlinks=follow_symlinks)\n",
            "  File \"/usr/lib/python3.10/shutil.py\", line 267, in copyfile\n",
            "    _fastcopy_sendfile(fsrc, fdst)\n",
            "  File \"/usr/lib/python3.10/shutil.py\", line 156, in _fastcopy_sendfile\n",
            "    raise err from None\n",
            "  File \"/usr/lib/python3.10/shutil.py\", line 142, in _fastcopy_sendfile\n",
            "    sent = os.sendfile(outfd, infd, offset, blocksize)\n",
            "OSError: [Errno 28] No space left on device: '/tmp/tmpzwj21cjx' -> '/workspace/results/Llama-3.1-8B/PEFT/checkpoints/megatron_gpt_peft_lora_tuning--validation_loss=1.791-step=100-consumed_samples=1600.0.ckpt'\n",
            "\n",
            "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n",
            "[NeMo W 2024-12-12 12:08:40 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmplhto2rfl'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Process is interrupted.\n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "MODEL_NAME=Llama-3.1-8B\n",
        "MODEL=results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama.nemo\n",
        "NUM_GPUS=4\n",
        "MAX_STEPS=100\n",
        "VAL_INTERVAL=1.0\n",
        "GBS=16\n",
        "MBS=1\n",
        "TP=4\n",
        "PP=1\n",
        "LR=1e-4\n",
        "SEQ_LEN=8192\n",
        "TRAIN_DS=[data/alpaca/training.jsonl]\n",
        "VALID_DS=[data/alpaca/validation.jsonl]\n",
        "TEST_DS=[data/alpaca/test.jsonl]\n",
        "CONCAT_SAMPLING_PROBS=[1.0]\n",
        "PROMPT_TEMPLATE=\"\\\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\\n",
        "You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\\n\\\n",
        "<|start_header_id|>user<|end_header_id|>\\n\\\n",
        "{input}<|eot_id|>\\n\\\n",
        "<|start_header_id|>assistant<|end_header_id|>\\n\\\n",
        "{output}\\\"\"\n",
        "\n",
        "python /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\n",
        "--config-path=/opt/NeMo/examples/nlp/language_modeling/tuning/conf --config-name=megatron_gpt_finetuning_config \\\n",
        "trainer.devices=$NUM_GPUS \\\n",
        "trainer.max_epochs=null \\\n",
        "trainer.max_steps=$MAX_STEPS \\\n",
        "trainer.val_check_interval=$VAL_INTERVAL \\\n",
        "exp_manager.explicit_log_dir=/workspace/results/$MODEL_NAME/PEFT \\\n",
        "exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True \\\n",
        "model.tensor_model_parallel_size=1 \\\n",
        "model.restore_from_path=$MODEL \\\n",
        "model.global_batch_size=$GBS \\\n",
        "model.micro_batch_size=$MBS \\\n",
        "model.data.train_ds.file_names=${TRAIN_DS} \\\n",
        "model.data.validation_ds.file_names=${VALID_DS} \\\n",
        "model.data.test_ds.file_names=${TEST_DS} \\\n",
        "model.data.train_ds.max_seq_length=$SEQ_LEN \\\n",
        "model.data.validation_ds.max_seq_length=$SEQ_LEN \\\n",
        "model.data.test_ds.max_seq_length=$SEQ_LEN \\\n",
        "model.data.train_ds.num_workers=0 \\\n",
        "model.data.validation_ds.num_workers=0 \\\n",
        "model.data.test_ds.num_workers=0 \\\n",
        "model.data.train_ds.concat_sampling_probabilities=${CONCAT_SAMPLING_PROBS} \\\n",
        "model.data.train_ds.prompt_template=\"$PROMPT_TEMPLATE\" \\\n",
        "model.optim.lr=$LR \\\n",
        "model.peft.peft_scheme=lora \\\n",
        "model.peft.lora_tuning.adapter_dim=32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df789864-471b-4c64-95a4-a9f736cc6e20",
      "metadata": {
        "id": "df789864-471b-4c64-95a4-a9f736cc6e20"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8b16882b-82a5-472b-92fe-3fa0cb12447a",
      "metadata": {
        "id": "8b16882b-82a5-472b-92fe-3fa0cb12447a"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "efcc65a7",
      "metadata": {
        "id": "efcc65a7"
      },
      "source": [
        "## 3 Evaluation <a name='s3'></a>\n",
        "\n",
        "If you want to evaluate an SFT .nemo file:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b0139c7-4c2a-4cbe-a4fa-cb6f8d7969ca",
      "metadata": {
        "id": "3b0139c7-4c2a-4cbe-a4fa-cb6f8d7969ca"
      },
      "source": [
        "# 8張H100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "df7a8701-7397-4871-a552-60fc180fa927",
      "metadata": {
        "id": "df7a8701-7397-4871-a552-60fc180fa927"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "MODEL_NAME=Llama-3.1-8B\n",
        "MODEL=results/Llama-3.1-8B/SFT/checkpoints/megatron_gpt_peft_None_tuning.nemo\n",
        "NUM_GPUS=8\n",
        "TP=8\n",
        "GB=64\n",
        "SEQ_LEN=8192\n",
        "TEST_DS=[data/alpaca/test.jsonl]\n",
        "OUTPUT=data/alpaca/prediction\n",
        "PROMPT_TEMPLATE=\"\\\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\\n",
        "You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\\n\\\n",
        "<|start_header_id|>user<|end_header_id|>\\n\\\n",
        "{input}<|eot_id|>\\n\\\n",
        "<|start_header_id|>assistant<|end_header_id|>\\n\\\n",
        "{output}\\\"\"\n",
        "\n",
        "python /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py \\\n",
        "trainer.precision=bf16 \\\n",
        "trainer.devices=$NUM_GPUS \\\n",
        "model.restore_from_path=$MODEL \\\n",
        "model.global_batch_size=$GB \\\n",
        "model.tensor_model_parallel_size=$TP \\\n",
        "model.pipeline_model_parallel_size=1 \\\n",
        "model.megatron_amp_O2=True \\\n",
        "model.peft.restore_from_path=null \\\n",
        "model.data.test_ds.file_names=$TEST_DS \\\n",
        "model.data.test_ds.names=\\['alpaca_test'] \\\n",
        "model.data.test_ds.global_batch_size=$GB \\\n",
        "model.data.test_ds.tokens_to_generate=128 \\\n",
        "model.data.test_ds.label_key='output' \\\n",
        "model.data.test_ds.add_eos=True \\\n",
        "model.data.test_ds.add_sep=False \\\n",
        "model.data.test_ds.add_bos=False \\\n",
        "model.data.test_ds.max_seq_length=$SEQ_LEN \\\n",
        "model.data.test_ds.truncation_field=\"input\" \\\n",
        "model.data.test_ds.prompt_template=\"$PROMPT_TEMPLATE\" \\\n",
        "model.data.test_ds.write_predictions_to_file=True \\\n",
        "model.data.test_ds.output_file_path_prefix=$OUTPUT"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b1b41b03-656b-4699-a814-6e478674c264",
      "metadata": {
        "id": "b1b41b03-656b-4699-a814-6e478674c264"
      },
      "source": [
        "# 原始的code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ae68bb21",
      "metadata": {
        "tags": [],
        "id": "ae68bb21",
        "outputId": "ec3a2541-e967-4ffd-849b-a4ed99768512"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 12:14:35 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-12 12:14:35 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 12:14:35 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:14:35 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-12 12:14:36 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-12 12:14:36 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-12 12:14:37 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-12 12:14:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-12 12:14:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-12 12:14:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-12 12:14:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "      ret = run_job(\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-12 12:14:38 megatron_gpt_generate:125] \n",
            "    \n",
            "    ************** Experiment configuration ***********\n",
            "[NeMo I 2024-12-12 12:14:38 megatron_gpt_generate:126] \n",
            "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
            "    trainer:\n",
            "      devices: 4\n",
            "      accelerator: gpu\n",
            "      num_nodes: 1\n",
            "      precision: bf16\n",
            "      logger: false\n",
            "      enable_checkpointing: false\n",
            "      use_distributed_sampler: false\n",
            "      max_epochs: 9999\n",
            "      max_steps: 20000\n",
            "      log_every_n_steps: 10\n",
            "      val_check_interval: 200\n",
            "      gradient_clip_val: 1.0\n",
            "    exp_manager:\n",
            "      explicit_log_dir: null\n",
            "      exp_dir: null\n",
            "      name: ${name}\n",
            "      create_wandb_logger: false\n",
            "      wandb_logger_kwargs:\n",
            "        project: null\n",
            "        name: null\n",
            "      resume_if_exists: true\n",
            "      resume_ignore_no_checkpoint: true\n",
            "      create_checkpoint_callback: true\n",
            "      checkpoint_callback_params:\n",
            "        monitor: validation_${model.data.test_ds.metric.name}\n",
            "        save_top_k: 1\n",
            "        mode: max\n",
            "        save_nemo_on_train_end: true\n",
            "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
            "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
            "        always_save_nemo: true\n",
            "        save_best_model: false\n",
            "    model:\n",
            "      seed: 1234\n",
            "      tensor_model_parallel_size: 1\n",
            "      pipeline_model_parallel_size: 1\n",
            "      global_batch_size: 32\n",
            "      micro_batch_size: 1\n",
            "      restore_from_path: results/Llama-3.1-8B/SFT/checkpoints/megatron_gpt_peft_None_tuning.nemo\n",
            "      resume_from_checkpoint: null\n",
            "      save_nemo_on_validation_end: true\n",
            "      sync_batch_comm: false\n",
            "      megatron_amp_O2: true\n",
            "      sequence_parallel: false\n",
            "      activations_checkpoint_granularity: null\n",
            "      activations_checkpoint_method: null\n",
            "      activations_checkpoint_num_layers: null\n",
            "      activations_checkpoint_layers_per_pipeline: null\n",
            "      answer_only_loss: true\n",
            "      gradient_as_bucket_view: false\n",
            "      hidden_dropout: 0.0\n",
            "      attention_dropout: 0.0\n",
            "      ffn_dropout: 0.0\n",
            "      peft:\n",
            "        peft_scheme: adapter\n",
            "        restore_from_path: null\n",
            "        restore_from_ckpt:\n",
            "          checkpoint_dir: null\n",
            "          checkpoint_name: null\n",
            "        adapter_tuning:\n",
            "          type: parallel_adapter\n",
            "          adapter_dim: 32\n",
            "          adapter_dropout: 0.0\n",
            "          norm_position: pre\n",
            "          column_init_method: xavier\n",
            "          row_init_method: zero\n",
            "          norm_type: mixedfusedlayernorm\n",
            "          layer_selection: null\n",
            "          weight_tying: false\n",
            "          position_embedding_strategy: null\n",
            "        lora_tuning:\n",
            "          variant: nemo\n",
            "          target_modules:\n",
            "          - attention_qkv\n",
            "          adapter_dim: 32\n",
            "          adapter_dropout: 0.0\n",
            "          column_init_method: xavier\n",
            "          row_init_method: zero\n",
            "          layer_selection: null\n",
            "          weight_tying: false\n",
            "          position_embedding_strategy: null\n",
            "        p_tuning:\n",
            "          virtual_tokens: 10\n",
            "          bottleneck_dim: 1024\n",
            "          embedding_dim: 1024\n",
            "          init_std: 0.023\n",
            "        ia3_tuning:\n",
            "          layer_selection: null\n",
            "      data:\n",
            "        test_ds:\n",
            "          file_names:\n",
            "          - data/alpaca/test.jsonl\n",
            "          names:\n",
            "          - alpaca_test\n",
            "          global_batch_size: 32\n",
            "          micro_batch_size: 1\n",
            "          shuffle: false\n",
            "          num_workers: 0\n",
            "          pin_memory: true\n",
            "          max_seq_length: 8192\n",
            "          min_seq_length: 1\n",
            "          drop_last: false\n",
            "          context_key: input\n",
            "          label_key: output\n",
            "          add_eos: true\n",
            "          add_sep: false\n",
            "          add_bos: false\n",
            "          write_predictions_to_file: true\n",
            "          output_file_path_prefix: data/alpaca/prediction\n",
            "          truncation_field: input\n",
            "          index_mapping_dir: null\n",
            "          prompt_template: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\\nYou\n",
            "            are a knowledgeable assistant trained to provide accurate and helpful information.\n",
            "            Please respond to the user's queries promptly and politely.<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\\n{input}<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n{output}\n",
            "          tokens_to_generate: 128\n",
            "          truncation_method: right\n",
            "          metric:\n",
            "            name: loss\n",
            "            average: null\n",
            "            num_classes: null\n",
            "    inference:\n",
            "      greedy: true\n",
            "      top_k: 0\n",
            "      top_p: 0.9\n",
            "      temperature: 1.0\n",
            "      all_probs: false\n",
            "      repetition_penalty: 1.0\n",
            "      min_tokens_to_generate: 0\n",
            "      compute_logprob: false\n",
            "      outfile_path: output.txt\n",
            "      compute_attention_mask: true\n",
            "    server: false\n",
            "    port: 5555\n",
            "    web_server: false\n",
            "    share: true\n",
            "    username: test\n",
            "    password: test2\n",
            "    web_port: 9889\n",
            "    chat: false\n",
            "    chatbot_config:\n",
            "      value: false\n",
            "      attributes:\n",
            "      - name: Quality\n",
            "        min: 0\n",
            "        max: 4\n",
            "        key: quality\n",
            "        type: int\n",
            "        default: 4\n",
            "      - name: Toxicity\n",
            "        min: 0\n",
            "        max: 4\n",
            "        key: toxcity\n",
            "        type: int\n",
            "        default: 0\n",
            "      - name: Humor\n",
            "        min: 0\n",
            "        max: 4\n",
            "        key: humor\n",
            "        type: int\n",
            "        default: 0\n",
            "      - name: Creativity\n",
            "        min: 0\n",
            "        max: 4\n",
            "        key: creativity\n",
            "        type: int\n",
            "        default: 0\n",
            "      - name: Violence\n",
            "        min: 0\n",
            "        max: 4\n",
            "        key: violence\n",
            "        type: int\n",
            "        default: 0\n",
            "      - name: Helpfulness\n",
            "        min: 0\n",
            "        max: 4\n",
            "        key: helpfulness\n",
            "        type: int\n",
            "        default: 4\n",
            "      - name: Not_Appropriate\n",
            "        min: 0\n",
            "        max: 4\n",
            "        key: not_appropriate\n",
            "        type: int\n",
            "        default: 0\n",
            "      - name: Language\n",
            "        choices:\n",
            "        - ar\n",
            "        - bg\n",
            "        - bn\n",
            "        - ca\n",
            "        - cs\n",
            "        - da\n",
            "        - de\n",
            "        - el\n",
            "        - en\n",
            "        - eo\n",
            "        - es\n",
            "        - eu\n",
            "        - fa\n",
            "        - fi\n",
            "        - fr\n",
            "        - gl\n",
            "        - he\n",
            "        - hu\n",
            "        - id\n",
            "        - it\n",
            "        - ja\n",
            "        - ko\n",
            "        - nb\n",
            "        - nl\n",
            "        - pl\n",
            "        - pt\n",
            "        - ro\n",
            "        - ru\n",
            "        - sk\n",
            "        - sv\n",
            "        - th\n",
            "        - tr\n",
            "        - uk\n",
            "        - vi\n",
            "        - zh\n",
            "        key: lang\n",
            "        type: list\n",
            "        default: en\n",
            "      user: User\n",
            "      assistant: Assistant\n",
            "      system: 'A chat between a curious human and an artificial intelligence assistant.\n",
            "        The assistant gives helpful, detailed, and polite answers to the human''s questions.\n",
            "    \n",
            "    \n",
            "        '\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-12 12:14:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
            "    \n",
            "[NeMo W 2024-12-12 12:14:38 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py:1659: DeprecationWarning: torch.set_autocast_gpu_dtype(dtype) is deprecated. Please use torch.set_autocast_dtype('cuda', dtype) instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:678.)\n",
            "      torch.set_autocast_gpu_dtype(dtype)\n",
            "    \n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "Error executing job with overrides: ['trainer.precision=bf16', 'trainer.devices=4', 'model.restore_from_path=results/Llama-3.1-8B/SFT/checkpoints/megatron_gpt_peft_None_tuning.nemo', 'model.global_batch_size=32', 'model.tensor_model_parallel_size=1', 'model.pipeline_model_parallel_size=1', 'model.megatron_amp_O2=True', 'model.peft.restore_from_path=null', 'model.data.test_ds.file_names=[data/alpaca/test.jsonl]', 'model.data.test_ds.names=[alpaca_test]', 'model.data.test_ds.global_batch_size=32', 'model.data.test_ds.tokens_to_generate=128', 'model.data.test_ds.label_key=output', 'model.data.test_ds.add_eos=True', 'model.data.test_ds.add_sep=False', 'model.data.test_ds.add_bos=False', 'model.data.test_ds.max_seq_length=8192', 'model.data.test_ds.truncation_field=input', 'model.data.test_ds.prompt_template=\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\\\nYou are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user\\'s queries promptly and politely.<|eot_id|>\\\\n<|start_header_id|>user<|end_header_id|>\\\\n{input}<|eot_id|>\\\\n<|start_header_id|>assistant<|end_header_id|>\\\\n{output}\"', 'model.data.test_ds.write_predictions_to_file=True', 'model.data.test_ds.output_file_path_prefix=data/alpaca/prediction']\n",
            "Traceback (most recent call last):\n",
            "  File \"/opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py\", line 132, in main\n",
            "    model_cfg = MegatronGPTSFTModel.merge_inference_cfg(cfg.model.restore_from_path, cfg)\n",
            "  File \"/opt/NeMo/nemo/collections/nlp/parts/mixins/nlp_adapter_mixins.py\", line 597, in merge_inference_cfg\n",
            "    peft_cfg = cls.restore_from(path, return_config=True)\n",
            "  File \"/opt/NeMo/nemo/collections/nlp/models/nlp_model.py\", line 478, in restore_from\n",
            "    return super().restore_from(\n",
            "  File \"/opt/NeMo/nemo/core/classes/modelPT.py\", line 462, in restore_from\n",
            "    raise FileNotFoundError(f\"Can't find {restore_path}\")\n",
            "FileNotFoundError: Can't find /workspace/results/Llama-3.1-8B/SFT/checkpoints/megatron_gpt_peft_None_tuning.nemo\n",
            "\n",
            "Set the environment variable HYDRA_FULL_ERROR=1 for a complete stack trace.\n",
            "[NeMo W 2024-12-12 12:14:39 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp276240ej'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n"
          ]
        },
        {
          "ename": "CalledProcessError",
          "evalue": "Command 'b'\\nMODEL_NAME=Llama-3.1-8B\\nMODEL=results/Llama-3.1-8B/SFT/checkpoints/megatron_gpt_peft_None_tuning.nemo\\nNUM_GPUS=4\\nTP=1\\nGB=32\\nSEQ_LEN=8192\\nTEST_DS=[data/alpaca/test.jsonl]\\nOUTPUT=data/alpaca/prediction\\nPROMPT_TEMPLATE=\"\\\\\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\\\n\\\\\\nYou are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user\\'s queries promptly and politely.<|eot_id|>\\\\n\\\\\\n<|start_header_id|>user<|end_header_id|>\\\\n\\\\\\n{input}<|eot_id|>\\\\n\\\\\\n<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\\\n{output}\\\\\"\"\\n\\npython /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py \\\\\\ntrainer.precision=bf16 \\\\\\ntrainer.devices=$NUM_GPUS \\\\\\nmodel.restore_from_path=$MODEL \\\\\\nmodel.global_batch_size=$GB \\\\\\nmodel.tensor_model_parallel_size=$TP \\\\\\nmodel.pipeline_model_parallel_size=1 \\\\\\nmodel.megatron_amp_O2=True \\\\\\nmodel.peft.restore_from_path=null \\\\\\nmodel.data.test_ds.file_names=$TEST_DS \\\\\\nmodel.data.test_ds.names=\\\\[\\'alpaca_test\\'] \\\\\\nmodel.data.test_ds.global_batch_size=$GB \\\\\\nmodel.data.test_ds.tokens_to_generate=128 \\\\\\nmodel.data.test_ds.label_key=\\'output\\' \\\\\\nmodel.data.test_ds.add_eos=True \\\\\\nmodel.data.test_ds.add_sep=False \\\\\\nmodel.data.test_ds.add_bos=False \\\\\\nmodel.data.test_ds.max_seq_length=$SEQ_LEN \\\\\\nmodel.data.test_ds.truncation_field=\"input\" \\\\\\nmodel.data.test_ds.prompt_template=\"$PROMPT_TEMPLATE\" \\\\\\nmodel.data.test_ds.write_predictions_to_file=True \\\\\\nmodel.data.test_ds.output_file_path_prefix=$OUTPUT\\n'' returned non-zero exit status 1.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mMODEL_NAME=Llama-3.1-8B\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mMODEL=results/Llama-3.1-8B/SFT/checkpoints/megatron_gpt_peft_None_tuning.nemo\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mNUM_GPUS=4\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mTP=1\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mGB=32\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mSEQ_LEN=8192\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mTEST_DS=[data/alpaca/test.jsonl]\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mOUTPUT=data/alpaca/prediction\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mPROMPT_TEMPLATE=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m<|begin_of_text|><|start_header_id|>system<|end_header_id|>\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mYou are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43ms queries promptly and politely.<|eot_id|>\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m<|start_header_id|>user<|end_header_id|>\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;132;43;01m{input}\u001b[39;49;00m\u001b[38;5;124;43m<|eot_id|>\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m<|start_header_id|>assistant<|end_header_id|>\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;132;43;01m{output}\u001b[39;49;00m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mpython /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mtrainer.precision=bf16 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mtrainer.devices=$NUM_GPUS \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.restore_from_path=$MODEL \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.global_batch_size=$GB \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.tensor_model_parallel_size=$TP \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.pipeline_model_parallel_size=1 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.megatron_amp_O2=True \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.peft.restore_from_path=null \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.data.test_ds.file_names=$TEST_DS \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.data.test_ds.names=\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43m[\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43malpaca_test\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m] \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.data.test_ds.global_batch_size=$GB \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.data.test_ds.tokens_to_generate=128 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.data.test_ds.label_key=\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43moutput\u001b[39;49m\u001b[38;5;130;43;01m\\'\u001b[39;49;00m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.data.test_ds.add_eos=True \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.data.test_ds.add_sep=False \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.data.test_ds.add_bos=False \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.data.test_ds.max_seq_length=$SEQ_LEN \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.data.test_ds.truncation_field=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.data.test_ds.prompt_template=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m$PROMPT_TEMPLATE\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.data.test_ds.write_predictions_to_file=True \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mmodel.data.test_ds.output_file_path_prefix=$OUTPUT\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:2517\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2515\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2516\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2517\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2519\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/script.py:154\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/script.py:314\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'\\nMODEL_NAME=Llama-3.1-8B\\nMODEL=results/Llama-3.1-8B/SFT/checkpoints/megatron_gpt_peft_None_tuning.nemo\\nNUM_GPUS=4\\nTP=1\\nGB=32\\nSEQ_LEN=8192\\nTEST_DS=[data/alpaca/test.jsonl]\\nOUTPUT=data/alpaca/prediction\\nPROMPT_TEMPLATE=\"\\\\\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\\\n\\\\\\nYou are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user\\'s queries promptly and politely.<|eot_id|>\\\\n\\\\\\n<|start_header_id|>user<|end_header_id|>\\\\n\\\\\\n{input}<|eot_id|>\\\\n\\\\\\n<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\\\n{output}\\\\\"\"\\n\\npython /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py \\\\\\ntrainer.precision=bf16 \\\\\\ntrainer.devices=$NUM_GPUS \\\\\\nmodel.restore_from_path=$MODEL \\\\\\nmodel.global_batch_size=$GB \\\\\\nmodel.tensor_model_parallel_size=$TP \\\\\\nmodel.pipeline_model_parallel_size=1 \\\\\\nmodel.megatron_amp_O2=True \\\\\\nmodel.peft.restore_from_path=null \\\\\\nmodel.data.test_ds.file_names=$TEST_DS \\\\\\nmodel.data.test_ds.names=\\\\[\\'alpaca_test\\'] \\\\\\nmodel.data.test_ds.global_batch_size=$GB \\\\\\nmodel.data.test_ds.tokens_to_generate=128 \\\\\\nmodel.data.test_ds.label_key=\\'output\\' \\\\\\nmodel.data.test_ds.add_eos=True \\\\\\nmodel.data.test_ds.add_sep=False \\\\\\nmodel.data.test_ds.add_bos=False \\\\\\nmodel.data.test_ds.max_seq_length=$SEQ_LEN \\\\\\nmodel.data.test_ds.truncation_field=\"input\" \\\\\\nmodel.data.test_ds.prompt_template=\"$PROMPT_TEMPLATE\" \\\\\\nmodel.data.test_ds.write_predictions_to_file=True \\\\\\nmodel.data.test_ds.output_file_path_prefix=$OUTPUT\\n'' returned non-zero exit status 1."
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "MODEL_NAME=Llama-3.1-8B\n",
        "MODEL=results/Llama-3.1-8B/SFT/checkpoints/megatron_gpt_peft_None_tuning.nemo\n",
        "NUM_GPUS=4\n",
        "TP=1\n",
        "GB=32\n",
        "SEQ_LEN=8192\n",
        "TEST_DS=[data/alpaca/test.jsonl]\n",
        "OUTPUT=data/alpaca/prediction\n",
        "PROMPT_TEMPLATE=\"\\\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\\n",
        "You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\\n\\\n",
        "<|start_header_id|>user<|end_header_id|>\\n\\\n",
        "{input}<|eot_id|>\\n\\\n",
        "<|start_header_id|>assistant<|end_header_id|>\\n\\\n",
        "{output}\\\"\"\n",
        "\n",
        "python /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py \\\n",
        "trainer.precision=bf16 \\\n",
        "trainer.devices=$NUM_GPUS \\\n",
        "model.restore_from_path=$MODEL \\\n",
        "model.global_batch_size=$GB \\\n",
        "model.tensor_model_parallel_size=$TP \\\n",
        "model.pipeline_model_parallel_size=1 \\\n",
        "model.megatron_amp_O2=True \\\n",
        "model.peft.restore_from_path=null \\\n",
        "model.data.test_ds.file_names=$TEST_DS \\\n",
        "model.data.test_ds.names=\\['alpaca_test'] \\\n",
        "model.data.test_ds.global_batch_size=$GB \\\n",
        "model.data.test_ds.tokens_to_generate=128 \\\n",
        "model.data.test_ds.label_key='output' \\\n",
        "model.data.test_ds.add_eos=True \\\n",
        "model.data.test_ds.add_sep=False \\\n",
        "model.data.test_ds.add_bos=False \\\n",
        "model.data.test_ds.max_seq_length=$SEQ_LEN \\\n",
        "model.data.test_ds.truncation_field=\"input\" \\\n",
        "model.data.test_ds.prompt_template=\"$PROMPT_TEMPLATE\" \\\n",
        "model.data.test_ds.write_predictions_to_file=True \\\n",
        "model.data.test_ds.output_file_path_prefix=$OUTPUT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bb57f32b-6f82-47b8-881b-3c1403963973",
      "metadata": {
        "tags": [],
        "id": "bb57f32b-6f82-47b8-881b-3c1403963973"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def modify_and_overwrite_jsonl(file_path):\n",
        "    data_list = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line in file:\n",
        "            data = json.loads(line)\n",
        "            data_list.append(data)\n",
        "\n",
        "    with open(file_path, 'w', encoding='utf-8') as file:\n",
        "        for data in data_list:\n",
        "            json_line = json.dumps(data, ensure_ascii=False) + \"\\n\"\n",
        "            file.write(json_line)\n",
        "\n",
        "file_path = \"data/alpaca/prediction_test_alpaca_test_inputs_preds_labels.jsonl\"\n",
        "modify_and_overwrite_jsonl(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f92b2d8f",
      "metadata": {
        "id": "f92b2d8f"
      },
      "source": [
        "If you want to evaluate a PEFT Model, you should provide a base GPT model and a PEFT model .nemo file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "af7fd0f3",
      "metadata": {
        "tags": [],
        "id": "af7fd0f3",
        "outputId": "50849ce8-dfcd-4e5a-dd28-d5c97dfe90c1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-06 09:00:36 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:280: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, input, weight, bias, allreduce_dgrad):\n",
            "    \n",
            "[NeMo W 2024-12-06 09:00:36 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:290: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-06 09:00:36 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:381: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-06 09:00:36 nemo_logging:349] /opt/megatron-lm/megatron/core/tensor_parallel/layers.py:420: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-06 09:00:37 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:17: DeprecationWarning: `torch.distributed._sharded_tensor` will be deprecated, use `torch.distributed._shard.sharded_tensor` instead\n",
            "      from torch.distributed._sharded_tensor import ShardedTensor as TorchShardedTensor\n",
            "    \n",
            "[NeMo W 2024-12-06 09:00:37 nemo_logging:349] /opt/megatron-lm/megatron/core/transformer/attention.py:29: DeprecationWarning: The 'megatron.core.transformer.custom_layers.transformer_engine' \n",
            "        module is deprecated and will be removed in 0.10.0. Please use \n",
            "        'megatron.core.extensions.transformer_engine' instead.\n",
            "      from megatron.core.transformer.custom_layers.transformer_engine import SplitAlongDim\n",
            "    \n",
            "[NeMo W 2024-12-06 09:00:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/modelopt/torch/quantization/tensor_quant.py:168: FutureWarning: `torch.library.impl_abstract` was renamed to `torch.library.register_fake`. Please use that instead; we will remove `torch.library.impl_abstract` in a future version of PyTorch.\n",
            "      quantize_op_abstract = torch.library.impl_abstract(\"tensorrt::quantize_op\")(\n",
            "    \n",
            "[NeMo W 2024-12-06 09:00:39 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pyannote/core/notebook.py:134: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "      cm = get_cmap(\"Set1\")\n",
            "    \n",
            "[NeMo W 2024-12-06 09:00:39 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/nvidia/dali/_autograph/pyct/gast_util.py:79: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      if get_gast_version() < LooseVersion(\"0.5\"):\n",
            "    \n",
            "[NeMo W 2024-12-06 09:00:39 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/setuptools/_distutils/version.py:337: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
            "      other = LooseVersion(other)\n",
            "    \n",
            "[NeMo W 2024-12-06 09:00:40 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.\n",
            "    See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.\n",
            "      ret = run_job(\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-06 09:00:40 megatron_gpt_generate:125] \n",
            "    \n",
            "    ************** Experiment configuration ***********\n",
            "[NeMo I 2024-12-06 09:00:40 megatron_gpt_generate:126] \n",
            "    name: megatron_gpt_peft_${model.peft.peft_scheme}_tuning\n",
            "    trainer:\n",
            "      devices: 4\n",
            "      accelerator: gpu\n",
            "      num_nodes: 1\n",
            "      precision: 16\n",
            "      logger: false\n",
            "      enable_checkpointing: false\n",
            "      use_distributed_sampler: false\n",
            "      max_epochs: 9999\n",
            "      max_steps: 20000\n",
            "      log_every_n_steps: 10\n",
            "      val_check_interval: 200\n",
            "      gradient_clip_val: 1.0\n",
            "    exp_manager:\n",
            "      explicit_log_dir: null\n",
            "      exp_dir: null\n",
            "      name: ${name}\n",
            "      create_wandb_logger: false\n",
            "      wandb_logger_kwargs:\n",
            "        project: null\n",
            "        name: null\n",
            "      resume_if_exists: true\n",
            "      resume_ignore_no_checkpoint: true\n",
            "      create_checkpoint_callback: true\n",
            "      checkpoint_callback_params:\n",
            "        monitor: validation_${model.data.test_ds.metric.name}\n",
            "        save_top_k: 1\n",
            "        mode: max\n",
            "        save_nemo_on_train_end: true\n",
            "        filename: ${name}--{${exp_manager.checkpoint_callback_params.monitor}:.3f}-{step}-{consumed_samples}\n",
            "        model_parallel_size: ${model.tensor_model_parallel_size}\n",
            "        always_save_nemo: true\n",
            "        save_best_model: false\n",
            "    model:\n",
            "      seed: 1234\n",
            "      tensor_model_parallel_size: 1\n",
            "      pipeline_model_parallel_size: 1\n",
            "      global_batch_size: 32\n",
            "      micro_batch_size: 1\n",
            "      restore_from_path: results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama.nemo\n",
            "      resume_from_checkpoint: null\n",
            "      save_nemo_on_validation_end: true\n",
            "      sync_batch_comm: false\n",
            "      megatron_amp_O2: false\n",
            "      sequence_parallel: false\n",
            "      activations_checkpoint_granularity: null\n",
            "      activations_checkpoint_method: null\n",
            "      activations_checkpoint_num_layers: null\n",
            "      activations_checkpoint_layers_per_pipeline: null\n",
            "      answer_only_loss: true\n",
            "      gradient_as_bucket_view: false\n",
            "      hidden_dropout: 0.0\n",
            "      attention_dropout: 0.0\n",
            "      ffn_dropout: 0.0\n",
            "      peft:\n",
            "        peft_scheme: lora\n",
            "        restore_from_path: results/Llama-3.1-8B/PEFT/checkpoints/megatron_gpt_peft_lora_tuning.nemo\n",
            "        restore_from_ckpt:\n",
            "          checkpoint_dir: null\n",
            "          checkpoint_name: null\n",
            "        adapter_tuning:\n",
            "          type: parallel_adapter\n",
            "          adapter_dim: 32\n",
            "          adapter_dropout: 0.0\n",
            "          norm_position: pre\n",
            "          column_init_method: xavier\n",
            "          row_init_method: zero\n",
            "          norm_type: mixedfusedlayernorm\n",
            "          layer_selection: null\n",
            "          weight_tying: false\n",
            "          position_embedding_strategy: null\n",
            "        lora_tuning:\n",
            "          variant: nemo\n",
            "          target_modules:\n",
            "          - attention_qkv\n",
            "          adapter_dim: 32\n",
            "          adapter_dropout: 0.0\n",
            "          column_init_method: xavier\n",
            "          row_init_method: zero\n",
            "          layer_selection: null\n",
            "          weight_tying: false\n",
            "          position_embedding_strategy: null\n",
            "        p_tuning:\n",
            "          virtual_tokens: 10\n",
            "          bottleneck_dim: 1024\n",
            "          embedding_dim: 1024\n",
            "          init_std: 0.023\n",
            "        ia3_tuning:\n",
            "          layer_selection: null\n",
            "      data:\n",
            "        test_ds:\n",
            "          file_names:\n",
            "          - data/alpaca/test.jsonl\n",
            "          names:\n",
            "          - alpaca_test\n",
            "          global_batch_size: 32\n",
            "          micro_batch_size: 1\n",
            "          shuffle: false\n",
            "          num_workers: 0\n",
            "          pin_memory: true\n",
            "          max_seq_length: 8192\n",
            "          min_seq_length: 1\n",
            "          drop_last: false\n",
            "          context_key: input\n",
            "          label_key: output\n",
            "          add_eos: true\n",
            "          add_sep: false\n",
            "          add_bos: false\n",
            "          write_predictions_to_file: true\n",
            "          output_file_path_prefix: data/alpaca/prediction_peft\n",
            "          truncation_field: input\n",
            "          index_mapping_dir: null\n",
            "          prompt_template: <|begin_of_text|><|start_header_id|>system<|end_header_id|>\\nYou\n",
            "            are a knowledgeable assistant trained to provide accurate and helpful information.\n",
            "            Please respond to the user's queries promptly and politely.<|eot_id|>\\n<|start_header_id|>user<|end_header_id|>\\n{input}<|eot_id|>\\n<|start_header_id|>assistant<|end_header_id|>\\n{output}\n",
            "          tokens_to_generate: 128\n",
            "          truncation_method: right\n",
            "          metric:\n",
            "            name: loss\n",
            "            average: null\n",
            "            num_classes: null\n",
            "    inference:\n",
            "      greedy: true\n",
            "      top_k: 0\n",
            "      top_p: 0.9\n",
            "      temperature: 1.0\n",
            "      all_probs: false\n",
            "      repetition_penalty: 1.0\n",
            "      min_tokens_to_generate: 0\n",
            "      compute_logprob: false\n",
            "      outfile_path: output.txt\n",
            "      compute_attention_mask: true\n",
            "    server: false\n",
            "    port: 5555\n",
            "    web_server: false\n",
            "    share: true\n",
            "    username: test\n",
            "    password: test2\n",
            "    web_port: 9889\n",
            "    chat: false\n",
            "    chatbot_config:\n",
            "      value: false\n",
            "      attributes:\n",
            "      - name: Quality\n",
            "        min: 0\n",
            "        max: 4\n",
            "        key: quality\n",
            "        type: int\n",
            "        default: 4\n",
            "      - name: Toxicity\n",
            "        min: 0\n",
            "        max: 4\n",
            "        key: toxcity\n",
            "        type: int\n",
            "        default: 0\n",
            "      - name: Humor\n",
            "        min: 0\n",
            "        max: 4\n",
            "        key: humor\n",
            "        type: int\n",
            "        default: 0\n",
            "      - name: Creativity\n",
            "        min: 0\n",
            "        max: 4\n",
            "        key: creativity\n",
            "        type: int\n",
            "        default: 0\n",
            "      - name: Violence\n",
            "        min: 0\n",
            "        max: 4\n",
            "        key: violence\n",
            "        type: int\n",
            "        default: 0\n",
            "      - name: Helpfulness\n",
            "        min: 0\n",
            "        max: 4\n",
            "        key: helpfulness\n",
            "        type: int\n",
            "        default: 4\n",
            "      - name: Not_Appropriate\n",
            "        min: 0\n",
            "        max: 4\n",
            "        key: not_appropriate\n",
            "        type: int\n",
            "        default: 0\n",
            "      - name: Language\n",
            "        choices:\n",
            "        - ar\n",
            "        - bg\n",
            "        - bn\n",
            "        - ca\n",
            "        - cs\n",
            "        - da\n",
            "        - de\n",
            "        - el\n",
            "        - en\n",
            "        - eo\n",
            "        - es\n",
            "        - eu\n",
            "        - fa\n",
            "        - fi\n",
            "        - fr\n",
            "        - gl\n",
            "        - he\n",
            "        - hu\n",
            "        - id\n",
            "        - it\n",
            "        - ja\n",
            "        - ko\n",
            "        - nb\n",
            "        - nl\n",
            "        - pl\n",
            "        - pt\n",
            "        - ro\n",
            "        - ru\n",
            "        - sk\n",
            "        - sv\n",
            "        - th\n",
            "        - tr\n",
            "        - uk\n",
            "        - vi\n",
            "        - zh\n",
            "        key: lang\n",
            "        type: list\n",
            "        default: en\n",
            "      user: User\n",
            "      assistant: Assistant\n",
            "      system: 'A chat between a curious human and an artificial intelligence assistant.\n",
            "        The assistant gives helpful, detailed, and polite answers to the human''s questions.\n",
            "    \n",
            "    \n",
            "        '\n",
            "    \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-06 09:00:40 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py:1451: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "      super().__init__(\n",
            "    \n",
            "[NeMo W 2024-12-06 09:00:40 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/_graveyard/precision.py:49: The `MixedPrecisionPlugin` is deprecated. Use `pytorch_lightning.plugins.precision.MixedPrecision` instead.\n",
            "    \n",
            "[NeMo W 2024-12-06 09:00:40 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/parts/nlp_overrides.py:1395: DeprecationWarning: torch.set_autocast_gpu_dtype(dtype) is deprecated. Please use torch.set_autocast_dtype('cuda', dtype) instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:678.)\n",
            "      torch.set_autocast_gpu_dtype(dtype)\n",
            "    \n",
            "GPU available: True (cuda), used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "HPU available: False, using: 0 HPUs\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-06 09:01:07 megatron_init:314] Rank 0 has data parallel group : [0, 1, 2, 3]\n",
            "[NeMo I 2024-12-06 09:01:07 megatron_init:320] Rank 0 has combined group of data parallel and context parallel : [0, 1, 2, 3]\n",
            "[NeMo I 2024-12-06 09:01:07 megatron_init:325] All data parallel group ranks with context parallel combined: [[0, 1, 2, 3]]\n",
            "[NeMo I 2024-12-06 09:01:07 megatron_init:328] Ranks 0 has data parallel rank: 0\n",
            "[NeMo I 2024-12-06 09:01:07 megatron_init:336] Rank 0 has context parallel group: [0]\n",
            "[NeMo I 2024-12-06 09:01:07 megatron_init:339] All context parallel group ranks: [[0], [1], [2], [3]]\n",
            "[NeMo I 2024-12-06 09:01:07 megatron_init:340] Ranks 0 has context parallel rank: 0\n",
            "[NeMo I 2024-12-06 09:01:07 megatron_init:347] Rank 0 has model parallel group: [0]\n",
            "[NeMo I 2024-12-06 09:01:07 megatron_init:348] All model parallel group ranks: [[0], [1], [2], [3]]\n",
            "[NeMo I 2024-12-06 09:01:07 megatron_init:357] Rank 0 has tensor model parallel group: [0]\n",
            "[NeMo I 2024-12-06 09:01:07 megatron_init:361] All tensor model parallel group ranks: [[0], [1], [2], [3]]\n",
            "[NeMo I 2024-12-06 09:01:07 megatron_init:362] Rank 0 has tensor model parallel rank: 0\n",
            "[NeMo I 2024-12-06 09:01:07 megatron_init:382] Rank 0 has pipeline model parallel group: [0]\n",
            "[NeMo I 2024-12-06 09:01:07 megatron_init:394] Rank 0 has embedding group: [0]\n",
            "[NeMo I 2024-12-06 09:01:07 megatron_init:400] All pipeline model parallel group ranks: [[0], [1], [2], [3]]\n",
            "[NeMo I 2024-12-06 09:01:07 megatron_init:401] Rank 0 has pipeline model parallel rank 0\n",
            "[NeMo I 2024-12-06 09:01:07 megatron_init:402] All embedding group ranks: [[0], [1], [2], [3]]\n",
            "[NeMo I 2024-12-06 09:01:07 megatron_init:403] Rank 0 has embedding rank: 0\n",
            "setting number of microbatches to constant 8\n",
            "[NeMo I 2024-12-06 09:01:07 tokenizer_utils:184] Getting HuggingFace AutoTokenizer with pretrained_model_name: meta-llama/Meta-Llama-3-8B\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-06 09:01:07 megatron_base_model:601] Padded vocab_size: 128256, original vocab_size: 128256, dummy tokens: 0.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: expert_model_parallel_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: moe_extended_tp in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: finalize_model_grads_func in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: use_te_rng_tracker in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_bulk_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_rs_dgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_ag in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_split_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_atomic_rs in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_qkv in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: tp_comm_overlap_disable_fc1 in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: defer_embedding_wgrad_compute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: wgrad_deferral_limit in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: pipeline_model_parallel_split_rank in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: _cpu_offloading_context in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_activations in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: cpu_offloading_weights in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:1186] The model: MegatronGPTSFTModel() does not have field.name: barrier_with_L1_time in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: first_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: last_pipeline_num_layers in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: activation_func_fp8_input_store in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: num_moe_experts in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: window_size in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: qk_layernorm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: test_mode in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: calculate_per_token_loss in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: memory_efficient_layer_norm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: fp8_wgrad in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: fp8_dot_product_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: fp8_multi_head_attention in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: tp_only_amax_red in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_router_load_balancing_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_router_topk in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_router_pre_softmax in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_grouped_gemm in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_aux_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_z_loss_coeff in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_input_jitter_eps in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dropping in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_token_dispatcher_type in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_per_layer_logging in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_expert_capacity_factor in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_pad_expert_input_to_capacity in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_token_drop_policy in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: moe_layer_recompute in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: clone_scatter_output_in_embedding in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: disable_parameter_transpose_cache in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: enable_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: external_cuda_graph in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n",
            "[NeMo W 2024-12-06 09:01:07 megatron_base_model:574] The model: MegatronGPTSFTModel() does not have field.name: config_logger_dir in its cfg. Add this key to cfg or config_mapping to make to make it configurable.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "apply rope scaling ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/4\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "apply rope scaling ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/4\n",
            "[rank1]:[W1206 09:02:04.152759686 ProcessGroupGloo.cpp:712] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "apply rope scaling ...\n",
            "apply rope scaling ...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Initializing distributed: GLOBAL_RANK: 2, MEMBER: 3/4\n",
            "[rank2]:[W1206 09:02:08.040314218 ProcessGroupGloo.cpp:712] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
            "Initializing distributed: GLOBAL_RANK: 3, MEMBER: 4/4\n",
            "[rank3]:[W1206 09:02:10.199016260 ProcessGroupGloo.cpp:712] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
            "----------------------------------------------------------------------------------------------------\n",
            "distributed_backend=nccl\n",
            "All distributed processes registered. Starting with 4 processes\n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "[rank0]:[W1206 09:02:10.208471371 ProcessGroupGloo.cpp:712] Warning: Unable to resolve hostname to a (local) address. Using the loopback address as fallback. Manually set the network interface to bind to with GLOO_SOCKET_IFNAME. (function operator())\n",
            "[NeMo W 2024-12-06 09:03:05 nemo_logging:349] /opt/megatron-lm/megatron/core/dist_checkpointing/strategies/torch.py:755: FutureWarning: `load_state_dict` is deprecated and will be removed in future versions. Please use `load` instead.\n",
            "      checkpoint.load_state_dict(\n",
            "    \n",
            "[NeMo W 2024-12-06 09:03:05 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/torch/distributed/checkpoint/planner_helpers.py:311: FutureWarning: Please use DTensor instead and we are deprecating ShardedTensor.\n",
            "      device = getattr(value, \"device\", None)\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-06 09:03:24 nlp_overrides:1358] Model MegatronGPTSFTModel was successfully restored from /workspace/results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama.nemo.\n",
            "[NeMo I 2024-12-06 09:03:29 nlp_adapter_mixins:249] Before adding PEFT params:\n",
            "      | Name  | Type     | Params | Mode \n",
            "    -------------------------------------------\n",
            "    0 | model | GPTModel | 8.0 B  | train\n",
            "    -------------------------------------------\n",
            "    0         Trainable params\n",
            "    8.0 B     Non-trainable params\n",
            "    8.0 B     Total params\n",
            "    32,121.045Total estimated model params size (MB)\n",
            "    649       Modules in train mode\n",
            "    0         Modules in eval mode\n",
            "[NeMo I 2024-12-06 09:03:31 nlp_adapter_mixins:254] After adding PEFT params:\n",
            "      | Name  | Type     | Params | Mode \n",
            "    -------------------------------------------\n",
            "    0 | model | GPTModel | 8.0 B  | train\n",
            "    -------------------------------------------\n",
            "    10.5 M    Trainable params\n",
            "    8.0 B     Non-trainable params\n",
            "    8.0 B     Total params\n",
            "    32,162.988Total estimated model params size (MB)\n",
            "    809       Modules in train mode\n",
            "    0         Modules in eval mode\n",
            "[NeMo I 2024-12-06 09:03:31 megatron_gpt_generate:156] Freezing parameters for PEFT eval:\n",
            "      | Name  | Type     | Params | Mode\n",
            "    ------------------------------------------\n",
            "    0 | model | GPTModel | 8.0 B  | eval\n",
            "    ------------------------------------------\n",
            "    0         Trainable params\n",
            "    8.0 B     Non-trainable params\n",
            "    8.0 B     Total params\n",
            "    32,162.988Total estimated model params size (MB)\n",
            "    0         Modules in train mode\n",
            "    809       Modules in eval mode\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-06 09:03:31 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:161: You have overridden `MegatronGPTSFTModel.configure_sharded_model` which is deprecated. Please override the `configure_model` hook instead. Instantiation with the newer hook will be created on the device right away and have the right data type depending on the precision setting in the Trainer.\n",
            "    \n",
            "[NeMo W 2024-12-06 09:03:31 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/configuration_validator.py:143: You are using the `dataloader_iter` step flavor. If you consume the iterator more than once per step, the `batch_idx` argument in any hook that takes it will not match with the batch index of the last batch consumed. This might have unforeseen effects on callbacks or code that expects to get the correct index. This will also not work well with gradient accumulation. This feature is very experimental and subject to change. Here be dragons.\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-06 09:03:32 megatron_gpt_sft_model:828] Building GPT SFT test datasets.\n",
            "[NeMo I 2024-12-06 09:03:32 text_memmap_dataset:116] Building data files\n",
            "[NeMo I 2024-12-06 09:03:32 text_memmap_dataset:527] Processing 1 data files using 127 workers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-06 09:03:34 text_memmap_dataset:542] Time building 0 / 1 mem-mapped files: 0:00:02.889242\n",
            "[NeMo I 2024-12-06 09:03:35 text_memmap_dataset:527] Processing 1 data files using 127 workers\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-06 09:03:38 text_memmap_dataset:542] Time building 0 / 1 mem-mapped files: 0:00:03.145660\n",
            "[NeMo I 2024-12-06 09:03:38 text_memmap_dataset:158] Loading data files\n",
            "[NeMo I 2024-12-06 09:03:38 text_memmap_dataset:249] Loading data/alpaca/test.jsonl\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-06 09:03:38 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/data/language_modeling/text_memmap_dataset.py:263: ResourceWarning: unclosed file <_io.BufferedReader name='data/alpaca/test.jsonl.idx.info'>\n",
            "      idx_info_dict = pickle.load(open(idx_fn + \".info\", \"rb\"))\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[NeMo I 2024-12-06 09:03:38 text_memmap_dataset:161] Time loading 1 mem-mapped files: 0:00:00.001094\n",
            "[NeMo I 2024-12-06 09:03:38 text_memmap_dataset:165] Computing global indices\n",
            "[NeMo I 2024-12-06 09:03:38 megatron_gpt_sft_model:831] Length of test dataset: 521\n",
            "[NeMo I 2024-12-06 09:03:38 megatron_gpt_sft_model:854] Building dataloader with consumed samples: 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
            "LOCAL_RANK: 3 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
            "LOCAL_RANK: 2 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2,3]\n",
            "[NeMo W 2024-12-06 09:03:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/data_connector.py:424: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=62` in the `DataLoader` to improve performance.\n",
            "    \n",
            "[NeMo W 2024-12-06 09:03:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/loops/utilities.py:148: Found `dataloader_iter` argument in the `test_step`. Note that the support for this signature is experimental and the behavior is subject to change.\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing: |          | 0/? [00:00<?, ?it/s]setting number of microbatches to constant 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-06 09:03:38 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-06 09:03:39 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-06 09:03:40 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-06 09:03:41 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-06 09:03:44 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-06 09:03:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/selective_scan_interface.py:164: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, xz, conv1d_weight, conv1d_bias, x_proj_weight, delta_proj_weight,\n",
            "    \n",
            "[NeMo W 2024-12-06 09:03:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/selective_scan_interface.py:240: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, dout):\n",
            "    \n",
            "[NeMo W 2024-12-06 09:03:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/layer_norm.py:986: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(\n",
            "    \n",
            "[NeMo W 2024-12-06 09:03:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/layer_norm.py:1045: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, dout, *args):\n",
            "    \n",
            "[NeMo W 2024-12-06 09:03:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm/distributed/tensor_parallel.py:26: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, x, weight, bias, process_group=None, sequence_parallel=True):\n",
            "    \n",
            "[NeMo W 2024-12-06 09:03:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm/distributed/tensor_parallel.py:62: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, grad_output):\n",
            "    \n",
            "[NeMo W 2024-12-06 09:03:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:758: FutureWarning: `torch.cuda.amp.custom_fwd(args...)` is deprecated. Please use `torch.amp.custom_fwd(args..., device_type='cuda')` instead.\n",
            "      def forward(ctx, zxbcdt, conv1d_weight, conv1d_bias, dt_bias, A, D, chunk_size, initial_states=None, seq_idx=None, dt_limit=(0.0, float(\"inf\")), return_final_states=False, activation=\"silu\",\n",
            "    \n",
            "[NeMo W 2024-12-06 09:03:45 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/mamba_ssm/ops/triton/ssd_combined.py:836: FutureWarning: `torch.cuda.amp.custom_bwd(args...)` is deprecated. Please use `torch.amp.custom_bwd(args..., device_type='cuda')` instead.\n",
            "      def backward(ctx, dout, *args):\n",
            "    \n",
            "[NeMo W 2024-12-06 09:03:45 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/modules/common/text_generation_utils.py:495: UserWarning: The torch.cuda.*DtypeTensor constructors are no longer recommended. It's best to use methods such as torch.tensor(data, dtype=*, device='cuda') to create tensors. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/tensor/python_tensor.cpp:78.)\n",
            "      input_info_tensor = torch.cuda.FloatTensor(input_info)\n",
            "    \n",
            "[NeMo W 2024-12-06 09:03:45 nemo_logging:349] /opt/NeMo/nemo/collections/nlp/modules/common/text_generation_utils.py:503: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:206.)\n",
            "      string_tensor = torch.as_tensor(\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing DataLoader 0:   0%|          | 0/17 [00:00<?, ?it/s]setting number of microbatches to constant 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-06 09:03:46 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-06 09:03:46 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-06 09:03:47 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-06 09:03:48 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
            "      warnings.warn(\n",
            "    \n",
            "[NeMo W 2024-12-06 09:03:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-06 09:03:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-06 09:03:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
            "      warnings.warn(\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "setting number of microbatches to constant 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-06 09:04:10 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-06 09:04:11 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-06 09:04:13 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing DataLoader 0:   6%|▌         | 1/17 [00:31<08:31,  0.03it/s]setting number of microbatches to constant 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-06 09:04:14 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-06 09:04:15 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n",
            "[NeMo W 2024-12-06 09:04:15 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
            "      warnings.warn(\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "setting number of microbatches to constant 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-06 09:04:30 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing DataLoader 0:  12%|█▏        | 2/17 [00:51<06:24,  0.04it/s]setting number of microbatches to constant 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-06 09:04:32 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
            "      warnings.warn(\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "setting number of microbatches to constant 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-06 09:04:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing DataLoader 0:  18%|█▊        | 3/17 [01:10<05:28,  0.04it/s]setting number of microbatches to constant 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-06 09:04:50 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
            "      warnings.warn(\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "setting number of microbatches to constant 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-06 09:05:05 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing DataLoader 0:  24%|██▎       | 4/17 [01:26<04:40,  0.05it/s]setting number of microbatches to constant 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-06 09:05:05 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
            "      warnings.warn(\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "setting number of microbatches to constant 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-06 09:05:20 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing DataLoader 0:  29%|██▉       | 5/17 [01:41<04:03,  0.05it/s]setting number of microbatches to constant 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-06 09:05:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
            "      warnings.warn(\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "setting number of microbatches to constant 8\n",
            "Testing DataLoader 0:  35%|███▌      | 6/17 [02:01<03:42,  0.05it/s]setting number of microbatches to constant 1\n",
            "setting number of microbatches to constant 8\n",
            "Testing DataLoader 0:  41%|████      | 7/17 [02:21<03:21,  0.05it/s]setting number of microbatches to constant 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[rank2]:W1206 09:06:15.180000 139969535759488 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
            "[rank2]:W1206 09:06:15.180000 139969535759488 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
            "[rank2]:W1206 09:06:15.180000 139969535759488 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 544, actual 736\n",
            "[rank2]:W1206 09:06:15.180000 139969535759488 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
            "[rank2]:W1206 09:06:15.180000 139969535759488 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "setting number of microbatches to constant 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-06 09:06:16 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing DataLoader 0:  47%|████▋     | 8/17 [02:37<02:57,  0.05it/s]setting number of microbatches to constant 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-06 09:06:16 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
            "      warnings.warn(\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "setting number of microbatches to constant 8\n",
            "Testing DataLoader 0:  53%|█████▎    | 9/17 [02:57<02:37,  0.05it/s]setting number of microbatches to constant 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[rank1]:W1206 09:06:50.536000 140583929025664 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
            "[rank1]:W1206 09:06:50.536000 140583929025664 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
            "[rank1]:W1206 09:06:50.536000 140583929025664 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 272, actual 688\n",
            "[rank1]:W1206 09:06:50.536000 140583929025664 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
            "[rank1]:W1206 09:06:50.536000 140583929025664 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "setting number of microbatches to constant 8\n",
            "Testing DataLoader 0:  59%|█████▉    | 10/17 [03:14<02:16,  0.05it/s]setting number of microbatches to constant 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[rank3]:W1206 09:07:07.066000 140178608280704 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
            "[rank3]:W1206 09:07:07.066000 140178608280704 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
            "[rank3]:W1206 09:07:07.066000 140178608280704 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 352, actual 560\n",
            "[rank3]:W1206 09:07:07.066000 140178608280704 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
            "[rank3]:W1206 09:07:07.066000 140178608280704 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "setting number of microbatches to constant 8\n",
            "Testing DataLoader 0:  65%|██████▍   | 11/17 [03:32<01:55,  0.05it/s]setting number of microbatches to constant 1\n",
            "setting number of microbatches to constant 8\n",
            "Testing DataLoader 0:  71%|███████   | 12/17 [03:51<01:36,  0.05it/s]setting number of microbatches to constant 1\n",
            "setting number of microbatches to constant 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-06 09:07:44 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing DataLoader 0:  76%|███████▋  | 13/17 [04:06<01:15,  0.05it/s]setting number of microbatches to constant 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-06 09:07:49 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
            "      warnings.warn(\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "setting number of microbatches to constant 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[rank0]:W1206 09:08:06.010000 139846295696512 torch/_dynamo/convert_frame.py:744] [4/8] torch._dynamo hit config.cache_size_limit (8)\n",
            "[rank0]:W1206 09:08:06.010000 139846295696512 torch/_dynamo/convert_frame.py:744] [4/8]    function: 'calculate_cross_entropy_loss' (/opt/megatron-lm/megatron/core/fusions/fused_cross_entropy.py:47)\n",
            "[rank0]:W1206 09:08:06.010000 139846295696512 torch/_dynamo/convert_frame.py:744] [4/8]    last reason: tensor 'L['exp_logits']' size mismatch at index 0. expected 768, actual 400\n",
            "[rank0]:W1206 09:08:06.010000 139846295696512 torch/_dynamo/convert_frame.py:744] [4/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
            "[rank0]:W1206 09:08:06.010000 139846295696512 torch/_dynamo/convert_frame.py:744] [4/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
            "[NeMo W 2024-12-06 09:08:06 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing DataLoader 0:  82%|████████▏ | 14/17 [04:27<00:57,  0.05it/s]setting number of microbatches to constant 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-06 09:08:07 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
            "      warnings.warn(\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "setting number of microbatches to constant 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-06 09:08:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/module/base.py:578: DeprecationWarning: torch.get_autocast_gpu_dtype() is deprecated. Please use torch.get_autocast_dtype('cuda') instead. (Triggered internally at /opt/pytorch/pytorch/torch/csrc/autograd/init.cpp:733.)\n",
            "      self.activation_dtype = torch.get_autocast_gpu_dtype()\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing DataLoader 0:  88%|████████▊ | 15/17 [04:43<00:37,  0.05it/s]setting number of microbatches to constant 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-06 09:08:25 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/transformer_engine/pytorch/attention.py:4155: UserWarning: window_size should be (-1, -1) or (>=0, >=0) for attn_mask_type=no_mask\n",
            "      warnings.warn(\n",
            "    \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "setting number of microbatches to constant 8\n",
            "Testing DataLoader 0:  94%|█████████▍| 16/17 [05:02<00:18,  0.05it/s]setting number of microbatches to constant 1\n",
            "setting number of microbatches to constant 8\n",
            "Testing DataLoader 0: 100%|██████████| 17/17 [05:16<00:00,  0.05it/s][NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
            "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
            "    <|start_header_id|>user<|end_header_id|>\n",
            "    給定一份財務資料樣本，計算每月總支出。\n",
            "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
            "    <|start_header_id|>assistant<|end_header_id|>\n",
            "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
            "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
            "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
            "    <|start_header_id|>user<|end_header_id|>\n",
            "    給定一份財務資料樣本，計算每月總支出。\n",
            "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
            "    <|start_header_id|>assistant<|end_header_id|>\n",
            "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
            "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
            "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
            "    <|start_header_id|>user<|end_header_id|>\n",
            "    給定一份財務資料樣本，計算每月總支出。\n",
            "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
            "    <|start_header_id|>assistant<|end_header_id|>\n",
            "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
            "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
            "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
            "    <|start_header_id|>user<|end_header_id|>\n",
            "    給定一份財務資料樣本，計算每月總支出。\n",
            "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
            "    <|start_header_id|>assistant<|end_header_id|>\n",
            "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
            "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
            "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
            "    <|start_header_id|>user<|end_header_id|>\n",
            "    給定一份財務資料樣本，計算每月總支出。\n",
            "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
            "    <|start_header_id|>assistant<|end_header_id|>\n",
            "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
            "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
            "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
            "    <|start_header_id|>user<|end_header_id|>\n",
            "    給定一份財務資料樣本，計算每月總支出。\n",
            "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
            "    <|start_header_id|>assistant<|end_header_id|>\n",
            "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
            "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
            "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
            "    <|start_header_id|>user<|end_header_id|>\n",
            "    給定一份財務資料樣本，計算每月總支出。\n",
            "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
            "    <|start_header_id|>assistant<|end_header_id|>\n",
            "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
            "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
            "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
            "    <|start_header_id|>user<|end_header_id|>\n",
            "    給定一份財務資料樣本，計算每月總支出。\n",
            "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
            "    <|start_header_id|>assistant<|end_header_id|>\n",
            "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
            "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
            "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
            "    <|start_header_id|>user<|end_header_id|>\n",
            "    給定一份財務資料樣本，計算每月總支出。\n",
            "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
            "    <|start_header_id|>assistant<|end_header_id|>\n",
            "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
            "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
            "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
            "    <|start_header_id|>user<|end_header_id|>\n",
            "    給定一份財務資料樣本，計算每月總支出。\n",
            "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
            "    <|start_header_id|>assistant<|end_header_id|>\n",
            "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
            "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
            "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
            "    <|start_header_id|>user<|end_header_id|>\n",
            "    給定一份財務資料樣本，計算每月總支出。\n",
            "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
            "    <|start_header_id|>assistant<|end_header_id|>\n",
            "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
            "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
            "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
            "    <|start_header_id|>user<|end_header_id|>\n",
            "    給定一份財務資料樣本，計算每月總支出。\n",
            "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
            "    <|start_header_id|>assistant<|end_header_id|>\n",
            "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
            "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
            "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
            "    <|start_header_id|>user<|end_header_id|>\n",
            "    給定一份財務資料樣本，計算每月總支出。\n",
            "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
            "    <|start_header_id|>assistant<|end_header_id|>\n",
            "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
            "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
            "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
            "    <|start_header_id|>user<|end_header_id|>\n",
            "    給定一份財務資料樣本，計算每月總支出。\n",
            "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
            "    <|start_header_id|>assistant<|end_header_id|>\n",
            "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
            "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
            "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
            "    <|start_header_id|>user<|end_header_id|>\n",
            "    給定一份財務資料樣本，計算每月總支出。\n",
            "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
            "    <|start_header_id|>assistant<|end_header_id|>\n",
            "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
            "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
            "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
            "    <|start_header_id|>user<|end_header_id|>\n",
            "    給定一份財務資料樣本，計算每月總支出。\n",
            "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
            "    <|start_header_id|>assistant<|end_header_id|>\n",
            "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
            "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
            "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
            "    <|start_header_id|>user<|end_header_id|>\n",
            "    給定一份財務資料樣本，計算每月總支出。\n",
            "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
            "    <|start_header_id|>assistant<|end_header_id|>\n",
            "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
            "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
            "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
            "    <|start_header_id|>user<|end_header_id|>\n",
            "    給定一份財務資料樣本，計算每月總支出。\n",
            "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
            "    <|start_header_id|>assistant<|end_header_id|>\n",
            "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
            "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
            "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
            "    <|start_header_id|>user<|end_header_id|>\n",
            "    給定一份財務資料樣本，計算每月總支出。\n",
            "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
            "    <|start_header_id|>assistant<|end_header_id|>\n",
            "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
            "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
            "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
            "    <|start_header_id|>user<|end_header_id|>\n",
            "    給定一份財務資料樣本，計算每月總支出。\n",
            "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
            "    <|start_header_id|>assistant<|end_header_id|>\n",
            "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
            "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
            "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
            "    <|start_header_id|>user<|end_header_id|>\n",
            "    給定一份財務資料樣本，計算每月總支出。\n",
            "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
            "    <|start_header_id|>assistant<|end_header_id|>\n",
            "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
            "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
            "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
            "    <|start_header_id|>user<|end_header_id|>\n",
            "    給定一份財務資料樣本，計算每月總支出。\n",
            "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
            "    <|start_header_id|>assistant<|end_header_id|>\n",
            "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
            "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:553] skipping autogenerated example example <|start_header_id|>system<|end_header_id|>\n",
            "    You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\n",
            "    <|start_header_id|>user<|end_header_id|>\n",
            "    給定一份財務資料樣本，計算每月總支出。\n",
            "    住房：$1,500，食品：$600，交通：$250，娛樂：$400<|eot_id|>\n",
            "    <|start_header_id|>assistant<|end_header_id|>\n",
            "     prediction 每月總支出為$2,750。 label 在這種情況下，每個類別的值之和將是每月的總支出：住房+食品+交通+娛樂= $1,500 + $600 + $250 + $400 = $2,750。\n",
            "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:586] Total deduplicated inference data size: 544 to 521\n",
            "[NeMo I 2024-12-06 09:09:01 megatron_gpt_sft_model:737] Predictions saved to data/alpaca/prediction_peft_test_alpaca_test_inputs_preds_labels.jsonl\n",
            "setting number of microbatches to constant 8\n",
            "Testing DataLoader 0: 100%|██████████| 17/17 [05:23<00:00,  0.05it/s]\n",
            "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
            "┃\u001b[1m \u001b[0m\u001b[1m       Test metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
            "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
            "│\u001b[36m \u001b[0m\u001b[36m        test_loss        \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.5607960224151611    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m  test_loss_alpaca_test  \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.5607960224151611    \u001b[0m\u001b[35m \u001b[0m│\n",
            "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   1.5607960224151611    \u001b[0m\u001b[35m \u001b[0m│\n",
            "└───────────────────────────┴───────────────────────────┘\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[NeMo W 2024-12-06 09:09:01 megatron_gpt_sft_model:677] No training data found, reconfiguring microbatches based on validation batch sizes.\n",
            "[NeMo W 2024-12-06 09:09:01 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "    \n",
            "[NeMo W 2024-12-06 09:09:01 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('test_loss_alpaca_test', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "    \n",
            "[NeMo W 2024-12-06 09:09:01 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('test_loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n",
            "    \n",
            "[NeMo W 2024-12-06 09:09:15 nemo_logging:349] /usr/lib/python3.10/tempfile.py:999: ResourceWarning: Implicitly cleaning up <TemporaryDirectory '/tmp/tmp6uivhzmw'>\n",
            "      _warnings.warn(warn_message, ResourceWarning)\n",
            "    \n"
          ]
        }
      ],
      "source": [
        "%%bash\n",
        "\n",
        "MODEL_NAME=Llama-3.1-8B\n",
        "MODEL=results/Llama-3.1-8B/pretrain/checkpoints/megatron_llama.nemo\n",
        "PEFT_MODEL=results/Llama-3.1-8B/PEFT/checkpoints/megatron_gpt_peft_lora_tuning.nemo\n",
        "NUM_GPUS=4\n",
        "GB=32\n",
        "SEQ_LEN=8192\n",
        "TEST_DS=[data/alpaca/test.jsonl]\n",
        "OUTPUT=data/alpaca/prediction_peft\n",
        "PROMPT_TEMPLATE=\"\\\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\\n",
        "You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\\n\\\n",
        "<|start_header_id|>user<|end_header_id|>\\n\\\n",
        "{input}<|eot_id|>\\n\\\n",
        "<|start_header_id|>assistant<|end_header_id|>\\n\\\n",
        "{output}\\\"\"\n",
        "\n",
        "python /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py \\\n",
        "model.restore_from_path=$MODEL \\\n",
        "model.peft.restore_from_path=$PEFT_MODEL \\\n",
        "model.peft.peft_scheme=lora \\\n",
        "trainer.devices=$NUM_GPUS \\\n",
        "model.global_batch_size=$GB \\\n",
        "model.data.test_ds.file_names=$TEST_DS \\\n",
        "model.data.test_ds.names=\\['alpaca_test'] \\\n",
        "model.data.test_ds.global_batch_size=$GB \\\n",
        "model.data.test_ds.tokens_to_generate=128 \\\n",
        "model.data.test_ds.label_key='output' \\\n",
        "model.data.test_ds.add_eos=True \\\n",
        "model.data.test_ds.add_sep=False \\\n",
        "model.data.test_ds.add_bos=False \\\n",
        "model.data.test_ds.max_seq_length=$SEQ_LEN \\\n",
        "model.data.test_ds.truncation_field=\"input\" \\\n",
        "model.data.test_ds.prompt_template=\"$PROMPT_TEMPLATE\" \\\n",
        "model.data.test_ds.write_predictions_to_file=True \\\n",
        "model.data.test_ds.output_file_path_prefix=$OUTPUT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a27d877",
      "metadata": {
        "tags": [],
        "id": "9a27d877"
      },
      "outputs": [],
      "source": [
        "file_path = \"data/alpaca/prediction_peft_test_alpaca_test_inputs_preds_labels.jsonl\"\n",
        "modify_and_overwrite_jsonl(file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8ce2655-fa5d-4cf9-99ce-2b2545e830fe",
      "metadata": {
        "id": "d8ce2655-fa5d-4cf9-99ce-2b2545e830fe"
      },
      "source": [
        "## 4. Export and Deploy a NeMo Checkpoint to TensorRT-LLM <a name='s4'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d1cc9bc-4121-406c-bd89-0b7d4837a565",
      "metadata": {
        "id": "4d1cc9bc-4121-406c-bd89-0b7d4837a565"
      },
      "source": [
        "Open a terminal and run the following code:\n",
        "\n",
        "```sh\n",
        "python /opt/NeMo/scripts/deploy/nlp/deploy_triton.py \\\n",
        "--nemo_checkpoint results/Llama-3.1-8B/SFT/checkpoints/megatron_gpt_peft_None_tuning.nemo \\\n",
        "--model_type llama \\\n",
        "--dtype bfloat16 \\\n",
        "--triton_model_name Llama\n",
        "```\n",
        "\n",
        "The command above launches a inference server. Keep it running and run the following cell to send a request to the server."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f7dc111b-b740-4aaa-aa18-1c0638bed8fc",
      "metadata": {
        "id": "f7dc111b-b740-4aaa-aa18-1c0638bed8fc"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "\n",
        "PROMPT_TEMPLATE=\"<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\\n",
        "You are a knowledgeable assistant trained to provide accurate and helpful information. Please respond to the user's queries promptly and politely.<|eot_id|>\\n\\\n",
        "<|start_header_id|>user<|end_header_id|>\\n\\\n",
        "{input}<|eot_id|>\\n\\\n",
        "<|start_header_id|>assistant<|end_header_id|>\\n\"\n",
        "\n",
        "INPUT=\"今天天氣好嗎?\"\n",
        "\n",
        "PROMPT=\"${PROMPT_TEMPLATE//\\{input\\}/$INPUT}\"\n",
        "\n",
        "python /opt/NeMo/scripts/deploy/nlp/query.py \\\n",
        "--url \"http://localhost:8000\" \\\n",
        "--model_name Llama \\\n",
        "--prompt \"$PROMPT\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f086ab6-476b-41bd-bd52-e0ffdcc7f66d",
      "metadata": {
        "id": "3f086ab6-476b-41bd-bd52-e0ffdcc7f66d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c1e7f62-ba35-4285-a338-abe809766713",
      "metadata": {
        "id": "5c1e7f62-ba35-4285-a338-abe809766713"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}