{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b95c683e",
   "metadata": {},
   "source": [
    "# NeMo Framework - Training a large language model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af423c6",
   "metadata": {},
   "source": [
    "## Overview\n",
    "Large language model (LLM) like ChatGPT possess astonishing versatility, being able to perform tasks such as induction, programming, translation, and more, with results comparable to or even superior to human experts. To learn how to pre-train a large language model (LLM). NVIDIA has introduced NeMo Framework that is capabilities to pre-process training data, distribute training across multiple GPUs efficiently.\n",
    "\n",
    "Pre-trained language model is powerful in a variety of tasks but often lack the specialized focus needed for domain-specific applications. Therefore, to adapt the language model to a domain-specific task, fine-tuning can be employed. In this notebook, you will learn how to implement two type of tuning methods, **(1) Fine-tuning** and **(2) PEFT methods** like **LoRA** for adapting language model on specific downstream task using NVIDIA NeMo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbabfa72",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "\n",
    "This course covers the below sections:\n",
    "1. [Pre-training](#s1)\n",
    "    - [1.1 Download dataset](#s1.1)\n",
    "    - [1.2 Data preprocessing](#s1.2)\n",
    "    - [1.3 Download pre-trained model for continued pre-training](#s1.3)\n",
    "    - [1.4 Run pre-training](#s1.4)\n",
    "2. [Instruction Tuning ](#s2)\n",
    "    - [2.1 Download dataset: erhwenkuo/alpaca-data-gpt4-chinese-zhtw](#s2.1)\n",
    "    - [2.2 Split the data into train, validation and test](#s2.2)\n",
    "    - [2.3 Full parameter fine-tuning](#s2.3)\n",
    "    - [2.4 Parameter Efficient Fine-tuning](#s2.4)\n",
    "3. [Evaluation](#s3)\n",
    "4. [Export and Deploy a NeMo Checkpoint to TensorRT-LLM](#s4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7ae1d7",
   "metadata": {},
   "source": [
    "## 1. Pre-training <a name='s1'></a>\n",
    "\n",
    "The initial phase of our process is concentrated on model pre-training, which serves as the primary stage for the model to acquire knowledge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caca4a69",
   "metadata": {},
   "source": [
    "### 1.1 Download dataset <a name='s1.1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "70c5955e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0036971569061279297,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": 30,
       "postfix": null,
       "prefix": "Creating json from Arrow format",
       "rate": null,
       "total": 10,
       "unit": "ba",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c623e10574be478c85fd8547123d9ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/10 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "13914259"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('erhwenkuo/wikinews-zhtw')['train']\n",
    "dataset.to_json('./data/custom_dataset/json/wikinews-zhtw.jsonl', force_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3d62c385",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NeMo I 2024-12-10 06:59:48 tokenizer_utils:178] Getting HuggingFace AutoTokenizer with pretrained_model_name: /workspace/tokenizer-llama32-3B\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Vocab size: 128256\n",
      "Output prefix: data/custom_dataset/preprocessed/wikinews\n",
      "Time to startup: 0.2579519748687744\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Processing file data/custom_dataset/json/wikinews-zhtw.jsonl 1/1\n",
      "[NeMo I 2024-12-10 06:59:48 tokenizer_utils:178] Getting HuggingFace AutoTokenizer with pretrained_model_name: /workspace/tokenizer-llama32-3B\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Processed 100 documents (283.4048551016947 docs/s, 0.1730657871887857 MB/s).\n",
      "Processed 200 documents (476.5466618871539 docs/s, 0.25336719894608334 MB/s).\n",
      "Processed 300 documents (614.8845818921748 docs/s, 0.32333098447702435 MB/s).\n",
      "Processed 400 documents (711.4527706524103 docs/s, 0.3803011072601852 MB/s).\n",
      "Processed 500 documents (829.3134122725535 docs/s, 0.4107686668003279 MB/s).\n",
      "Processed 600 documents (956.2896643918878 docs/s, 0.43205929751775624 MB/s).\n",
      "Processed 700 documents (1082.1692385888957 docs/s, 0.4484878999189482 MB/s).\n",
      "Processed 800 documents (1180.3237353384814 docs/s, 0.4673816384880344 MB/s).\n",
      "Processed 900 documents (1270.5293623277282 docs/s, 0.4852541295269804 MB/s).\n",
      "Processed 1000 documents (1354.356957948553 docs/s, 0.49960977068336465 MB/s).\n",
      "Processed 1100 documents (1402.1062586649016 docs/s, 0.5167341624895839 MB/s).\n",
      "Processed 1200 documents (1454.1402637475028 docs/s, 0.5314970104860506 MB/s).\n",
      "Processed 1300 documents (1494.5852416772723 docs/s, 0.5463543583976133 MB/s).\n",
      "Processed 1400 documents (1522.2648672288549 docs/s, 0.5602874662307284 MB/s).\n",
      "Processed 1500 documents (1546.0835144052298 docs/s, 0.5722576393008503 MB/s).\n",
      "Processed 1600 documents (1555.8513729907884 docs/s, 0.5825951743447798 MB/s).\n",
      "Processed 1700 documents (1574.155501992002 docs/s, 0.5923515778607183 MB/s).\n",
      "Processed 1800 documents (1567.8097732435062 docs/s, 0.5972089144287769 MB/s).\n",
      "Processed 1900 documents (1603.5343370844214 docs/s, 0.6085632338637013 MB/s).\n",
      "Processed 2000 documents (1615.326076574925 docs/s, 0.6156503507707096 MB/s).\n",
      "Processed 2100 documents (1612.5370178886842 docs/s, 0.6226006110331616 MB/s).\n",
      "Processed 2200 documents (1612.563762236259 docs/s, 0.6269829882904145 MB/s).\n",
      "Processed 2300 documents (1613.381354521972 docs/s, 0.6328394353524365 MB/s).\n",
      "Processed 2400 documents (1617.4767899037788 docs/s, 0.6388840193351722 MB/s).\n",
      "Processed 2500 documents (1609.0615286004727 docs/s, 0.643024306960601 MB/s).\n",
      "Processed 2600 documents (1611.786394218185 docs/s, 0.6475044055464585 MB/s).\n",
      "Processed 2700 documents (1609.8885294337992 docs/s, 0.6525593535665307 MB/s).\n",
      "Processed 2800 documents (1600.3294396786491 docs/s, 0.6560809732642313 MB/s).\n",
      "Processed 2900 documents (1600.0635632740032 docs/s, 0.6595200078296484 MB/s).\n",
      "Processed 3000 documents (1594.175940330298 docs/s, 0.6626213361601236 MB/s).\n",
      "Processed 3100 documents (1589.1524926560771 docs/s, 0.665379522081268 MB/s).\n",
      "Processed 3200 documents (1593.4371407968365 docs/s, 0.6679679996504874 MB/s).\n",
      "Processed 3300 documents (1579.9181550260373 docs/s, 0.669837110215623 MB/s).\n",
      "Processed 3400 documents (1578.1947007329095 docs/s, 0.6719530520110464 MB/s).\n",
      "Processed 3500 documents (1579.421475928506 docs/s, 0.6749768117622921 MB/s).\n",
      "Processed 3600 documents (1580.7279569265474 docs/s, 0.6772777194377738 MB/s).\n",
      "Processed 3700 documents (1583.7195413916227 docs/s, 0.6809837212558459 MB/s).\n",
      "Processed 3800 documents (1614.6030285339118 docs/s, 0.6836525441182116 MB/s).\n",
      "Processed 3900 documents (1635.3218146171773 docs/s, 0.6858056544807035 MB/s).\n",
      "Processed 4000 documents (1657.2772619686289 docs/s, 0.6881742140777158 MB/s).\n",
      "Processed 4100 documents (1682.2169761163711 docs/s, 0.6903838344986098 MB/s).\n",
      "Processed 4200 documents (1702.6072316032526 docs/s, 0.6924690693401556 MB/s).\n",
      "Processed 4300 documents (1713.8689291571034 docs/s, 0.694585932453892 MB/s).\n",
      "Processed 4400 documents (1708.5790027660464 docs/s, 0.6957497946781959 MB/s).\n",
      "Processed 4500 documents (1702.9659125635994 docs/s, 0.6968887146918232 MB/s).\n",
      "Processed 4600 documents (1683.613074551836 docs/s, 0.6970827830744768 MB/s).\n",
      "Processed 4700 documents (1670.9669117977387 docs/s, 0.6977537101510846 MB/s).\n",
      "Processed 4800 documents (1650.7835635488225 docs/s, 0.696029295283138 MB/s).\n",
      "Processed 4900 documents (1656.3762510917472 docs/s, 0.6960038162897189 MB/s).\n",
      "Processed 5000 documents (1659.9772400924865 docs/s, 0.6963062603043514 MB/s).\n",
      "Processed 5100 documents (1685.9293652990464 docs/s, 0.6984431185296764 MB/s).\n",
      "Processed 5200 documents (1704.0759206087937 docs/s, 0.6999368463769955 MB/s).\n",
      "Processed 5300 documents (1717.1970440476755 docs/s, 0.7012665884149627 MB/s).\n",
      "Processed 5400 documents (1715.888683960222 docs/s, 0.7019200035758347 MB/s).\n",
      "Processed 5500 documents (1713.14011782805 docs/s, 0.7029319775156538 MB/s).\n",
      "Processed 5600 documents (1705.2475847089734 docs/s, 0.7037667236120657 MB/s).\n",
      "Processed 5700 documents (1683.1942847439127 docs/s, 0.704335809595099 MB/s).\n",
      "Processed 5800 documents (1667.9261572519347 docs/s, 0.7051673633110266 MB/s).\n",
      "Processed 5900 documents (1661.6721006226974 docs/s, 0.7060201249423785 MB/s).\n",
      "Processed 6000 documents (1648.5592463556668 docs/s, 0.7062706218739649 MB/s).\n",
      "Processed 6100 documents (1658.9438782629354 docs/s, 0.7081867304483235 MB/s).\n",
      "Processed 6200 documents (1615.2372415053362 docs/s, 0.7078284581729231 MB/s).\n",
      "Processed 6300 documents (1578.62636208503 docs/s, 0.7074174803311803 MB/s).\n",
      "Processed 6400 documents (1547.4532163319607 docs/s, 0.7069377564972777 MB/s).\n",
      "Processed 6500 documents (1517.2118804422853 docs/s, 0.7070862477530213 MB/s).\n",
      "Processed 6600 documents (1506.3313143464661 docs/s, 0.7077093570902342 MB/s).\n",
      "Processed 6700 documents (1502.8691854997471 docs/s, 0.7083918954974148 MB/s).\n",
      "Processed 6800 documents (1506.5771000947116 docs/s, 0.7092692105136661 MB/s).\n",
      "Processed 6900 documents (1511.0438299182076 docs/s, 0.7099072014853193 MB/s).\n",
      "Processed 7000 documents (1510.6702973362821 docs/s, 0.7102317520725061 MB/s).\n",
      "Processed 7100 documents (1509.0767855295937 docs/s, 0.7107390851159799 MB/s).\n",
      "Processed 7200 documents (1513.8963190935087 docs/s, 0.7114177371826088 MB/s).\n",
      "Processed 7300 documents (1509.125267412863 docs/s, 0.7121002213432283 MB/s).\n",
      "Processed 7400 documents (1510.2580632761199 docs/s, 0.7126782637933119 MB/s).\n",
      "Processed 7500 documents (1505.3568011282825 docs/s, 0.71251147239927 MB/s).\n",
      "Processed 7600 documents (1500.3814835889195 docs/s, 0.7134558805844203 MB/s).\n",
      "Processed 7700 documents (1494.6134405313649 docs/s, 0.7139028663771151 MB/s).\n",
      "Processed 7800 documents (1486.211644745333 docs/s, 0.7139477085233465 MB/s).\n",
      "Processed 7900 documents (1471.1522355896461 docs/s, 0.7159626736262575 MB/s).\n",
      "Processed 8000 documents (1456.4639685636323 docs/s, 0.7157788003751148 MB/s).\n",
      "Processed 8100 documents (1448.7104531445127 docs/s, 0.7161014921854858 MB/s).\n",
      "Processed 8200 documents (1452.1164650568478 docs/s, 0.7166259776623467 MB/s).\n",
      "Processed 8300 documents (1452.323995521802 docs/s, 0.7175352756546461 MB/s).\n",
      "Processed 8400 documents (1452.4215536242236 docs/s, 0.7175452269947067 MB/s).\n",
      "Processed 8500 documents (1450.601916084719 docs/s, 0.717854187579599 MB/s).\n",
      "Processed 8600 documents (1434.1388524643414 docs/s, 0.7173002178899397 MB/s).\n",
      "Processed 8700 documents (1425.7935212013185 docs/s, 0.7175896545151779 MB/s).\n",
      "Processed 8800 documents (1420.4043721356588 docs/s, 0.7173813673564496 MB/s).\n",
      "Processed 8900 documents (1413.171403768676 docs/s, 0.7174312162082305 MB/s).\n",
      "Processed 9000 documents (1412.3923903362186 docs/s, 0.7178463718353166 MB/s).\n",
      "Processed 9100 documents (1405.1314685676648 docs/s, 0.717741029082023 MB/s).\n",
      "Processed 9200 documents (1409.5041710077946 docs/s, 0.7183021981781088 MB/s).\n",
      "Processed 9300 documents (1391.707509788631 docs/s, 0.7169752983238761 MB/s).\n",
      "Processed 9400 documents (1384.33354712903 docs/s, 0.7167771617179932 MB/s).\n",
      "Processed 9500 documents (1380.1790067345485 docs/s, 0.7166537461421575 MB/s).\n",
      "Processed 9600 documents (1363.9235405379836 docs/s, 0.7161181156236925 MB/s).\n",
      "Processed 9700 documents (1356.405294737505 docs/s, 0.7160491925579648 MB/s).\n",
      "Processed 9800 documents (1356.1124479686 docs/s, 0.7160485330681122 MB/s).\n"
     ]
    }
   ],
   "source": [
    "# Data preprocessing\n",
    "!mkdir -p data/custom_dataset/preprocessed\n",
    "\n",
    "!python /opt/NeMo/scripts/nlp_language_modeling/preprocess_data_for_megatron.py \\\n",
    "--input data/custom_dataset/json/wikinews-zhtw.jsonl \\\n",
    "--json-keys text \\\n",
    "--dataset-impl mmap \\\n",
    "--tokenizer-library huggingface \\\n",
    "--tokenizer-type=/workspace/tokenizer-llama32-3B \\\n",
    "--output-prefix data/custom_dataset/preprocessed/wikinews \\\n",
    "--append-eod "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "057799cc",
   "metadata": {},
   "source": [
    "### 1.3 Download pre-trained model for continued pre-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c7e0e1f4-2e1b-4ec5-a6ab-888703694f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 16 files: 100%|██████████| 16/16 [00:00<00:00, 170760.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: convert_llama_hf_to_nemo.py [-h] --input_name_or_path\n",
      "                                   INPUT_NAME_OR_PATH --output_path\n",
      "                                   OUTPUT_PATH [--hparams_file HPARAMS_FILE]\n",
      "                                   [--precision PRECISION]\n",
      "convert_llama_hf_to_nemo.py: error: unrecognized arguments: --llama3 True\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b\"export HF_TOKEN='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'\\nHF_MODEL=meta-llama/Llama-3.2-3B-Instruct ## Download from HF\\n\\nhuggingface-cli download $HF_MODEL\\n\\npython /opt/NeMo/scripts/checkpoint_converters/convert_llama_hf_to_nemo.py \\\\\\n--input_name_or_path $HF_MODEL \\\\\\n--output_path Llama-3.2-3B-Instruct.nemo \\\\\\n--llama3 True \\\\\\n--precision bf16\\n\"' returned non-zero exit status 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mexport HF_TOKEN=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhf_oDgakKBLRNvVpdhOAYMpOYTjRSGwKLKYvM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mHF_MODEL=meta-llama/Llama-3.2-3B-Instruct ## Download from HF\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mhuggingface-cli download $HF_MODEL\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43mpython /opt/NeMo/scripts/checkpoint_converters/convert_llama_hf_to_nemo.py \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m--input_name_or_path $HF_MODEL \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m--output_path Llama-3.2-3B-Instruct.nemo \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m--llama3 True \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m--precision bf16\u001b[39;49m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py:2517\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2515\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2516\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2517\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2519\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2521\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2522\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/script.py:154\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    153\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/IPython/core/magics/script.py:314\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    311\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    313\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 314\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b\"export HF_TOKEN='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'\\nHF_MODEL=meta-llama/Llama-3.2-3B-Instruct ## Download from HF\\n\\nhuggingface-cli download $HF_MODEL\\n\\npython /opt/NeMo/scripts/checkpoint_converters/convert_llama_hf_to_nemo.py \\\\\\n--input_name_or_path $HF_MODEL \\\\\\n--output_path Llama-3.2-3B-Instruct.nemo \\\\\\n--llama3 True \\\\\\n--precision bf16\\n\"' returned non-zero exit status 2."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "export HF_TOKEN='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'\n",
    "HF_MODEL=meta-llama/Llama-3.2-3B-Instruct ## Download from HF\n",
    "\n",
    "huggingface-cli download $HF_MODEL\n",
    "\n",
    "python /opt/NeMo/scripts/checkpoint_converters/convert_llama_hf_to_nemo.py \\\n",
    "--input_name_or_path $HF_MODEL \\\n",
    "--output_path Llama-3.2-3B-Instruct.nemo \\\n",
    "--llama31 True \\\n",
    "--precision bf16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b4d6aaef",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: HF_MODEL=meta-llama/Llama-3.2-3B-Instruct\n",
      "env: HF_TOKEN=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\n",
      "[NeMo I 2024-12-10 06:41:22 convert_llama_hf_to_nemo:111] loading checkpoint meta-llama/Llama-3.2-3B-Instruct\n",
      "[NeMo W 2024-12-10 06:41:22 nemo_logging:349] /usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "      warnings.warn(\n",
      "    \n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/NeMo/scripts/checkpoint_converters/convert_llama_hf_to_nemo.py\", line 312, in <module>\n",
      "    convert(args)\n",
      "  File \"/opt/NeMo/scripts/checkpoint_converters/convert_llama_hf_to_nemo.py\", line 112, in convert\n",
      "    model = LlamaForCausalLM.from_pretrained(args.input_name_or_path)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\", line 2981, in from_pretrained\n",
      "    config, model_kwargs = cls.config_class.from_pretrained(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 611, in from_pretrained\n",
      "    return cls.from_dict(config_dict, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/configuration_utils.py\", line 763, in from_dict\n",
      "    config = cls(**config_dict)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/configuration_llama.py\", line 160, in __init__\n",
      "    self._rope_scaling_validation()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/models/llama/configuration_llama.py\", line 180, in _rope_scaling_validation\n",
      "    raise ValueError(\n",
      "ValueError: `rope_scaling` must be a dictionary with with two fields, `type` and `factor`, got {'factor': 32.0, 'high_freq_factor': 4.0, 'low_freq_factor': 1.0, 'original_max_position_embeddings': 8192, 'rope_type': 'llama3'}\n"
     ]
    }
   ],
   "source": [
    "# 設定環境變數\n",
    "%env HF_MODEL=meta-llama/Llama-3.2-3B-Instruct \n",
    "%env HF_TOKEN=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\n",
    "\n",
    "# 呼叫轉換腳本\n",
    "!python /opt/NeMo/scripts/checkpoint_converters/convert_llama_hf_to_nemo.py \\\n",
    "    --input_name_or_path $HF_MODEL \\\n",
    "    --output_path Llama-3.2-3B-Instruct.nemo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6075984",
   "metadata": {},
   "source": [
    "### 1.4 Run pre-training <a name='s1.4'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833935b3-13a8-4f79-b294-e739b4076081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update continue training script for nemo:24.05\n",
    "\n",
    "file_path = '/opt/NeMo/examples/nlp/language_modeling/megatron_gpt_continue_training.py'\n",
    "insert_line = 158\n",
    "new_line = \"        cfg.trainer.precision = None\\n\"\n",
    "\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    lines = file.readlines()\n",
    "\n",
    "lines.insert(insert_line, new_line)\n",
    "\n",
    "with open(file_path, 'w', encoding='utf-8') as file:\n",
    "    file.writelines(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20380c65",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env MODEL_NAME=Llama-3.1-8B \n",
    "%env MODEL=Llama-3.1-8B-Instruct.nemo\n",
    "%env NUM_GPUS=2 \n",
    "%env MAX_STEPS=100\n",
    "%env MBS=1\n",
    "%env GBS=1\n",
    "%env TP=1\n",
    "%env PP=1\n",
    "%env LR=1e-4\n",
    "%env DATA_SPLITS='9990,8,2'\n",
    "%env DATA_PREFIX=[1.0,data/custom_dataset/preprocessed/wikinews_text_document]\n",
    "\n",
    "!python /opt/NeMo/examples/nlp/language_modeling/megatron_gpt_continue_training.py \\\n",
    "--config-path=/opt/NeMo-Framework-Launcher/launcher_scripts/conf/training/llama --config-name=llama2_7b \\\n",
    "+restore_from_path=$MODEL \\\n",
    "+base_results_dir=results \\\n",
    "+model.seq_len_interpolation_factor=null \\\n",
    "trainer.num_nodes=1 \\\n",
    "trainer.devices=$NUM_GPUS \\\n",
    "trainer.precision=16 \\\n",
    "trainer.max_steps=$MAX_STEPS \\\n",
    "trainer.limit_val_batches=32 \\\n",
    "trainer.val_check_interval=100 \\\n",
    "exp_manager.explicit_log_dir=/workspace/results/$MODEL_NAME/Pretraining \\\n",
    "exp_manager.wandb_logger_kwargs.name=$MODEL_NAME \\\n",
    "exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True \\\n",
    "exp_manager.checkpoint_callback_params.model_parallel_size=$(($TP*$PP)) \\\n",
    "+exp_manager.checkpoint_callback_params.every_n_train_steps=50 \\\n",
    "+exp_manager.checkpoint_callback_params.every_n_epochs=null \\\n",
    "exp_manager.checkpoint_callback_params.monitor=\"epoch\" \\\n",
    "exp_manager.checkpoint_callback_params.save_top_k=-1 \\\n",
    "model.micro_batch_size=$MBS \\\n",
    "model.global_batch_size=$GBS \\\n",
    "model.tensor_model_parallel_size=$TP \\\n",
    "model.pipeline_model_parallel_size=$PP \\\n",
    "model.tokenizer.library=huggingface \\\n",
    "model.tokenizer.type=tokenizer \\\n",
    "model.tokenizer.model=null \\\n",
    "model.optim.lr=$LR \\\n",
    "model.data.splits_string=${DATA_SPLITS} \\\n",
    "model.data.data_prefix=${DATA_PREFIX} \\\n",
    "model.data.num_workers=0 \\\n",
    "model.data.seq_length=1024"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a783e2f1",
   "metadata": {},
   "source": [
    "## 2. Instruction Tuning <a name='s2'></a>\n",
    "\n",
    "We will be using the [erhwenkuo/alpaca-data-gpt4-chinese-zhtw](https://huggingface.co/datasets/erhwenkuo/alpaca-data-gpt4-chinese-zhtw) is a dataset that contains Chinese (zh-tw) Instruction-Following generated by GPT-4 using Alpaca prompts for fine-tuning LLMs.\n",
    "\n",
    "The dataset was originaly shared in [this repository](https://github.com/Instruction-Tuning-with-GPT-4/GPT-4-LLM). This dataset is a translation from English to Chinese.\n",
    "\n",
    "### 2.1 Download dataset: erhwenkuo/alpaca-data-gpt4-chinese-zhtw <a name='s2.1'></a>\n",
    "Let's download dataset and save it as json first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c0c8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset('erhwenkuo/alpaca-data-gpt4-chinese-zhtw')['train']\n",
    "output_path = 'data/alpaca/gpt4-chinese-zhtw.jsonl'\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w') as f:\n",
    "    for human_instruction, human_input, assistant_output in zip(dataset['instruction'], dataset['input'], dataset['output']):\n",
    "        f.write(json.dumps({'input': '\\n'.join([human_instruction.strip(),human_input.strip()]).strip(), 'output': assistant_output.strip()}, ensure_ascii=False)+ '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbfe455",
   "metadata": {},
   "outputs": [],
   "source": [
    "!head -n 1 data/alpaca/gpt4-chinese-zhtw.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482ce086",
   "metadata": {},
   "source": [
    "### 2.2 Split the data into train, validation and test. <a name='s2.2'></a>\n",
    "\n",
    "Generate the train, test and validation splits- you may use your own script to do this or create a new script and use the following sample split_train_val.py by copying it over in the chinese-dolly directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1e05f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "input_file = \"data/alpaca/gpt4-chinese-zhtw.jsonl\"\n",
    "training_output_file = \"data/alpaca/training.jsonl\"\n",
    "validation_output_file = \"data/alpaca/validation.jsonl\"\n",
    "test_output_file = \"data/alpaca/test.jsonl\"\n",
    "\n",
    "# Specify the proportion of data for training and validation\n",
    "train_proportion = 0.98\n",
    "validation_proportion = 0.01\n",
    "test_proportion = 0.01\n",
    "\n",
    "# Read the JSONL file and shuffle the JSON objects\n",
    "with open(input_file, \"r\") as f:\n",
    "    lines = f.readlines()\n",
    "    random.shuffle(lines)\n",
    "\n",
    "# Calculate split indices\n",
    "total_lines = len(lines)\n",
    "train_index = int(total_lines * train_proportion)\n",
    "val_index = int(total_lines * validation_proportion)\n",
    "\n",
    "# Distribute JSON objects into training and validation sets\n",
    "train_data = lines[:train_index]\n",
    "validation_data = lines[train_index:train_index+val_index]\n",
    "test_data = lines[train_index+val_index:]\n",
    "\n",
    "# Write JSON objects to training file\n",
    "with open(training_output_file, \"w\") as f:\n",
    "    for line in train_data:\n",
    "        f.write(line.strip() + \"\\n\")\n",
    "\n",
    "# Write JSON objects to validation file\n",
    "with open(validation_output_file, \"w\") as f:\n",
    "    for line in validation_data:\n",
    "        f.write(line.strip() + \"\\n\")\n",
    "\n",
    "# Write JSON objects to training file\n",
    "with open(test_output_file, \"w\") as f:\n",
    "    for line in test_data:\n",
    "        f.write(line.strip() + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bb5dacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What the dataset looks like after spliting\n",
    "!head -1 data/alpaca/training.jsonl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7a47e6",
   "metadata": {},
   "source": [
    "### 2.3 Full parameter fine-tuning  <a name='s2.3'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9044fde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env MODEL_NAME=TinyLlama-1.1B\n",
    "%env MODEL=results/TinyLlama-1.1B/Pretraining/checkpoints/megatron_llama.nemo\n",
    "%env NUM_GPUS=1\n",
    "%env MAX_STEPS=100\n",
    "%env VAL_INTERVAL=1.0\n",
    "%env GBS=16\n",
    "%env MBS=1\n",
    "%env TP=1\n",
    "%env PP=1\n",
    "%env LR=1e-4\n",
    "%env TRAIN_DS=[data/alpaca/training.jsonl]\n",
    "%env VALID_DS=[data/alpaca/validation.jsonl]\n",
    "%env TEST_DS=[data/alpaca/test.jsonl]\n",
    "%env CONCAT_SAMPLING_PROBS=[1.0]\n",
    "%env PROMPT_TEMPLATE=\"<|user|>\\n{input}</s>\\n<|assistant|>\\n{output}\"\n",
    "\n",
    "!python /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\n",
    "--config-path=/opt/NeMo/examples/nlp/language_modeling/tuning/conf --config-name=megatron_gpt_finetuning_config \\\n",
    "trainer.devices=$NUM_GPUS \\\n",
    "trainer.max_steps=$MAX_STEPS \\\n",
    "trainer.precision=16 \\\n",
    "trainer.val_check_interval=$VAL_INTERVAL \\\n",
    "exp_manager.explicit_log_dir=results/$MODEL_NAME/SFT \\\n",
    "exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True \\\n",
    "model.tensor_model_parallel_size=$TP \\\n",
    "model.pipeline_model_parallel_size=$PP \\\n",
    "model.restore_from_path=$MODEL \\\n",
    "model.global_batch_size=$GBS \\\n",
    "model.micro_batch_size=$MBS \\\n",
    "model.data.train_ds.file_names=${TRAIN_DS} \\\n",
    "model.data.validation_ds.file_names=${VALID_DS} \\\n",
    "model.data.test_ds.file_names=${TEST_DS} \\\n",
    "model.data.train_ds.num_workers=0 \\\n",
    "model.data.validation_ds.num_workers=0 \\\n",
    "model.data.test_ds.num_workers=0 \\\n",
    "model.data.train_ds.concat_sampling_probabilities=${CONCAT_SAMPLING_PROBS} \\\n",
    "model.data.train_ds.prompt_template=\"$PROMPT_TEMPLATE\" \\\n",
    "model.data.train_ds.max_seq_length=1024 \\\n",
    "model.optim.lr=$LR \\\n",
    "model.peft.peft_scheme=null"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df427c8",
   "metadata": {},
   "source": [
    "### 2.4. Parameter Efficient Fine-tuning <a name='s2.4'></a>\n",
    "Fine-tuning language model can be computationally expensive and risk overfitting, especially with small, specialized datasets. Parameter-efficient fine-tuning methods like LoRA offer a solution. These techniques adapt the model to specific tasks by modifying only a subset of parameters, reducing computational costs and mitigating overfitting risks. In essence, LoRA enable a more efficient and targeted adaptation of large language models for specialized tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b3f0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env MODEL_NAME=TinyLlama-1.1B\n",
    "%env MODEL=results/TinyLlama-1.1B/Pretraining/checkpoints/megatron_llama.nemo\n",
    "%env NUM_GPUS=1\n",
    "%env MAX_STEPS=10\n",
    "%env VAL_INTERVAL=1.0\n",
    "%env GBS=16\n",
    "%env MBS=1\n",
    "%env TP=1\n",
    "%env PP=1\n",
    "%env LR=1e-4\n",
    "%env TRAIN_DS=[data/alpaca/training.jsonl]\n",
    "%env VALID_DS=[data/alpaca/validation.jsonl]\n",
    "%env TEST_DS=[data/alpaca/test.jsonl]\n",
    "%env CONCAT_SAMPLING_PROBS=[1.0]\n",
    "%env PROMPT_TEMPLATE=\"<|user|>\\n{input}</s>\\n<|assistant|>\\n{output}\"\n",
    "\n",
    "!python /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_finetuning.py \\\n",
    "--config-path=/opt/NeMo/examples/nlp/language_modeling/tuning/conf --config-name=megatron_gpt_finetuning_config \\\n",
    "trainer.devices=$NUM_GPUS \\\n",
    "trainer.max_steps=$MAX_STEPS \\\n",
    "trainer.precision=16 \\\n",
    "trainer.val_check_interval=$VAL_INTERVAL \\\n",
    "exp_manager.explicit_log_dir=/workspace/results/$MODEL_NAME/PEFT \\\n",
    "exp_manager.checkpoint_callback_params.save_nemo_on_train_end=True \\\n",
    "model.tensor_model_parallel_size=$TP \\\n",
    "model.pipeline_model_parallel_size=$PP \\\n",
    "model.restore_from_path=$MODEL \\\n",
    "model.global_batch_size=$GBS \\\n",
    "model.micro_batch_size=$MBS \\\n",
    "model.data.train_ds.file_names=${TRAIN_DS} \\\n",
    "model.data.validation_ds.file_names=${VALID_DS} \\\n",
    "model.data.test_ds.file_names=${TEST_DS} \\\n",
    "model.data.train_ds.num_workers=0 \\\n",
    "model.data.validation_ds.num_workers=0 \\\n",
    "model.data.test_ds.num_workers=0 \\\n",
    "model.data.train_ds.concat_sampling_probabilities=${CONCAT_SAMPLING_PROBS} \\\n",
    "model.data.train_ds.prompt_template=\"$PROMPT_TEMPLATE\" \\\n",
    "model.data.train_ds.max_seq_length=1024 \\\n",
    "model.optim.lr=$LR \\\n",
    "model.peft.peft_scheme=lora \\\n",
    "model.peft.lora_tuning.adapter_dim=32"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331fd92e",
   "metadata": {},
   "source": [
    "## 3 Evaluation <a name='s3'></a>\n",
    "\n",
    "If you want to evaluate an SFT .nemo file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1048ec2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env MODEL_NAME=TinyLlama-1.1B\n",
    "%env MODEL=results/TinyLlama-1.1B/SFT/checkpoints/megatron_gpt_peft_None_tuning.nemo\n",
    "%env NUM_GPUS=1\n",
    "%env TEST_DS=[data/alpaca/test.jsonl]\n",
    "%env OUTPUT=data/alpaca/prediction\n",
    "%env PROMPT_TEMPLATE=\"<|user|>\\n{input}</s>\\n<|assistant|>\\n{output}\"\n",
    "\n",
    "!python /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py \\\n",
    "trainer.precision=16 \\\n",
    "trainer.devices=$NUM_GPUS \\\n",
    "model.restore_from_path=$MODEL \\\n",
    "model.tensor_model_parallel_size=$NUM_GPUS \\\n",
    "model.pipeline_model_parallel_size=1 \\\n",
    "model.megatron_amp_O2=False \\\n",
    "model.peft.restore_from_path=null \\\n",
    "model.data.test_ds.file_names=$TEST_DS \\\n",
    "model.data.test_ds.names=\\['alpaca_test'] \\\n",
    "model.data.test_ds.global_batch_size=32 \\\n",
    "model.data.test_ds.micro_batch_size=1 \\\n",
    "model.data.test_ds.tokens_to_generate=30 \\\n",
    "model.data.test_ds.label_key='output' \\\n",
    "model.data.test_ds.add_eos=True \\\n",
    "model.data.test_ds.add_sep=False \\\n",
    "model.data.test_ds.add_bos=False \\\n",
    "model.data.test_ds.truncation_field=\"input\" \\\n",
    "model.data.test_ds.prompt_template=\"$PROMPT_TEMPLATE\" \\\n",
    "model.data.test_ds.write_predictions_to_file=True \\\n",
    "model.data.test_ds.output_file_path_prefix=$OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cab6ca-591b-43b5-a078-8f8c5f10008e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "def modify_and_overwrite_jsonl(file_path):\n",
    "    data_list = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data = json.loads(line)\n",
    "            data_list.append(data)\n",
    "    \n",
    "    with open(file_path, 'w', encoding='utf-8') as file:\n",
    "        for data in data_list:\n",
    "            json_line = json.dumps(data, ensure_ascii=False) + \"\\n\"\n",
    "            file.write(json_line)\n",
    "\n",
    "file_path = \"/workspace/data/alpaca/prediction_test_alpaca_test_inputs_preds_labels.jsonl\"\n",
    "modify_and_overwrite_jsonl(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d032e3",
   "metadata": {},
   "source": [
    "If you want to evaluate a PEFT Model, you should provide a base GPT model and a PEFT model .nemo file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bff533",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env MODEL_NAME=TinyLlama-1.1B\n",
    "%env MODEL=results/TinyLlama-1.1B/Pretraining/checkpoints/megatron_llama.nemo\n",
    "%env PEFT_MODEL=results/TinyLlama-1.1B/PEFT/checkpoints/megatron_gpt_peft_lora_tuning.nemo\n",
    "%env NUM_GPUS=1\n",
    "%env TEST_DS=[data/alpaca/test.jsonl]\n",
    "%env OUTPUT=/workspace/data/alpaca/prediction_peft\n",
    "%env PROMPT_TEMPLATE=\"<|user|>\\n{input}</s>\\n<|assistant|>\\n{output}\"\n",
    "\n",
    "!python /opt/NeMo/examples/nlp/language_modeling/tuning/megatron_gpt_generate.py \\\n",
    "trainer.precision=16 \\\n",
    "trainer.devices=$NUM_GPUS \\\n",
    "model.restore_from_path=$MODEL \\\n",
    "model.megatron_amp_O2=False \\\n",
    "model.peft.restore_from_path=$PEFT_MODEL \\\n",
    "model.peft.peft_scheme=lora \\\n",
    "model.data.test_ds.file_names=$TEST_DS \\\n",
    "model.data.test_ds.names=\\['alpaca_test'] \\\n",
    "model.data.test_ds.global_batch_size=32 \\\n",
    "model.data.test_ds.micro_batch_size=1 \\\n",
    "model.data.test_ds.tokens_to_generate=30 \\\n",
    "model.data.test_ds.label_key='output' \\\n",
    "model.data.test_ds.add_eos=True \\\n",
    "model.data.test_ds.add_sep=False \\\n",
    "model.data.test_ds.add_bos=False \\\n",
    "model.data.test_ds.truncation_field=\"input\" \\\n",
    "model.data.test_ds.prompt_template=\"$PROMPT_TEMPLATE\" \\\n",
    "model.data.test_ds.write_predictions_to_file=True \\\n",
    "model.data.test_ds.output_file_path_prefix=$OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d24c3c6c-0733-45f1-8794-5c1a33303486",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"/workspace/data/alpaca/prediction_peft_test_alpaca_test_inputs_preds_labels.jsonl\"\n",
    "modify_and_overwrite_jsonl(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d98bdb0",
   "metadata": {},
   "source": [
    "## 4. Export and Deploy a NeMo Checkpoint to TensorRT-LLM <a name='s4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f0e857-08a6-4b47-b055-7c6fff03762f",
   "metadata": {},
   "source": [
    "Open a terminal and run the following code:\n",
    "\n",
    "```sh\n",
    "python /opt/NeMo/scripts/deploy/nlp/deploy_triton.py \\\n",
    "--nemo_checkpoint /workspace/results/TinyLlama-1.1B/SFT/checkpoints/megatron_gpt_peft_None_tuning.nemo \\\n",
    "--model_type llama \\\n",
    "--dtype float16 \\\n",
    "--triton_model_name TinyLlama\n",
    "```\n",
    "\n",
    "The command above launches a inference server. Keep it running and run the following cell to send a request to the server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02836ad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python /opt/NeMo/scripts/deploy/nlp/query.py \\\n",
    "--url \"http://localhost:8000\" \\\n",
    "--model_name TinyLlama \\\n",
    "--prompt '<|system|>\\nYou are a helpful chatbot.</s>\\n<|user|>\\nHi, how are you?</s>\\n<|assistant|>\\n'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46166896",
   "metadata": {},
   "source": [
    "## Clear your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec93d29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf data results TinyLlama-1.1B-Chat-v1.0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
